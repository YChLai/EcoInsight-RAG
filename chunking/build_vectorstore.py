import os
import json
import shutil
import uuid
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from zhipuai import ZhipuAI
from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility

# --- é…ç½® ---
# ä½¿ç”¨ç»å¯¹è·¯å¾„
PROCESSED_DATA_FOLDER = "d:\\LLM\\RAG\\Nvidia-Finance-Rag\\processed_data"
# Milvusé…ç½®
MILVUS_HOST = "localhost"
MILVUS_PORT = 19530
MILVUS_COLLECTION = "environmental_data_rag"
EMBEDDING_MODEL_NAME = "BAAI/bge-m3"

# --- åˆå§‹åŒ– LLM ç”¨äºç”Ÿæˆæ‘˜è¦ ---
try:
    client = ZhipuAI(api_key=os.getenv("ZHIPUAI_API_KEY"))
    print("æ™ºè°±AIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼Œå°†ç”¨äºç”Ÿæˆæ‘˜è¦ã€‚")
except Exception as e:
    client = None
    print(f"æ™ºè°±AIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}ã€‚æ‘˜è¦ç”ŸæˆåŠŸèƒ½å°†ä¸å¯ç”¨ã€‚")


def generate_summary_with_llm(content):
    """ä½¿ç”¨LLMä¸ºæ–‡æœ¬å—ç”Ÿæˆæ‘˜è¦ã€‚"""
    if not client:
        return "æ‘˜è¦åŠŸèƒ½æœªå¯ç”¨ã€‚"
    if len(content) < 500:
        return content
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¯å¢ƒæ•°æ®åˆ†æå¸ˆã€‚è¯·ä¸ºä»¥ä¸‹ç¯å¢ƒæŠ¥å‘Šå†…å®¹ç”Ÿæˆä¸€ä¸ªç®€æ´ã€ç²¾ç¡®ã€ä¿¡æ¯å¯†é›†çš„æ‘˜è¦ï¼Œä¸è¶…è¿‡150ä¸ªå­—ã€‚æ‘˜è¦éœ€è¦æ•æ‰æœ€å…³é”®çš„å®ä½“ã€æ•°æ®å’Œç»“è®ºã€‚\n\nå†…å®¹å¦‚ä¸‹ï¼š\n---\n{content}\n---\n\næ‘˜è¦ï¼š"""
    try:
        response = client.chat.completions.create(
            model="glm-4-flash",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=300, temperature=0.0
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"  - è°ƒç”¨LLMç”Ÿæˆæ‘˜è¦æ—¶å‡ºé”™: {e}")
        return "æ‘˜è¦ç”Ÿæˆå¤±è´¥ã€‚"


def load_and_prepare_docs():
    """
    åŠ è½½æ‰€æœ‰å¤„ç†å¥½çš„JSONå’ŒTXTæ–‡ä»¶ï¼Œåˆ›å»ºçˆ¶æ–‡æ¡£ã€‚
    """
    print("--- æ­¥éª¤ä¸€: åŠ è½½å¹¶å‡†å¤‡çˆ¶æ–‡æ¡£ (å·²ä¿®æ­£) ---")

    image_summaries_path = os.path.join(PROCESSED_DATA_FOLDER, "image_summaries.json")
    if os.path.exists(image_summaries_path):
        with open(image_summaries_path, 'r', encoding='utf-8') as f:
            image_summaries = json.load(f)
        print(f"  - å·²åŠ è½½ {len(image_summaries)} æ¡å›¾ç‰‡æ‘˜è¦ã€‚")
    else:
        image_summaries = {}
        print("  - æœªæ‰¾åˆ°å›¾ç‰‡æ‘˜è¦æ–‡ä»¶ï¼Œå°†ç»§ç»­å¤„ç†ã€‚")

    parent_docs = []
    for filename in sorted(os.listdir(PROCESSED_DATA_FOLDER)):
        file_path = os.path.join(PROCESSED_DATA_FOLDER, filename)

        # --- æ–°å¢é€»è¾‘ï¼šåŒæ—¶å¤„ç† .txt æ–‡ä»¶ ---
        if filename.endswith("_transcribed.txt"):
            print(f"  - æ­£åœ¨å¤„ç†éŸ³é¢‘è½¬å½•ç¨¿: {filename}")
            with open(file_path, 'r', encoding='utf-8') as f:
                full_text = f.read()

            # å°†æ•´ä¸ªè½¬å½•ç¨¿è§†ä¸ºä¸€ä¸ªå¤§çš„çˆ¶æ–‡æ¡£
            doc_id = str(uuid.uuid4())
            doc = Document(
                page_content=full_text,
                metadata={
                    "source": filename.replace("_transcribed.txt", ""),
                    "page": 1,  # å¯¹äºè½¬å½•ç¨¿ï¼Œæˆ‘ä»¬å°†å…¶è§†ä¸ºå•é¡µ
                    "doc_id": doc_id
                }
            )
            parent_docs.append(doc)

        # --- åŸæœ‰é€»è¾‘ï¼šå¤„ç† .json æ–‡ä»¶ ---
        elif filename.endswith(".json") and filename != "image_summaries.json":
            print(f"  - æ­£åœ¨å¤„ç†PDFè§£ææ–‡ä»¶: {filename}")
            with open(file_path, 'r', encoding='utf-8') as f:
                blocks = json.load(f)

            pages = {}
            for block in blocks:
                page_num = block["metadata"]["page"]
                if page_num not in pages:
                    pages[page_num] = []

                content = block["content"]
                if block["type"] == "image_placeholder":
                    img_name = content.replace("[IMAGE: ", "").replace("]", "")
                    summary = image_summaries.get(img_name, "")
                    content = f"--- [å‚è€ƒå›¾ç‰‡: {img_name}] ---\n{summary}\n--- [å›¾ç‰‡æè¿°ç»“æŸ] ---"

                pages[page_num].append(content)

            for page_num, contents in sorted(pages.items()):
                full_page_content = "\n\n".join(contents)
                doc_id = str(uuid.uuid4())
                doc = Document(
                    page_content=full_page_content,
                    metadata={
                        "source": filename.replace("_processed.json", ""),
                        "page": page_num,
                        "doc_id": doc_id
                    }
                )
                parent_docs.append(doc)

    print(f"  - æ€»å…±åˆ›å»ºäº† {len(parent_docs)} ä¸ªçˆ¶æ–‡æ¡£ (æ¥è‡ªPDFå’ŒTXT)ã€‚")
    return parent_docs


def create_child_chunks_and_summaries(parent_docs):
    """
    ä¸ºçˆ¶æ–‡æ¡£åˆ›å»ºå­å—å’Œæ‘˜è¦ã€‚
    """
    print("\n--- æ­¥éª¤äºŒ: åˆ›å»ºå­å—ä¸æ‘˜è¦ ---")

    child_chunks = []
    summary_docs = []

    child_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)

    for i, parent_doc in enumerate(parent_docs):
        doc_id = parent_doc.metadata["doc_id"]
        source_info = f"{parent_doc.metadata['source']} (é¡µç  {parent_doc.metadata['page']})"
        print(f"  - æ­£åœ¨å¤„ç†çˆ¶æ–‡æ¡£ ({i + 1}/{len(parent_docs)}): {source_info}")

        chunks = child_splitter.split_text(parent_doc.page_content)
        for j, chunk_text in enumerate(chunks):
            chunk_doc = Document(
                page_content=chunk_text,
                metadata={**parent_doc.metadata, "is_child": True, "chunk_index": j}
            )
            child_chunks.append(chunk_doc)

        summary_text = generate_summary_with_llm(parent_doc.page_content)
        summary_doc = Document(
            page_content=summary_text,
            metadata={**parent_doc.metadata, "is_summary": True}
        )
        summary_docs.append(summary_doc)

    print(f"  - å·²åˆ›å»º {len(child_chunks)} ä¸ªå­å—ã€‚")
    print(f"  - å·²åˆ›å»º {len(summary_docs)} ä¸ªæ‘˜è¦ã€‚")

    return child_chunks, summary_docs


def create_and_persist_vector_store(docs_to_index, embeddings):
    """åˆ›å»ºå¹¶æŒä¹…åŒ–å‘é‡åº“ã€‚"""
    print("\n--- æ­¥éª¤ä¸‰: åµŒå…¥æ•°æ®å¹¶æ„å»ºå‘é‡åº“ ---")
    
    # è¿æ¥åˆ°Milvus
    print(f"  - è¿æ¥åˆ°Milvus: {MILVUS_HOST}:{MILVUS_PORT}")
    connections.connect("default", host=MILVUS_HOST, port=MILVUS_PORT)
    
    # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™åˆ é™¤
    if utility.has_collection(MILVUS_COLLECTION):
        print(f"  - å‘ç°æ—§çš„é›†åˆï¼Œæ­£åœ¨åˆ é™¤: {MILVUS_COLLECTION}")
        utility.drop_collection(MILVUS_COLLECTION)
    
    # åˆ›å»ºå­—æ®µ
    fields = [
        FieldSchema(name="id", dtype=DataType.VARCHAR, max_length=40, is_primary=True),
        FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=65535),
        FieldSchema(name="source", dtype=DataType.VARCHAR, max_length=255),
        FieldSchema(name="page", dtype=DataType.INT32),
        FieldSchema(name="doc_id", dtype=DataType.VARCHAR, max_length=36),
        FieldSchema(name="is_child", dtype=DataType.BOOL, default_value=False),
        FieldSchema(name="is_summary", dtype=DataType.BOOL, default_value=False),
        FieldSchema(name="chunk_index", dtype=DataType.INT64, default_value=-1),
        FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=1024)  # BAAI/bge-m3 æ¨¡å‹çš„ç»´åº¦
    ]
    
    # åˆ›å»ºé›†åˆ
    schema = CollectionSchema(fields, description="ç¯å¢ƒæ•°æ®RAGå‘é‡åº“")
    collection = Collection(MILVUS_COLLECTION, schema)
    print(f"  - é›†åˆåˆ›å»ºæˆåŠŸ: {MILVUS_COLLECTION}")
    
    # å‡†å¤‡æ•°æ®
    print("  - æ­£åœ¨å¤„ç†æ–‡æ¡£å¹¶ç”ŸæˆåµŒå…¥å‘é‡...")
    data = []
    for doc in docs_to_index:
        # ç”Ÿæˆå”¯ä¸€ID
        doc_id = doc.metadata.get("doc_id", str(uuid.uuid4()))
        chunk_index = doc.metadata.get("chunk_index", -1)
        if chunk_index != -1:
            # ä½¿ç”¨å‰32ä¸ªå­—ç¬¦çš„doc_idåŠ ä¸Š4ä½chunk_index
            short_doc_id = doc_id.replace("-", "")[:32]
            unique_id = f"{short_doc_id}{chunk_index:04d}"
            # ç¡®ä¿é•¿åº¦ä¸è¶…è¿‡36ä¸ªå­—ç¬¦
            unique_id = unique_id[:36]
        else:
            unique_id = doc_id
        
        # ç”ŸæˆåµŒå…¥å‘é‡
        embedding = embeddings.embed_query(doc.page_content)
        
        # æå–å…ƒæ•°æ®
        source = doc.metadata.get("source", "")
        page = doc.metadata.get("page", 0)
        is_child = doc.metadata.get("is_child", False)
        is_summary = doc.metadata.get("is_summary", False)
        
        # æ·»åŠ åˆ°æ•°æ®åˆ—è¡¨
        data.append({
            "id": unique_id,
            "content": doc.page_content,
            "source": source,
            "page": page,
            "doc_id": doc_id,
            "is_child": is_child,
            "is_summary": is_summary,
            "chunk_index": chunk_index,
            "embedding": embedding
        })
    
    # æ‰¹é‡æ’å…¥æ•°æ®
    print(f"  - æ­£åœ¨æ’å…¥ {len(data)} æ¡æ•°æ®...")
    collection.insert([
        [item["id"] for item in data],
        [item["content"] for item in data],
        [item["source"] for item in data],
        [item["page"] for item in data],
        [item["doc_id"] for item in data],
        [item["is_child"] for item in data],
        [item["is_summary"] for item in data],
        [item["chunk_index"] for item in data],
        [item["embedding"] for item in data]
    ])
    
    # åˆ›å»ºç´¢å¼•
    print("  - æ­£åœ¨åˆ›å»ºç´¢å¼•...")
    index_params = {
        "index_type": "HNSW",
        "metric_type": "L2",
        "params": {
            "M": 8,
            "efConstruction": 64
        }
    }
    collection.create_index("embedding", index_params)
    
    # åŠ è½½é›†åˆåˆ°å†…å­˜
    collection.load()
    print("  - å‘é‡æ•°æ®åº“åˆ›å»ºå®Œæ¯•å¹¶å·²æˆåŠŸåŠ è½½åˆ°å†…å­˜ï¼")
    
    return collection


def verify_retrieval(embeddings):
    """éªŒè¯æ£€ç´¢æ•ˆæœã€‚"""
    print("\n--- æ­¥éª¤å››: éªŒè¯æ£€ç´¢æ•ˆæœ ---")
    
    # è¿æ¥åˆ°Milvus
    try:
        connections.connect("default", host=MILVUS_HOST, port=MILVUS_PORT)
    except Exception as e:
        print(f"è¿æ¥Milvuså¤±è´¥: {e}")
        return
    
    # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
    if not utility.has_collection(MILVUS_COLLECTION):
        print("é›†åˆä¸å­˜åœ¨ï¼Œæ— æ³•éªŒè¯ã€‚")
        return
    
    # åŠ è½½é›†åˆ
    collection = Collection(MILVUS_COLLECTION)
    collection.load()
    
    # éªŒè¯ä¸€ä¸ªæ¥è‡ªéŸ³é¢‘è½¬å½•ç¨¿çš„æŸ¥è¯¢
    query = "are there other that have yet to be announced of the same kind of scale and magnitude?"
    print(f"  - æ¨¡æ‹ŸéŸ³é¢‘å†…å®¹æŸ¥è¯¢: '{query}'")
    
    # ç”ŸæˆæŸ¥è¯¢å‘é‡
    query_embedding = embeddings.embed_query(query)
    
    # æ‰§è¡Œç›¸ä¼¼æ€§æœç´¢
    search_params = {
        "metric_type": "L2",
        "params": {
            "ef": 64
        }
    }
    
    # æ„å»ºè¿‡æ»¤æ¡ä»¶
    expr = "is_child == True"
    
    # æ‰§è¡Œæœç´¢
    results = collection.search(
        data=[query_embedding],
        anns_field="embedding",
        param=search_params,
        limit=3,
        expr=expr,
        output_fields=["id", "content", "source", "page", "doc_id", "is_child", "chunk_index"]
    )
    
    print("\n  - æ£€ç´¢åˆ°çš„å‰3ä¸ªç²¾ç¡®å—ä¸ºï¼š\n")
    if not results or not results[0]:
        print("    æœªèƒ½æ£€ç´¢åˆ°ä»»ä½•ç›¸å…³å†…å®¹ã€‚")
    else:
        for i, hit in enumerate(results[0]):
            content = hit.entity.get("content", "")
            source = hit.entity.get("source", "")
            page = hit.entity.get("page", 0)
            doc_id = hit.entity.get("doc_id", "")
            is_child = hit.entity.get("is_child", False)
            chunk_index = hit.entity.get("chunk_index", -1)
            
            print(f"    å†…å®¹ç‰‡æ®µ: {content[:150].replace('\n', ' ')}...")
            print(f"      å…ƒæ•°æ®: {{'source': '{source}', 'page': {page}, 'doc_id': '{doc_id}', 'is_child': {is_child}, 'chunk_index': {chunk_index}}}")
            print(f"      ç›¸ä¼¼åº¦: {hit.distance:.4f}")
            print("      " + "-" * 20)


if __name__ == "__main__":
    # 1. åŠ è½½å¹¶å‡†å¤‡çˆ¶æ–‡æ¡£
    parent_documents = load_and_prepare_docs()

    if parent_documents:
        # 2. åˆ›å»ºå­å—å’Œæ‘˜è¦
        child_chunks, summary_docs = create_child_chunks_and_summaries(parent_documents)
        all_docs_to_index = child_chunks + summary_docs

        # 3. åµŒå…¥ä¸å­˜å‚¨
        print("\næ­£åœ¨åˆå§‹åŒ–åµŒå…¥æ¨¡å‹...")
        embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL_NAME,
            model_kwargs={"device": "cuda"},
            encode_kwargs={"normalize_embeddings": True}
        )
        create_and_persist_vector_store(all_docs_to_index, embeddings)

        # 4. éªŒè¯
        verify_retrieval(embeddings)
        print(f"\nğŸ‰ æ­å–œï¼æ•°æ®å¤„ç†ä¸ç´¢å¼•æµç¨‹å·²å…¨éƒ¨ä¿®å¤å¹¶å®Œæˆï¼")
    else:
        print("æœªèƒ½åŠ è½½ä»»ä½•æ–‡æ¡£ï¼Œæµç¨‹ç»ˆæ­¢ã€‚")