import os
import json
import shutil
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings

PROCESSED_DATA_FOLDER = "../processed_data"
VECTOR_DB_PATH = "../chroma_db"
EMBEDDING_MODEL_NAME = "BAAI/bge-m3"

def load_docs_from_json():
    """
    èŒè´£ä¸€ï¼šåŠ è½½æ‰€æœ‰å¤„ç†å¥½çš„JSONæ–‡ä»¶ï¼Œæ•´åˆå›¾ç‰‡æ‘˜è¦ï¼Œ
    å¹¶è¿”å›ä¸€ä¸ª Document å¯¹è±¡çš„åˆ—è¡¨ã€‚
    """
    print("--- æ­¥éª¤ä¸€: ä»JSONåŠ è½½å¹¶æ•´åˆæ‰€æœ‰æ–‡æ¡£å— ---")

    # åŠ è½½å›¾ç‰‡æ‘˜è¦
    image_summaries_path = os.path.join(PROCESSED_DATA_FOLDER, "image_summaries.json")
    with open(image_summaries_path, 'r', encoding='utf-8') as f:
        image_summaries = json.load(f)
    print(f"  - å·²åŠ è½½ {len(image_summaries)} æ¡å›¾ç‰‡æ‘˜è¦ã€‚")

    all_docs = []
    for filename in sorted(os.listdir(PROCESSED_DATA_FOLDER)):
        if filename.endswith(".json") and filename != "image_summaries.json":
            file_path = os.path.join(PROCESSED_DATA_FOLDER, filename)
            with open(file_path, 'r', encoding='utf-8') as f:
                blocks = json.load(f)

            print(f"  - æ­£åœ¨å¤„ç†: {filename}")
            for block in blocks:
                content = block["content"]
                metadata = block["metadata"]

                if block["type"] == "image_placeholder":
                    img_name = content.replace("[IMAGE: ", "").replace("]", "")
                    summary = image_summaries.get(img_name, "")  # ä½¿ç”¨ .get() é¿å…KeyError
                    content = f"--- [å‚è€ƒå›¾ç‰‡: {img_name} æ¥è‡ªç¬¬ {metadata['page']} é¡µ] ---\n{summary}\n--- [å›¾ç‰‡æè¿°ç»“æŸ] ---"

                # ä¸ºæ¯ä¸ªå—åˆ›å»ºä¸€ä¸ªDocumentå¯¹è±¡
                doc = Document(page_content=content, metadata=metadata)
                # å°†å—çš„ç±»å‹ä¹ŸåŠ å…¥å…ƒæ•°æ®ï¼Œæ–¹ä¾¿ä¸‹ä¸€æ­¥å¤„ç†
                doc.metadata["type"] = block["type"]
                all_docs.append(doc)

    print(f"  - æ€»å…±åŠ è½½äº† {len(all_docs)} ä¸ªåŸå§‹æ–‡æ¡£å—ã€‚")
    return all_docs

def chunk_documents(docs):
    """
    èŒè´£äºŒï¼šå¯¹åŠ è½½å¥½çš„Documentåˆ—è¡¨è¿›è¡Œå·®å¼‚åŒ–åˆ†å‰²ã€‚
    """
    print("\n--- æ­¥éª¤äºŒ: å¯¹æ–‡æ¡£å—è¿›è¡Œå·®å¼‚åŒ–åˆ†å‰² ---")

    final_chunks = []
    # åˆå§‹åŒ–ä¸€ä¸ªä»…ç”¨äºå¤„ç†é•¿ç¯‡æ–‡æœ¬çš„åˆ†å‰²å™¨
    prose_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

    for doc in docs:
        # æ ¹æ®å—çš„ç±»å‹å†³å®šå¤„ç†ç­–ç•¥
        doc_type = doc.metadata.get("type", "prose")

        if doc_type in ["table", "table_summary", "image_placeholder"]:
            # å¯¹äºè¿™äº›ç±»å‹ï¼Œæˆ‘ä»¬ä¿ç•™å…¶å®Œæ•´æ€§ï¼Œä¸è¿›è¡Œåˆ†å‰²
            final_chunks.append(doc)
        else:  # é»˜è®¤æƒ…å†µï¼ŒåŒ…æ‹¬ "prose" ç±»å‹
            # å¯¹æ™®é€šæ–‡æœ¬å—è¿›è¡Œåˆ†å‰²
            prose_chunks = prose_splitter.split_documents([doc])
            final_chunks.extend(prose_chunks)

    print(f"  - æ‰€æœ‰å—è¢«æˆåŠŸå¤„ç†æˆ {len(final_chunks)} ä¸ªæœ€ç»ˆChunkã€‚")
    return final_chunks

def create_and_persist_vector_store(chunks, embeddings):
    print("\n--- æ­¥éª¤ä¸‰: åµŒå…¥æ•°æ®å¹¶æ„å»ºå‘é‡åº“ ---")
    if os.path.exists(VECTOR_DB_PATH):
        print(f"  - å‘ç°æ—§çš„æ•°æ®åº“ï¼Œæ­£åœ¨åˆ é™¤: {VECTOR_DB_PATH}")
        shutil.rmtree(VECTOR_DB_PATH)
    Chroma.from_documents(chunks, embeddings, persist_directory=VECTOR_DB_PATH)
    print("  - å‘é‡æ•°æ®åº“åˆ›å»ºå®Œæ¯•å¹¶å·²æˆåŠŸæŒä¹…åŒ–ï¼")

def verify_metadata(embeddings):
    print("\n--- æ­¥éª¤å››: éªŒè¯å…ƒæ•°æ® ---")
    if not os.path.exists(VECTOR_DB_PATH):
        print("æ•°æ®åº“ä¸å­˜åœ¨ï¼Œæ— æ³•éªŒè¯ã€‚")
        return
    db = Chroma(persist_directory=VECTOR_DB_PATH, embedding_function=embeddings)
    retrieved_docs = db.similarity_search("revenue", k=3)
    print("  - å¯¹'revenue'è¿›è¡Œç›¸ä¼¼åº¦æœç´¢ï¼Œæ£€ç´¢åˆ°çš„å‰3ä¸ªå—ä¸ºï¼š\n")
    for doc in retrieved_docs:
        print("å†…å®¹ç‰‡æ®µ: {}...".format(doc.page_content[:120].replace("\n", " ")))
        print(f"    å…ƒæ•°æ®: {doc.metadata}")
        print("    " + "-" * 20)

if __name__ == "__main__":
    # --- ç»è¿‡é‡æ„ï¼Œä¸»æµç¨‹æ›´åŠ æ¸…æ™° ---
    # 1. åŠ è½½
    all_documents = load_docs_from_json()

    if all_documents:
        # 2. åˆ†å‰²
        final_chunks = chunk_documents(all_documents)

        # 3. åµŒå…¥ä¸å­˜å‚¨
        print("\næ­£åœ¨åˆå§‹åŒ–åµŒå…¥æ¨¡å‹...")
        embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL_NAME,
            model_kwargs={"device": "cuda"},
            encode_kwargs={"normalize_embeddings": True}
        )
        create_and_persist_vector_store(final_chunks, embeddings)

        # 4. éªŒè¯
        verify_metadata(embeddings)
        print(f"\nğŸ‰ æ­å–œï¼æ•°æ®å¤„ç†ä¸ç´¢å¼•æµç¨‹å·²å…¨éƒ¨ä¼˜åŒ–å®Œæˆï¼")
    else:
        print("æœªèƒ½åŠ è½½ä»»ä½•æ–‡æ¡£ï¼Œæµç¨‹ç»ˆæ­¢ã€‚")