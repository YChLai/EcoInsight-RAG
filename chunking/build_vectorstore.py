import os
import json
import shutil
import uuid
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from zhipuai import ZhipuAI

# --- é…ç½® ---
PROCESSED_DATA_FOLDER = "../processed_data"
VECTOR_DB_PATH = "../chroma_db_optimized"
EMBEDDING_MODEL_NAME = "BAAI/bge-m3"

# --- åˆå§‹åŒ– LLM ç”¨äºç”Ÿæˆæ‘˜è¦ ---
try:
    client = ZhipuAI(api_key=os.getenv("ZHIPU_API_KEY"))
    print("æ™ºè°±AIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼Œå°†ç”¨äºç”Ÿæˆæ‘˜è¦ã€‚")
except Exception as e:
    client = None
    print(f"æ™ºè°±AIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}ã€‚æ‘˜è¦ç”ŸæˆåŠŸèƒ½å°†ä¸å¯ç”¨ã€‚")


def generate_summary_with_llm(content):
    """ä½¿ç”¨LLMä¸ºæ–‡æœ¬å—ç”Ÿæˆæ‘˜è¦ã€‚"""
    if not client:
        return "æ‘˜è¦åŠŸèƒ½æœªå¯ç”¨ã€‚"
    if len(content) < 500:
        return content
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èæ–‡æ¡£åˆ†æå¸ˆã€‚è¯·ä¸ºä»¥ä¸‹è´¢æŠ¥å†…å®¹ç”Ÿæˆä¸€ä¸ªç®€æ´ã€ç²¾ç¡®ã€ä¿¡æ¯å¯†é›†çš„æ‘˜è¦ï¼Œä¸è¶…è¿‡150ä¸ªå­—ã€‚æ‘˜è¦éœ€è¦æ•æ‰æœ€å…³é”®çš„å®ä½“ã€æ•°æ®å’Œç»“è®ºã€‚\n\nå†…å®¹å¦‚ä¸‹ï¼š\n---\n{content}\n---\n\næ‘˜è¦ï¼š"""
    try:
        response = client.chat.completions.create(
            model="glm-4-flash",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=300, temperature=0.0
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"  - è°ƒç”¨LLMç”Ÿæˆæ‘˜è¦æ—¶å‡ºé”™: {e}")
        return "æ‘˜è¦ç”Ÿæˆå¤±è´¥ã€‚"


def load_and_prepare_docs():
    """
    åŠ è½½æ‰€æœ‰å¤„ç†å¥½çš„JSONå’ŒTXTæ–‡ä»¶ï¼Œåˆ›å»ºçˆ¶æ–‡æ¡£ã€‚
    """
    print("--- æ­¥éª¤ä¸€: åŠ è½½å¹¶å‡†å¤‡çˆ¶æ–‡æ¡£ (å·²ä¿®æ­£) ---")

    image_summaries_path = os.path.join(PROCESSED_DATA_FOLDER, "image_summaries.json")
    if os.path.exists(image_summaries_path):
        with open(image_summaries_path, 'r', encoding='utf-8') as f:
            image_summaries = json.load(f)
        print(f"  - å·²åŠ è½½ {len(image_summaries)} æ¡å›¾ç‰‡æ‘˜è¦ã€‚")
    else:
        image_summaries = {}
        print("  - æœªæ‰¾åˆ°å›¾ç‰‡æ‘˜è¦æ–‡ä»¶ï¼Œå°†ç»§ç»­å¤„ç†ã€‚")

    parent_docs = []
    for filename in sorted(os.listdir(PROCESSED_DATA_FOLDER)):
        file_path = os.path.join(PROCESSED_DATA_FOLDER, filename)

        # --- æ–°å¢é€»è¾‘ï¼šåŒæ—¶å¤„ç† .txt æ–‡ä»¶ ---
        if filename.endswith("_transcribed.txt"):
            print(f"  - æ­£åœ¨å¤„ç†éŸ³é¢‘è½¬å½•ç¨¿: {filename}")
            with open(file_path, 'r', encoding='utf-8') as f:
                full_text = f.read()

            # å°†æ•´ä¸ªè½¬å½•ç¨¿è§†ä¸ºä¸€ä¸ªå¤§çš„çˆ¶æ–‡æ¡£
            doc_id = str(uuid.uuid4())
            doc = Document(
                page_content=full_text,
                metadata={
                    "source": filename.replace("_transcribed.txt", ""),
                    "page": 1,  # å¯¹äºè½¬å½•ç¨¿ï¼Œæˆ‘ä»¬å°†å…¶è§†ä¸ºå•é¡µ
                    "doc_id": doc_id
                }
            )
            parent_docs.append(doc)

        # --- åŸæœ‰é€»è¾‘ï¼šå¤„ç† .json æ–‡ä»¶ ---
        elif filename.endswith(".json") and filename != "image_summaries.json":
            print(f"  - æ­£åœ¨å¤„ç†PDFè§£ææ–‡ä»¶: {filename}")
            with open(file_path, 'r', encoding='utf-8') as f:
                blocks = json.load(f)

            pages = {}
            for block in blocks:
                page_num = block["metadata"]["page"]
                if page_num not in pages:
                    pages[page_num] = []

                content = block["content"]
                if block["type"] == "image_placeholder":
                    img_name = content.replace("[IMAGE: ", "").replace("]", "")
                    summary = image_summaries.get(img_name, "")
                    content = f"--- [å‚è€ƒå›¾ç‰‡: {img_name}] ---\n{summary}\n--- [å›¾ç‰‡æè¿°ç»“æŸ] ---"

                pages[page_num].append(content)

            for page_num, contents in sorted(pages.items()):
                full_page_content = "\n\n".join(contents)
                doc_id = str(uuid.uuid4())
                doc = Document(
                    page_content=full_page_content,
                    metadata={
                        "source": filename.replace("_processed.json", ""),
                        "page": page_num,
                        "doc_id": doc_id
                    }
                )
                parent_docs.append(doc)

    print(f"  - æ€»å…±åˆ›å»ºäº† {len(parent_docs)} ä¸ªçˆ¶æ–‡æ¡£ (æ¥è‡ªPDFå’ŒTXT)ã€‚")
    return parent_docs


def create_child_chunks_and_summaries(parent_docs):
    """
    ä¸ºçˆ¶æ–‡æ¡£åˆ›å»ºå­å—å’Œæ‘˜è¦ã€‚
    """
    print("\n--- æ­¥éª¤äºŒ: åˆ›å»ºå­å—ä¸æ‘˜è¦ ---")

    child_chunks = []
    summary_docs = []

    child_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)

    for i, parent_doc in enumerate(parent_docs):
        doc_id = parent_doc.metadata["doc_id"]
        source_info = f"{parent_doc.metadata['source']} (é¡µç  {parent_doc.metadata['page']})"
        print(f"  - æ­£åœ¨å¤„ç†çˆ¶æ–‡æ¡£ ({i + 1}/{len(parent_docs)}): {source_info}")

        chunks = child_splitter.split_text(parent_doc.page_content)
        for j, chunk_text in enumerate(chunks):
            chunk_doc = Document(
                page_content=chunk_text,
                metadata={**parent_doc.metadata, "is_child": True, "chunk_index": j}
            )
            child_chunks.append(chunk_doc)

        summary_text = generate_summary_with_llm(parent_doc.page_content)
        summary_doc = Document(
            page_content=summary_text,
            metadata={**parent_doc.metadata, "is_summary": True}
        )
        summary_docs.append(summary_doc)

    print(f"  - å·²åˆ›å»º {len(child_chunks)} ä¸ªå­å—ã€‚")
    print(f"  - å·²åˆ›å»º {len(summary_docs)} ä¸ªæ‘˜è¦ã€‚")

    return child_chunks, summary_docs


def create_and_persist_vector_store(docs_to_index, embeddings):
    """åˆ›å»ºå¹¶æŒä¹…åŒ–å‘é‡åº“ã€‚"""
    print("\n--- æ­¥éª¤ä¸‰: åµŒå…¥æ•°æ®å¹¶æ„å»ºå‘é‡åº“ ---")
    if os.path.exists(VECTOR_DB_PATH):
        print(f"  - å‘ç°æ—§çš„æ•°æ®åº“ï¼Œæ­£åœ¨åˆ é™¤: {VECTOR_DB_PATH}")
        shutil.rmtree(VECTOR_DB_PATH)

    Chroma.from_documents(docs_to_index, embeddings, persist_directory=VECTOR_DB_PATH)
    print("  - å‘é‡æ•°æ®åº“åˆ›å»ºå®Œæ¯•å¹¶å·²æˆåŠŸæŒä¹…åŒ–ï¼")


def verify_retrieval(embeddings):
    """éªŒè¯æ£€ç´¢æ•ˆæœã€‚"""
    print("\n--- æ­¥éª¤å››: éªŒè¯æ£€ç´¢æ•ˆæœ ---")
    if not os.path.exists(VECTOR_DB_PATH):
        print("æ•°æ®åº“ä¸å­˜åœ¨ï¼Œæ— æ³•éªŒè¯ã€‚")
        return

    db = Chroma(persist_directory=VECTOR_DB_PATH, embedding_function=embeddings)

    # éªŒè¯ä¸€ä¸ªæ¥è‡ªéŸ³é¢‘è½¬å½•ç¨¿çš„æŸ¥è¯¢
    query = "are there other that have yet to be announced of the same kind of scale and magnitude?"
    print(f"  - æ¨¡æ‹ŸéŸ³é¢‘å†…å®¹æŸ¥è¯¢: '{query}'")

    retrieved_chunks = db.similarity_search(
        query,
        k=3,
        filter={"is_child": True}
    )

    print("\n  - æ£€ç´¢åˆ°çš„å‰3ä¸ªç²¾ç¡®å—ä¸ºï¼š\n")
    if not retrieved_chunks:
        print("    æœªèƒ½æ£€ç´¢åˆ°ä»»ä½•ç›¸å…³å†…å®¹ã€‚")
    else:
        for doc in retrieved_chunks:
            print("    å†…å®¹ç‰‡æ®µ: {}...".format(doc.page_content[:150].replace("\n", " ")))
            print(f"      å…ƒæ•°æ®: {doc.metadata}")
            print("      " + "-" * 20)


if __name__ == "__main__":
    # 1. åŠ è½½å¹¶å‡†å¤‡çˆ¶æ–‡æ¡£
    parent_documents = load_and_prepare_docs()

    if parent_documents:
        # 2. åˆ›å»ºå­å—å’Œæ‘˜è¦
        child_chunks, summary_docs = create_child_chunks_and_summaries(parent_documents)
        all_docs_to_index = child_chunks + summary_docs

        # 3. åµŒå…¥ä¸å­˜å‚¨
        print("\næ­£åœ¨åˆå§‹åŒ–åµŒå…¥æ¨¡å‹...")
        embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL_NAME,
            model_kwargs={"device": "cuda"},
            encode_kwargs={"normalize_embeddings": True}
        )
        create_and_persist_vector_store(all_docs_to_index, embeddings)

        # 4. éªŒè¯
        verify_retrieval(embeddings)
        print(f"\nğŸ‰ æ­å–œï¼æ•°æ®å¤„ç†ä¸ç´¢å¼•æµç¨‹å·²å…¨éƒ¨ä¿®å¤å¹¶å®Œæˆï¼")
    else:
        print("æœªèƒ½åŠ è½½ä»»ä½•æ–‡æ¡£ï¼Œæµç¨‹ç»ˆæ­¢ã€‚")