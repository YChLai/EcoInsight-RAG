import os
import json
import streamlit as st
from typing import List, Tuple
from langchain_core.documents import Document
import retriever
import config

DATA_DIR = "../processed_data"

@st.cache_resource
def init():
    retriever.get_llm()
    retriever.get_embeddings()
    retriever.get_db()

@st.cache_data
def load_data():
    data = {}
    for f in os.listdir(DATA_DIR):
        path = os.path.join(DATA_DIR, f)
        if f.endswith(".json"):
            data[f.replace(".json", "")] = {'type': 'json', 'content': json.load(open(path, 'r', encoding='utf-8'))}
        elif f.endswith(".txt"):
            data[f.replace(".txt", "")] = {'type': 'txt', 'content': open(path, 'r', encoding='utf-8').read()}
    return data

def rephrase(question, history):
    if not history:
        return question
    formatted = "\n".join([f"ç”¨æˆ·: {u}\nåŠ©æ‰‹: {a}" for u, a in history])
    client = retriever.get_llm()
    prompt = config.REPHRASE_PROMPT.format(history=formatted, q=question)
    resp = client.chat.completions.create(model=config.LLM_MODEL, messages=[{"role": "user", "content": prompt}])
    result = resp.choices[0].message.content.strip()
    st.info(f"é‡æ„å: {result}")
    return result

def find_content(data, source, page):
    src = source.replace(".pdf", "").replace(".json", "")
    if src not in data:
        return ""
    d = data[src]
    if d['type'] == 'txt':
        return d['content']
    return "\n\n".join([b['content'] for b in d['content'] if b['metadata'].get('page') == page])

def build_context(docs):
    context, sources = "", []
    data = load_data()
    for doc in docs:
        src, page = doc.metadata.get('source', 'æœªçŸ¥'), doc.metadata.get('page', 'æœªçŸ¥')
        content = find_content(data, src, page)
        if content:
            context += f"--- æ¥æº: {src}, ç¬¬ {page} é¡µ ---\n{content}\n\n"
            sources.append({"source": src, "page": page})
    return context.strip(), sources

def stream_answer(query, context):
    if not context:
        yield "æ ¹æ®æä¾›çš„èµ„æ–™ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚"
        return
    client = retriever.get_llm()
    prompt = config.ANSWER_PROMPT.format(ctx=context, q=query)
    resp = client.chat.completions.create(model=config.LLM_MODEL, messages=[{"role": "user", "content": prompt}], stream=True)
    for chunk in resp:
        yield chunk.choices[0].delta.content or ""

st.set_page_config(page_title="ç¯å¢ƒæ•°æ®æ™ºèƒ½åˆ†æåŠ©æ‰‹", page_icon="ğŸŒ±")
st.title("ğŸŒ± ç¯å¢ƒæ•°æ®æ™ºèƒ½åˆ†æåŠ©æ‰‹")

init()

if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.history = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

if prompt := st.chat_input("è¯·è¾“å…¥é—®é¢˜..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        q = rephrase(prompt, st.session_state.history)
        with st.spinner("æ£€ç´¢ä¸­..."):
            docs = retriever.rerank(retriever.search(retriever.expand_query(q)))
        
        if not docs:
            resp = "æœªèƒ½æ£€ç´¢åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
            st.warning(resp)
        else:
            context, sources = build_context(docs[:3])
            with st.sidebar:
                st.subheader("å¼•ç”¨æ¥æº")
                for s in sources:
                    st.markdown(f"- {s['source']}, ç¬¬ {s['page']} é¡µ")
            resp = st.write_stream(stream_answer(q, context))
    
    st.session_state.messages.append({"role": "assistant", "content": resp})
    st.session_state.history.append((prompt, resp))

with st.sidebar:
    st.header("è¯´æ˜")
    st.markdown("åŸºäºRAGçš„ç¯å¢ƒæ•°æ®é—®ç­”ç³»ç»Ÿï¼Œæ”¯æŒå¤šè½®å¯¹è¯ã€‚")
