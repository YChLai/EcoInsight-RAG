[
    {
        "question": "英伟达在2026财年第一季度的总收入是多少？",
        "ground_truth": "截至 2025 年 4 月 27 日的 2026 财年第一季度，总收入（Revenue）为 440.62 亿美元，同比增长 69%。",
        "answer": "英伟达在2026财年第一季度的总收入为440.62亿美元 [来源: Q1_2026_presentation, 第 3 页]。",
        "contexts": [
            "[0s->5s]Good afternoon. My name is Sarah and I will be your conference operator today.\n[5s->13s]At this time, I would like to welcome everyone to NVIDIA's first quarter fiscal 2026 financial results conference call.\n[13s->17s]All lines have been placed on mute to prevent any background noise.\n[17s->21s]After the speaker's remarks, there will be a question and answer session.\n[21s->27s]If you would like to ask a question during this time, simply press star one on your telephone keypad.\n[27s->32s]If you would like to withdraw your question, please press star one again. Thank you.\n[32s->36s]Toshia Hari, you may begin your conference.\n[36s->44s]Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the first quarter of fiscal 2026.\n[44s->54s]With me today from NVIDIA are Jensen Wong, President and Chief Executive Officer, and Collette Press, Executive Vice President and Chief Financial Officer.\n[54s->60s]I'd like to remind you that our call is being webcast live on NVIDIA's investor relations website.\n[60s->68s]The webcast will be available for replay until the conference call to discuss our financial results for the second quarter of fiscal 2026.\n[68s->76s]The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent.\n[76s->81s]During this call, we may make forward looking statements based on current expectations.\n[81s->88s]These are subject to a number of significant risks and uncertainties, and our actual results may differ materially.\n[88s->96s]For a discussion of factors that could affect our future financial results in business, please refer to the disclosure in today's earnings release,\n[96s->104s]our most recent forms 10-K and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission.\n[105s->112s]All our statements are made as of today, May 28, 2025, based on information currently available to us.\n[112s->118s]Except as required by law, we assume no obligation to update any such statements.\n[118s->122s]During this call, we will discuss non-GAAP financial measures.\n[122s->130s]You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website.\n[130s->133s]With that, let me turn the call over to Collette.\n[133s->135s]Thank you, Toshio.\n[135s->148s]We delivered another strong quarter with revenue of $44 billion, up 69% year-over-year, exceeding our outlook in what proved to be a challenging operating environment.\n[148s->153s]Data center revenue of $39 billion grew 73% year-on-year.\n[153s->161s]AI workloads have transitioned strongly to inference, and AI factory build-outs are driving significant revenue.\n[161s->165s]Our customers' commitments are firm.\n[165s->176s]On April 9, the U.S. government issued new export controls on H20, our data center GPU designed specifically for the China market.\n[176s->181s]We sold H20 with the approval of the previous administration.\n[181s->187s]Although our H20 has been in the market for over a year and does not have a market outside of China,\n[187s->195s]the new export controls on H20 did not provide a grace period to allow us to sell through our inventory.\n[195s->204s]In Q1, we recognized $4.6 billion in H20 revenue, which occurred prior to April 9,\n[204s->216s]but also recognized a $4.5 billion charge as we wrote down inventory and purchase obligations tied to orders we had received prior to April 9.\n[216s->226s]We were unable to ship $2.5 billion in H20 revenue in the first quarter due to the new export controls.\n[226s->234s]The $4.5 billion charge was less than what we initially anticipated as we were able to reuse certain materials.\n[234s->244s]We are still evaluating our limited options to supply data center compute products compliant with the U.S. government's revised export control rules,\n[244s->252s]losing access to the China AI accelerator market, which we believe will grow to nearly $50 billion,\n[252s->262s]would have a material adverse impact on our business going forward and benefit our foreign competitors in China and worldwide.\n[262s->271s]Our Blackwell ramp, the fastest in our company's history, drove a 73% year-on-year increase in data center revenue.\n[271s->280s]Blackwell contributed nearly 70% of data center compute revenue in the quarter, with a transition from Hopper nearly complete.\n[280s->293s]The introduction of GB200 NVL was a fundamental architectural change to enable data center scale workloads and to achieve the lowest cost per inference token.\n[293s->305s]While these systems are complex to build, we have seen a significant improvement in manufacturing yields and rack shipments are moving to strong rates to end customers.\n[305s->315s]GB200 NVL racks are now generally available for modern builders, enterprises, and sovereign customers to develop and deploy AI.\n[315s->332s]On average, major hyperscalers are each deploying nearly 1,000 NVL 72 racks or 72,000 Blackwell GPUs per week and are on track to further ramp output this quarter.\n[332s->346s]Microsoft, for example, has already deployed tens of thousands of Blackwell GPUs and is expected to ramp to hundreds of thousands of GB200s with OpenAI as one of its key customers.\n[346s->356s]Key learnings from the GB200 ramp will allow for a smooth transition to the next phase of our product roadmap, Blackwell Ultra.\n[357s->370s]Sampling of GB300 systems began earlier this month at the major CSPs, and we expect production shipments to commerce later this quarter.\n[370s->381s]GB300 will leverage the same architecture, same physical footprint, and the same electrical and mechanical specifications as GB200.\n[381s->394s]The GB300 drop-in design will allow CSPs to seamlessly transition their systems and manufacturing used for GB200 while maintaining high yields.\n[394s->407s]B300 GPUs with 50% more HBM will deliver another 50% increase in dense FP4 inference compute performance compared to the B200.\n[407s->421s]We remain committed to our annual product cadence, with our roadmap extending through 2028 tightly aligned with the multiple-year planning cycles of our customers.\n[421s->432s]We are witnessing a sharp jump in inference demand. OpenAI, Microsoft, and Google are seeing a step-function leap in token generation.\n[432s->441s]Microsoft processed over 100 trillion tokens in Q1, a five-fold increase on a year-over-year basis.\n[441s->453s]This exponential growth in Azure OpenAI is representative of strong demand for Azure AI Foundry as well as other AI services across Microsoft's platform.\n[453s->471s]Inference-serving startups are now serving models using B200, tripling their token generation rate and corresponding revenues for high-value reasoning models such as DeepSeq R1, as reported by Artificial Analysis.\n[471s->485s]NVIDIA Dynamo on Blackwell NVL72 turbocharges AI inference throughput by 30x for the new reasoning models sweeping the industry.\n[485s->497s]Developer engagements increased with adoption ranging from LLM providers such as Perplexity to financial services institutions such as Capital One,\n[497s->503s]who reduced agentic chatbox latency by 5x with Dynamo.\n[503s->511s]In the latest MLPerf inference results, we submitted our first results using GB200 NVL72,\n[511s->523s]delivering up to 30x higher inference throughput compared to our 8 GPU H200 submission on the challenging Llama 3.1 benchmark.\n[523s->535s]This feat was achieved through a combination of tripling the performance per GPU as well as 9x more GPUs, all connected on a single NVLink domain.\n[535s->547s]And while Blackwell is still early in its lifecycle, software optimizations have already improved its performance by 1.5x in the last month alone.\n[547s->555s]We expect to continue improving the performance of Blackwell through its operational life as we have done with Hopper and Amper.\n[555s->560s]For example, we increased the inference performance of Hopper by 4x over two years.\n[560s->568s]This is the benefit of NVIDIA's programmable CUDA architecture and rich ecosystem.\n[568s->578s]The pace and scale of AI factory deployments are accelerating with nearly 100 NVIDIA-powered AI factories in flight this quarter,\n[578s->588s]a two-fold increase year over year, with the average number of GPUs powering each factory also doubling in the same period.\n[588s->593s]And more AI factory projects are starting across industries and geographies.\n[593s->605s]NVIDIA's full-stack architecture is underpinning AI factory deployments as industry leaders like AT&T, BYD, Capital One, Foxconn, MediaTek, and Telenor\n[605s->614s]are strategically vital sovereign clouds like those recently announced in Saudi Arabia, Taiwan, and the UAE.\n[614s->623s]We have a line of sight to projects requiring tens of gigawatts of NVIDIA AI infrastructure in the not-too-distant future.\n[623s->639s]The transition from generative to agentic AI, AI capable of perceiving, reasoning, planning, and acting, will transform every industry, every company, and country.\n[639s->650s]We envision AI agents as a new digital workforce capable of handling tasks ranging from customer service to complex decision-making processes.\n[650s->660s]We introduced the LAMA Nemotron family of open reasoning models designed to supercharge identical AI platforms for enterprises.\n[660s->672s]Built on the LAMA architecture, these models are available as NIMS or NVIDIA inference microservices with multiple sizes to meet diverse deployment needs.\n[672s->681s]Our post-training enhancements have yielded a 20% accuracy boost and a 5x increase in inference speed.\n[681s->690s]Leading platform companies including Accenture, Cadence, Deloitte, and Microsoft are transforming work with our reasoning models.\n[690s->702s]NVIDIA NEMO microservices are generally available across industries that are being leveraged by leading enterprises to build, optimize, and scale AI applications.\n[702s->711s]With NEMO, Cisco increased model accuracy by 40% and improved response time by 10x in its code assistant.\n[711s->719s]NASDAQ realized a 30% improvement in accuracy and response time in its AI platform's search capabilities.\n[719s->727s]And Shell's Custom LLM achieved a 30% increase in accuracy when trained with NVIDIA NEMO.\n[727s->735s]NEMO's parallelism techniques accelerated model training time by 20% when compared to other frameworks.\n[735s->757s]We also announced a partnership with Yum Brands, the world's largest restaurant company, to bring NVIDIA AI to 500 of its restaurants this year and expanding to 61,000 restaurants over time to streamline order taking, optimize operations, and enhance service across its restaurants.\n[757s->779s]For AI-powered cybersecurity, leading companies like Checkpoint, CloudStrike, and Palo Alto Networks are using NVIDIA's AI security and software stack to build, optimize, and secure agentic workflows, with CloudStrike realizing 2x faster detection triage with 50% less compute cost.\n[780s->790s]Moving to networking, sequential growth in networking resumed in Q1, with revenue up 64% quarter-over-quarter to $5 billion.\n[790s->798s]Our customers continue to leverage our platform to efficiently scale up and scale out AI factory workloads.\n[798s->813s]We created the world's fastest switch, NVLink. For scale up, our NVLink compute fabric in its fifth generation offers 14x the bandwidth of PCIe Gen 5.\n[813s->825s]NVLink 72 carries 130 TB per second of bandwidth in a single rack, equivalent to the entirety of the world's peak internet traffic.\n[825s->833s]NVLink is a new growth vector and is off to a great start, with Q1 shipments exceeding $1 billion.\n[833s->848s]At Computex, we announced NVLink Fusion. Hyperscale customers can now build semi-custom CCUs and accelerators that connect directly to the NVIDIA platform with NVLink.\n[848s->868s]We are now enabling key partners, including ASIC providers such as MediaTek, Marvell, Alchip Technologies, and Astera Labs, as well as CPU suppliers such as Fujitsu and Qualcomm, to leverage NVLink Fusion to connect our respective ecosystems.\n[868s->878s]For scale out, our enhanced Ethernet offerings deliver the highest throughput, lowest latency networking for AI.\n[878s->886s]SpectrumX posted strong sequential and year-on-year growth and is now annualizing over $8 billion in revenue.\n[886s->896s]Adoption is widespread across major CSPs and consumer internet companies, including CoreWe, Microsoft Azure, Oracle Cloud, and XAI.\n[896s->903s]This quarter, we added Google Cloud and Meta to the growing list of SpectrumX customers.\n[903s->913s]We introduced SpectrumX and QuantumX, silicon photonics switches featuring the world's most advanced co-packaged optics.\n[913s->933s]These platforms will enable next level AI factory scaling to millions of GPUs through the increasingly power efficiency by 3.5x and network resiliency by 10x while accelerating customer time to market by 1.3x.\n[933s->938s]Transitioning to a quick summary of our revenue by geography.\n[938s->949s]China, as a percentage of our data center revenue, was slightly below our expectations and down sequentially due to H20 export licensing controls.\n[949s->955s]For Q2, we expect a meaningful decrease in China data center revenue.\n[955s->971s]As a reminder, while Singapore represented nearly 20% of our Q1 billed revenue, as many of our large customers use Singapore for centralized invoicing, our products are almost always sold elsewhere.\n[971s->984s]Note that over 99% of H100, H200, and Blackwell data center compute revenue billed to Singapore was for orders from US based customers.\n[984s->996s]Moving to gaming and AI PCs. Gaming revenue was a record 3.8 billion, increasing 48% sequentially and 42% year on year.\n[996s->1012s]Strong adoption by gamers, creatives, and AI enthusiasts have made Blackwell our fastest ramp ever. Against a backdrop of robust demand, we greatly improved our supply and availability in Q1 and expect to continue these efforts in Q2.\n[1012s->1017s]AI is transforming PC and creator and gamers.\n[1017s->1035s]With a 100 million user installed base, GeForce represents the largest footprint for PC developers. This quarter, we added to our AI PC laptop offerings, including models capable of running Microsoft's Copilot Plus.\n[1035s->1049s]This past quarter, we brought Blackwell architecture to mainstream gaming with its launch of GeForce RTX 5060 and 5060 Ti, starting at just $299.\n[1049s->1061s]The RTX 5060 also debuted in laptops starting at $1,099. These systems that double the frame rate and slash latency.\n[1061s->1070s]These GeForce RTX 5060 and 5060 Ti desktop GPUs and laptops are now available.\n[1070s->1090s]In console gaming, the recently unveiled Nintendo Switch 2 leverages Nvidia's neural rendering and AI technologies, including next generation custom RTX GPUs with DLSS technology, to deliver a giant leap in gaming performance to millions of players worldwide.\n[1090s->1100s]Nintendo has shipped over 150 million Switch consoles to date, making it one of the most successful gaming systems in history.\n[1100s->1108s]Moving to pro visualization. Revenue of $509 million was flat sequentially and up 19% year on year.\n[1108s->1120s]Tariff related uncertainty temporarily impacted Q1 systems and demand for AI workstations is strong and we expect sequential revenue growth to resume in Q2.\n[1120s->1130s]Nvidia DGX Spark and Station revolutionize personal computing by putting the power of an AI supercomputer in a desktop form factor.\n[1130s->1143s]DGX Spark delivers up to one petaflop of AI compute while DGX Station offers an incredible 20 petaflops and is powered by the GB300 Superchip.\n[1143s->1150s]DGX Spark will be available in Calendar Q3 and DGX Station later this year.\n[1150s->1163s]We have deepened Omniverse's integration and adoption into some of the world's leading software platforms, including Databricks, SAP, and Schneider Electric.\n[1163s->1178s]New Omniverse blueprints such as Mega for at scale robotic fleet management are being leveraged in Keyon Group, Pegatron, Accenture, and other leading companies to enhance industrial operations.\n[1178s->1189s]At Confitex, we showcased Omniverse's great traction with technology manufacturing leaders, including TSMC, Quanta, Foxconn, Pegatron.\n[1189s->1194s]Using Omniverse, TSMC saves months in work by designing fabs virtually.\n[1194s->1205s]Foxconn accelerates thermal simulations by 150x and Pegatron reduced assembly line defects rates by 67%.\n[1205s->1214s]Lastly, with our automotive group, revenue was $567 million, down 1% sequentially but up 72% year on year.\n[1214s->1222s]Year on year growth was driven by the ramp of self-driving across a number of customers and robust end demand for NEVs.\n[1222s->1232s]We are partnering with GM to build the next-gen vehicles, factories, and robots using NVIDIA AI, simulation, and accelerated computing.\n[1232s->1241s]And we are now in production with our full-stack solution for Mercedes-Benz, starting with the new CLA, hitting roads in the next few months.\n[1241s->1254s]We announced Isaac, Groot, and won the world's first open fully customizable foundation model for humanoid robots, enabling generalized reasoning and skill development.\n[1254s->1260s]We also launched new open NVIDIA Cosmo World Foundation models.\n[1260s->1268s]Leading companies include 1X, Agility Robots, Robotics, Figure AI, Uber, and Wabi.\n[1268s->1284s]We've begun integrating Cosmos into their operations for synthetic data generation, while Agility Robotics, Boston Dynamics, and XPen Robotics are harnessing Isaac simulation to advance their humanoid efforts.\n[1284s->1300s]GE Healthcare is using the new NVIDIA Isaac platform for healthcare simulation, built on NVIDIA Omniverse and using NVIDIA Cosmos, the platform speeds development of robotic imaging and surgery systems.\n[1300s->1314s]The era of robotics is here. Billions of robots, hundreds of millions of autonomous vehicles, and hundreds of thousands of robotic factories and warehouses will be developed.\n[1314s->1335s]All right, moving to the rest of the P&L. Gap gross margins and non-gap gross margins were 60.5% and 61%, respectively. Excluding the $4.5 billion charge, Q1 non-gap gross margins would have been 71.3%, slightly above our outlook at the beginning of the quarter.\n[1335s->1346s]Sequentially, gap operating expenses were up 7% and non-gap operating expenses were up 6%, reflecting higher compensation and employee growth.\n[1346s->1355s]Our investments include expanding our infrastructure capabilities and AI solutions, and we plan to grow these investments throughout the fiscal year.\n[1355s->1370s]In Q1, we returned a record $14.3 billion to shareholders in the form of share repurchases and cash dividends. Our capital return program continues to be a key element of our capital allocation strategy.\n[1370s->1384s]Let me turn to the outlook for the second quarter. Total revenue is expected to be $45 billion, plus or minus 2%. We expect modest sequential growth across all of our platforms.\n[1384s->1403s]In data center, we anticipate the continued ramp of Blackwell to be partially offset by a decline in China revenue. Note, our outlook reflects a loss in H20 revenue of approximately $8 billion for the second quarter.\n[1403s->1420s]Gap and non-gap gross margins are expected to be 71.8% and 72% respectively, plus or minus 50 basis points. We expect better Blackwell profitability to drive modest sequential improvement in gross margins.\n[1420s->1427s]We are continuing to work towards achieving gross margins in the mid-70s range late this year.\n[1427s->1444s]Gap and non-gap operating expenses are expected to be approximately $5.7 billion and $4 billion respectively, and we continue to expect full-year fiscal year 26 operating expense growth to be in the mid-30% range.\n[1444s->1457s]Gap and non-gap other income and expenses are expected to be an income of approximately $450 million, excluding gays and losses from non-marketable and publicly held equity securities.\n[1457s->1468s]Gap and non-gap tax rates are expected to be 16.5%, plus or minus 1%, excluding any discrete items.\n[1468s->1480s]Further financial details are included in the CFO commentary and other information available on our IR website, including a new financially information AI agent.\n[1480s->1488s]Let me highlight upcoming events for the financial community. We will be at the B of A Global Technology Conference in San Francisco on June 4.\n[1488s->1500s]The Rosenblatt Virtual AI Summit and NASDAQ Investor Conference in London on June 10, and GTC Paris at Viva Tech on June 11 in Paris.\n[1500s->1511s]We look forward to seeing you at these events. Our earnings call to discuss the results of our second quarter of fiscal 2026 is scheduled for August 27.\n[1511s->1517s]Well now, let me turn it over to Jensen to make some remarks.\n[1517s->1526s]Thanks, Collette. We've had a busy and productive year. Let me share my perspective on some topics we're frequently asked.\n[1526s->1536s]On export control, China is one of the world's largest AI markets and a springboard to global success.\n[1536s->1543s]With half of the world's AI researchers based there, the platform that wins China is positioned to lead globally.\n[1543s->1551s]Today, however, the $50 billion China market is effectively closed to U.S. industry.\n[1551s->1560s]The H20 export ban ended our Hopper data center business in China. We cannot reduce Hopper further to comply.\n[1561s->1568s]As a result, we are taking a multi-billion dollar write-off on inventory that cannot be sold or repurposed.\n[1568s->1575s]We are exploring limited ways to compete, but Hopper is no longer an option.\n[1575s->1584s]China's AI moves on with or without U.S. chips. It has to compute to train and deploy advanced models.\n[1585s->1590s]The question is not whether China will have AI. It already does.\n[1590s->1596s]The question is whether one of the world's largest AI markets will run on American platforms.\n[1596s->1604s]Shielding Chinese chipmakers from U.S. competition only strengthens them abroad and weakens America's position.\n[1604s->1609s]Export restrictions have spurred China's innovation in scale.\n[1609s->1616s]The AI race is not just about chips. It's about which stack the world runs on.\n[1616s->1624s]As that stack grows to include 6G and quantum, U.S. global infrastructure leadership is at stake.\n[1624s->1630s]The U.S. has based its policy on the assumption that China cannot make AI chips.\n[1630s->1635s]That assumption was always questionable, and now it's clearly wrong.\n[1635s->1639s]China has enormous manufacturing capability.\n[1639s->1645s]In the end, the platform that wins the AI developers wins AI.\n[1645s->1653s]Export controls should strengthen U.S. platforms, not drive half of the world's AI talent to rivals.\n[1654s->1657s]On DeepSeek.\n[1657s->1665s]DeepSeek and QN from China are among the best open source AI models.\n[1665s->1671s]Released freely, they've gained traction across the U.S., Europe, and beyond.\n[1671s->1680s]DeepSeek R1, like ChatGPT, introduced reasoning AI that produces better answers the longer it thinks.\n[1680s->1690s]Reasoning AI enables step-by-step problem solving, planning, and tool use, turning models into intelligent agents.\n[1690s->1705s]Reasoning is compute intensive, requires hundreds to thousands of times more tokens per task than previous one-shot inference.\n[1705s->1711s]Reasoning models are driving a step function surge in inference demand.\n[1711s->1721s]AI scaling laws remain firmly intact, not only for training, but now inference, too, requires massive scale compute.\n[1721s->1726s]DeepSeek also underscores the strategic value of open source AI.\n[1726s->1734s]When popular models are trained and optimized on U.S. platforms, it drives usage, feedback, and continuous improvement.\n[1734s->1737s]Reinforcing American leadership across the stack.\n[1737s->1743s]U.S. platforms must remain the preferred platform for open source AI.\n[1743s->1749s]That means supporting collaboration with top developers globally, including in China.\n[1749s->1757s]America wins when models like DeepSeek and QN runs best on American infrastructure.\n[1757s->1760s]Regarding onshore manufacturing.\n[1760s->1770s]President Trump has outlined a bold vision to reshore advanced manufacturing, create jobs, and strengthen national security.\n[1770s->1774s]Future plants will be highly computerized in robotics.\n[1774s->1776s]We share this vision.\n[1776s->1783s]TSMC is building six fabs and two advanced packaging plants in Arizona to make chips for Nvidia.\n[1783s->1789s]Process qualification is underway, with volume production expected by year end.\n[1790s->1798s]Spill and Amcor are also investing in Arizona, constructing packaging, assembly, and test facilities.\n[1798s->1806s]In Houston, we're partnering with Foxconn to construct a million-square-foot factory to build AI supercomputers.\n[1806s->1811s]Wistron is building a similar plant in Fort Worth, Texas.\n[1811s->1817s]To encourage and support these investments, we've made substantial long-term purchase commitments.\n[1817s->1821s]A deep investment in America's AI manufacturing future.\n[1823s->1827s]Our goal, from chip to supercomputer.\n[1827s->1830s]Built in America within a year.\n[1830s->1840s]Each GB200 and Beelink 72 racks contains 1.2 million components and weighs nearly two tons.\n[1840s->1844s]No one has produced supercomputers on this scale.\n[1844s->1847s]Our partners are doing an extraordinary job.\n[1849s->1857s]On AI diffusion rule, President Trump rescinded the AI diffusion rule, calling it counterproductive,\n[1857s->1864s]and proposed a new policy to promote US AI tech with trusted partners.\n[1864s->1868s]On his Middle East tour, he announced historic investments.\n[1868s->1875s]I was honored to join him in announcing a 500-megawatt AI infrastructure project in Saudi Arabia\n[1875s->1880s]and a 5-gigawatt AI campus in the UAE.\n[1880s->1883s]President Trump wants US tech to lead.\n[1883s->1887s]The deals he announced are wins for America.\n[1887s->1895s]Creating jobs, advancing infrastructure, generating tax revenue, and reducing the US trade deficit.\n[1895s->1902s]The US will always be Nvidia's largest market and home to the largest install base of our infrastructure.\n[1902s->1909s]Every nation now sees AI as core to the next industrial revolution.\n[1909s->1915s]A new industry that produces intelligence and essential infrastructure for every economy.\n[1915s->1922s]Countries are racing to build national AI platforms to elevate their digital capabilities.\n[1922s->1930s]At Computex, we announced Taiwan's first AI factory in partnership with Foxconn and the Taiwan government.\n[1930s->1936s]Last week, I was in Sweden to launch its first national AI infrastructure.\n[1936s->1947s]Japan, Korea, India, Canada, France, the UK, Germany, Italy, Spain, and more are now building national AI factories\n[1947s->1951s]to empower startups, industries, and societies.\n[1951s->1956s]Sovereign AI is a new growth engine for Nvidia.\n[1958s->1960s]Toshia, back to you. Thank you.\n[1960s->1966s]Operator, we will now open the call for questions. Would you please pull for questions?\n[1966s->1974s]Thank you. At this time, I would like to remind everyone in order to ask a question, press star, then the number one on your telephone keypad.\n[1974s->1978s]We'll pause for just a moment to compile the Q&A roster.\n[1982s->1988s]Your first question comes from the line of Joe Moore with Morgan Stanley. Your line is open.\n[1990s->1998s]Great. Thank you. You guys have talked about this scaling up of inference around reasoning models for at least a year now,\n[1998s->2002s]and we've really seen that come to fruition as you talked about. We've heard it from your customers.\n[2002s->2007s]Can you give us a sense for how much of that demand you're able to serve?\n[2007s->2010s]And give us a sense for maybe how big the inference business is for you guys.\n[2010s->2016s]And do we need full-on NVL-72 rack-scale solutions for reasoning inference going forward?\n[2019s->2026s]Well, we would like to serve all of it. And I think we're on track to serve most of it.\n[2026s->2038s]Grace Blackwell NVLink-72 is the ideal engine today, the ideal computer thinking machine, if you will, for reasoning AI.\n[2038s->2046s]There's a couple of reasons for that. The first reason is that the token generation amount,\n[2046s->2059s]the number of tokens reasoning goes through is a hundred, a thousand times more than a one-shot chatbot.\n[2059s->2069s]You know, it's essentially thinking to itself, breaking down a problem step by step. It might be planning multiple paths to an answer.\n[2069s->2083s]It could be using tools, reading PDFs, reading web pages, watching videos, and then producing a result, an answer.\n[2083s->2089s]The longer it thinks, the better the answer, the smarter the answer is.\n[2090s->2101s]And so what we would like to do and the reason why Grace Blackwell was designed to give such a giant step up in inference performance\n[2101s->2106s]is so that you could do all this and still get a response as quickly as possible.\n[2106s->2117s]Compared to Hopper, Grace Blackwell is some 40 times higher speed and throughput compared.\n[2117s->2132s]And so this is going to be a huge, huge benefit and driving down the cost while improving the quality of response with excellent quality of service at the same time.\n[2132s->2139s]So that's the fundamental reason. That was the core driving reason for Grace Blackwell NVLink-72.\n[2139s->2151s]Of course, in order to do that, we had to reinvent, literally redesign the entire way that these supercomputers are built.\n[2151s->2161s]But now we're in full production. It's going to be exciting. It's going to be incredibly exciting.\n[2162s->2171s]The next question comes from Vivek Arya with Bank of America Securities. Your line is open.\n[2171s->2176s]Thanks for the question. Just a clarification for Colette first.\n[2176s->2184s]So on the China impact, I think previously it was mentioned that at about $15 billion, so you had the $8 billion in Q2.\n[2184s->2190s]So is there still some left as a headwind for the remaining quarters, just Colette, how to model that?\n[2190s->2201s]And then question, Jensen, for you. Back at GTC, you had outlined a path towards almost a trillion dollars of AI spending over the next few years.\n[2201s->2212s]Where are we in that build out? And do you think it's going to be uniform that you will see every spender, whether it's ESP, sovereigns, enterprises or build out?\n[2212s->2215s]Should we expect some periods of digestion in between?\n[2215s->2221s]What are your customer discussions telling you about how to model growth for next year?\n[2223s->2232s]Yes, Vivek. Thanks so much for the question regarding H20. Yes, we recognized 4.6 H20 in Q1.\n[2232s->2241s]We were unable to ship $2.5 billion. So the total for Q1 should have been $7 billion.\n[2241s->2248s]When we look at our Q2, our Q2 is going to be meaningfully down in terms of China data center revenue.\n[2248s->2258s]And we had highlighted in terms of the amount of orders that we had planned for H20 and Q2, and that was $8 billion.\n[2258s->2266s]Now, going forward, we did have other orders going forward that we will not be able to fulfill.\n[2266s->2275s]That is what was incorporated, therefore, in the amount that we wrote down of the $4.5 billion.\n[2275s->2280s]That write down was about inventory and purchase commitments.\n[2280s->2287s]And our purchase commitments were about what we expected regarding the orders that we had received.\n[2287s->2295s]Going forward, though, it's a bigger issue regarding the amount of the market that we will not be able to serve.\n[2295s->2306s]We assess that TAM to be close to about $50 billion in the future as we don't have a product to enable for the China.\n[2310s->2318s]Vivek, probably the best way to think through it is that AI is several things.\n[2318s->2326s]Of course, we know that AI is this incredible technology that's going to transform every industry,\n[2326s->2344s]from, of course, the way we do software to health care and financial services to retail to, I guess, every industry, transportation, manufacturing.\n[2344s->2347s]And we're at the beginning of that.\n[2347s->2352s]But maybe another way to think about that is where do we need intelligence?\n[2352s->2354s]Where do we need digital intelligence?\n[2354s->2358s]And it's in every country, it's in every industry.\n[2358s->2365s]And we know because of that, we recognize that AI is also an infrastructure.\n[2365s->2371s]It's a way of delivering a technology that requires factories.\n[2371s->2374s]And these factories produce tokens.\n[2374s->2380s]And they, as I mentioned, are important to every single industry in every single country.\n[2380s->2389s]And so on that basis, we're really at the very beginning of it because the adoption of this technology is really kind of in its early stages.\n[2389s->2399s]Now, we've reached an extraordinary milestone with AIs that are reasoning, are thinking, what people call inference time scaling.\n[2399s->2413s]And of course, it created a whole new, we've entered an era where inference is going to be a significant part of the compute workload.\n[2413s->2417s]But anyhow, it's going to be a new infrastructure.\n[2417s->2421s]And we're building it out in the clouds.\n[2421s->2429s]The United States is really the early starter and available in US clouds.\n[2429s->2432s]And this is our largest market, our largest installed base.\n[2432s->2435s]And we can continue to see that happening.\n[2435s->2442s]But beyond that, we're going to have to, we're going to see AI go into enterprise, which is on prem.\n[2442s->2447s]Because so much of the data is still on prem, access control is really important.\n[2447s->2452s]It's really hard to move all of every company's data into the cloud.\n[2452s->2455s]And so we're going to move AI into the enterprise.\n[2455s->2460s]And you saw that we announced a couple of really exciting new products.\n[2460s->2468s]Our RTX Pro Enterprise AI server that runs everything enterprise and AI.\n[2468s->2476s]Our DGX Spark and DGX Station, which is designed for developers who want to work on prem.\n[2476s->2481s]And so enterprise AI is just taking off.\n[2481s->2490s]Telcos, today, a lot of the telco infrastructure will be in the future software defined and built on AI.\n[2490s->2492s]And so 6G is going to be built on AI.\n[2492s->2495s]And that infrastructure needs to be built out.\n[2495s->2498s]And they said it's very, very early stages.\n[2498s->2505s]And then, of course, every factory today that makes things will have an AI factory that sits with it.\n[2505s->2513s]And the AI factory is going to be drive creating AI and operating AI for the factory itself.\n[2513s->2518s]But also to power the products and the things that are made by the factory.\n[2518s->2523s]So it's very clear that every car company will have AI factories.\n[2523s->2527s]And very soon there'll be robotics companies, robot companies.\n[2527s->2534s]And those companies will be also building AIs to drive the robots.\n[2534s->2542s]And so we're at the beginning of all of this build out.\n[2542s->2547s]The next question comes from CJ News with Cantor Fitzgerald.\n[2547s->2549s]Your line is open.\n[2549s->2552s]Yeah, good afternoon. Thank you for taking the question.\n[2552s->2556s]There have been many large GPU cluster investment announcements in the last month.\n[2556s->2560s]And you alluded to a few of them with Saudi Arabia, the UAE.\n[2560s->2563s]And then also, you know, we heard from Oracle and XAI, just to name a few.\n[2563s->2570s]So my question, are there other that have yet to be announced of the same kind of scale and magnitude?\n[2570s->2583s]And perhaps more importantly, how are these orders impacting your lead times for Blackwell and your current visibility sitting here today, you know, almost halfway through 2025?\n[2583s->2593s]Well, we have more orders today than we did at the last time I spoke about orders at GTC.\n[2593s->2599s]However, we're also increasing our supply chain and building out our supply chain.\n[2599s->2602s]They're doing a fantastic job.\n[2602s->2613s]We're building it here on shore in the United States, but we're going to keep our supply chain quite busy for several many more years coming.\n[2613s->2625s]And with respect to further announcements, I'm going to be on the road next week through Europe.\n[2625s->2640s]And it's just about every country needs to build out AI infrastructure and their umpteen AI factories being planned.\n[2640s->2650s]I think in the remarks, Colette mentioned there's some 100 AI factories being built.\n[2650s->2653s]There's a whole bunch that haven't been announced.\n[2653s->2673s]And I think the important concept here, which makes it easier to understand, is that like other technologies that impact literally every single industry, of course, electricity was one, and it became infrastructure.\n[2673s->2683s]Of course, the information infrastructure, which we now know as the Internet, affects every single industry, every country, every society.\n[2683s->2686s]Intelligence is surely one of those things.\n[2686s->2693s]I don't know any company, industry, country who thinks that intelligence is optional.\n[2693s->2694s]It's essential infrastructure.\n[2694s->2701s]And so we've now digitalized intelligence.\n[2701s->2708s]And so I think we're clearly in the beginning of the build out of this infrastructure.\n[2708s->2712s]And every country will have it.\n[2712s->2713s]I'm certain of that.\n[2713s->2714s]Every industry will use it.\n[2714s->2716s]That I'm certain of.\n[2716s->2722s]And what's unique about this infrastructure is that it needs factories.\n[2722s->2728s]You know, it's a little bit like the energy infrastructure, electricity.\n[2728s->2730s]It needs factories.\n[2730s->2733s]We need factories to produce this intelligence.\n[2733s->2736s]And the intelligence is getting more sophisticated.\n[2736s->2743s]We were talking about earlier that we had a huge breakthrough in the last couple of years with reasoning AI.\n[2743s->2748s]And now there are agents that reason and there are super agents that use a whole bunch of tools.\n[2748s->2754s]And then there's clusters of super agents where agents are working with agents, solving problems.\n[2754s->2769s]And so you could just imagine compared to one shot chat bots and the agents that are now using AI built on these large language models, how much more compute intensive they really need to be and are.\n[2769s->2774s]And so I think we're in the beginning of the build out.\n[2774s->2782s]And there should be many, many more announcements in the future.\n[2782s->2783s]Your next question.\n[2783s->2784s]Excuse me.\n[2784s->2788s]Your next question comes from Ben Reitz's with Melius.\n[2788s->2790s]Your line is open.\n[2790s->2791s]Yeah, hi.\n[2791s->2793s]Thanks for the question.\n[2793s->2801s]I wanted to ask first to Collette just a little clarification around the guidance and maybe putting it in a different way.\n[2801s->2813s]The 8 billion for H20 just seems like it's roughly 3 billion more than most people thought with regard to what you'd be foregoing in the second quarter.\n[2813s->2823s]So that would mean that with regard to your guidance, the rest of the business in order to hit 45 is doing 2 to 3 billion or so better.\n[2824s->2828s]So I was wondering if that math made sense to you.\n[2828s->2837s]And then in terms of the guidance, that would imply the non-China business is doing a bit better than the street expected.\n[2837s->2843s]So wondering what the primary driver was there in your view.\n[2843s->2851s]And then this second part of my question, Jensen, I know you guide one quarter at a time.\n[2851s->2871s]But with regard to the AI diffusion rule being lifted and this momentum was sovereign, there's been times in your history where you guys have said on calls like this where you have more conviction and sequential growth throughout the year, et cetera.\n[2871s->2886s]And given the unleashing of demand with AI diffusion being revoked and the supply chain increasing, does the environment give you more conviction and sequential growth as we go throughout the year?\n[2886s->2891s]So first one for Collette and then next one for Jensen. Thanks so much.\n[2891s->2908s]Thanks, Ben, for the question. When we look at our Q2 guidance and our commentary that we provided that had the export controls not occurred, we would have had orders of about 8 billion for H20.\n[2908s->2918s]That's correct. That was a possibility for what we would have had in our outlook for this quarter in Q2.\n[2918s->2932s]So what we also have talked about here is the growth that we've seen in Blackwell, Blackwell across many of our customers, as well as the growth that we continue to have in terms of supply that we need for our customers.\n[2932s->2938s]So putting those together, that's where we came through with the guidance that we provided.\n[2938s->2942s]I'm going to turn the rest over to Jensen to see how he wants to.\n[2942s->2945s]Yeah, thanks. Thanks, Ben.\n[2945s->2957s]I would say compared to the beginning of the year, compared to GTC timeframe, there are four positive surprises.\n[2957s->2967s]The first positive surprise is the step function demand increase of reasoning AI.\n[2967s->2977s]I think it is fairly clear now that AI is going through an exponential growth, and reasoning AI really busts it through.\n[2977s->2986s]Concerns about hallucination or its ability to really solve problems.\n[2986s->2999s]I think a lot of people are crossing that barrier and realizing how incredibly effective agentic AI is and reasoning AI is.\n[2999s->3007s]So number one is inference reasoning and the exponential growth there, demand growth.\n[3007s->3012s]The second one, you mentioned AI diffusion.\n[3012s->3019s]It's really terrific to see that the AI diffusion rule was rescinded.\n[3019s->3032s]President Trump wants America to win, and he also realizes that we're not the only country in the race.\n[3032s->3042s]He wants the United States to win and recognizes that we have to get the American stack out to the world\n[3042s->3047s]and have the world build on top of American stacks instead of alternatives.\n[3047s->3052s]And so AI diffusion happened.\n[3052s->3062s]The rescinding of it happened at almost precisely the time that the countries around the world are awakening the importance of AI as an infrastructure,\n[3062s->3072s]not just as a technology of great curiosity and great importance, but infrastructure for their industries and startups and society.\n[3072s->3078s]Just as they had to build out infrastructure for electricity and Internet, you got to build out an infrastructure for AI.\n[3078s->3083s]I think that that's an awakening and that creates a lot of opportunity.\n[3083s->3085s]The third is enterprise AI.\n[3085s->3093s]Agents work and agents are doing these agents are really quite successful.\n[3093s->3098s]Much more than generative AI, agentic AI is game changing.\n[3098s->3113s]Agents can understand ambiguous and rather implicit instructions and able to problem solve and use tools and have memory and so on.\n[3113s->3118s]And so I think enterprise AI is ready to take off.\n[3118s->3133s]And it's taken us a few years to build a computing system that is able to integrate, run enterprise AI stacks, run enterprise IT stacks, but add AI to it.\n[3133s->3142s]And this is the RTX Pro Enterprise server that we announced at Computex just last week.\n[3142s->3149s]And just about every major IT company has joined us super excited about that.\n[3149s->3152s]And so computing is one stat, one part of it.\n[3152s->3157s]But remember, enterprise IT is really three pillars.\n[3157s->3161s]It's compute, storage and networking.\n[3161s->3164s]And we've now put all three of them together for finally.\n[3164s->3167s]And we're going to market with that.\n[3167s->3170s]And then lastly, industrial AI.\n[3170s->3186s]Remember, one of the implications of the world reordering, if you will, is regions onshore manufacturing and building plants everywhere.\n[3186s->3195s]In addition to AI factories, of course, there are new electronics manufacturing chip manufacturing being built around the world.\n[3195s->3210s]And all of these new plants and these new factories are creating exactly the right time when Omniverse and AI and all the work that we're doing with robotics is emerging.\n[3210s->3215s]And so this fourth pillar is quite important.\n[3215s->3218s]Every factory will have an AI factory associated with it.\n[3218s->3226s]And in order to create these physical AI systems, you really have to train a vast amount of data.\n[3226s->3232s]So back to more data, more training, more AIs to be created, more computers.\n[3232s->3243s]And so these four drivers are really kicking into turbocharge.\n[3243s->3248s]Your next question comes from Timothy R. Curie with UBS.\n[3248s->3250s]Your line is open.\n[3250s->3251s]Thanks a lot.\n[3251s->3253s]Jensen, I wanted to ask about China.\n[3253s->3257s]It sounds like the July guidance assumes there's no SKU replacement for the H-20.\n[3257s->3262s]But if the president wants the U.S. to win, it seems like you're going to have to be allowed to ship something into China.\n[3262s->3265s]So I guess I had two points on that.\n[3265s->3269s]First of all, have you been approved to ship a new modified version into China?\n[3269s->3272s]And you're currently building it, but you just can't ship it in fiscal Q2.\n[3272s->3277s]And then you were sort of run rating $7 to $8 billion a quarter into China.\n[3277s->3283s]Can we get back to those sorts of quarterly run rates once you get something that you're allowed to ship back into China?\n[3283s->3287s]I think we're all trying to figure out how much to add back to our models and when.\n[3287s->3289s]So whatever you can say there would be great.\n[3289s->3291s]Thanks.\n[3291s->3293s]The president has a plan.\n[3293s->3296s]He has a vision and I trust him.\n[3296s->3305s]With respect to our export controls, it's a set of limits.\n[3305s->3322s]And the new set of limits pretty much make it impossible for us to reduce Hopper any further for any productive use.\n[3322s->3330s]And so the new limits, it's kind of the end of the road for Hopper.\n[3330s->3333s]We have limited options.\n[3333s->3337s]And so the key is to understand the limits.\n[3337s->3346s]The key is to understand the limits and see if we can come up with interesting products that could continue to serve the Chinese market.\n[3346s->3350s]We don't have anything at the moment.\n[3350s->3352s]But we're considering it.\n[3352s->3354s]We're thinking about it.\n[3354s->3358s]Obviously the limits are quite stringent at the moment.\n[3358s->3363s]And we have nothing to announce today.\n[3363s->3373s]And when the time comes, you know, we'll engage the administration and discuss that.\n[3373s->3378s]Your final question comes from the line of Aaron Rakers with Wells Fargo.\n[3378s->3381s]Your line is open.\n[3381s->3382s]Hi.\n[3382s->3383s]This is Jake on for Aaron.\n[3383s->3386s]Thanks for taking the question and congrats on the great quarter.\n[3386s->3399s]I was wondering if you could give some additional color around the strength you saw within the networking business, particularly around the adoption of your Ethernet solutions at CSPs as well as your\n[3400s->3404s]any change you're seeing in network attach rates.\n[3404s->3407s]Yeah, thank you for that.\n[3407s->3415s]We now have three networking platforms, maybe four.\n[3415s->3425s]The first one is the scale up platform to turn a computer into a much larger computer.\n[3425s->3427s]Scaling up is incredibly hard to do.\n[3427s->3429s]Scaling out is easier to do.\n[3429s->3431s]But scaling up is hard to do.\n[3431s->3433s]And that platform is called NVLink.\n[3433s->3446s]And NVLink comes with it, chips and switches and, you know, NVLink spines.\n[3446s->3448s]It's really complicated.\n[3448s->3452s]But anyways, that's our new platform, scale up platform.\n[3452s->3455s]In addition to InfiniBand, we also have SpectrumX.\n[3455s->3466s]We've been fairly, fairly consistent that Ethernet was designed for a lot of traffic that are independent.\n[3466s->3471s]But in the case of AI, you have a lot of computers working together.\n[3471s->3477s]And the traffic of AI is insanely bursty.\n[3477s->3479s]Latency matters a lot.\n[3479s->3484s]Because the AI is thinking and it wants to get work done as quickly as possible.\n[3484s->3487s]And you got a whole bunch of nodes working together.\n[3487s->3499s]And so we enhanced Ethernet, added capabilities like extremely low latency, congestion control, adaptive routing,\n[3499s->3506s]the type of technologies that were available only in InfiniBand to Ethernet.\n[3506s->3511s]And as a result, we improved the utilization of Ethernet in these clusters.\n[3511s->3519s]These clusters are gigantic from as low as 50% to as high as 85%, 90%.\n[3519s->3531s]And so the difference is if you had a cluster that's $10 billion and you improved its effectiveness by 40%, that's worth $4 billion.\n[3532s->3534s]It's incredible.\n[3534s->3538s]And so SpectrumX has been really, quite frankly, a home run.\n[3538s->3553s]And this last quarter, as we said in the prepared remarks, we added two very significant CSPs to the SpectrumX adoption.\n[3553s->3558s]And then the last one, the last one is Bluefield, which is our control plane.\n[3558s->3566s]And so in those four, the control plane and network, which is used for storage, is used for security,\n[3566s->3576s]and for many of these clusters that want to achieve isolation among its users, multi-tenant clusters,\n[3576s->3583s]and still be able to use and have extremely high performance, bare metal performance,\n[3584s->3591s]Bluefield is ideal for that and is used in a lot of these cases.\n[3591s->3599s]And so we have these four networking platforms that are all growing and we're doing really well.\n[3599s->3601s]I'm very proud of the team.\n[3603s->3607s]That is all the time we have for questions.\n[3607s->3610s]Denton, I will turn the call back to you.\n[3610s->3611s]Thank you.\n[3611s->3612s]Thank you.\n[3612s->3615s]This is the start of a powerful new wave of growth.\n[3615s->3617s]Grace Blackwell is in full production.\n[3617s->3619s]We're off to the races.\n[3619s->3621s]We now have multiple significant growth engines.\n[3621s->3627s]Inference, one's delighted workload is surging with revenue generating AI services.\n[3627s->3635s]AI is growing faster and will be larger than any platform shifts before, including the internet, mobile, and cloud.\n[3635s->3645s]Blackwell is built to power the full AI lifecycle from training frontier models to running complex inference and reasoning agents at scale.\n[3645s->3655s]Training demands continues to rise with breakthroughs in post-training and like reinforcement learning and synthetic data generation.\n[3655s->3657s]But inference is exploding.\n[3657s->3661s]Reasoning AI agents require orders of magnitude more compute.\n[3661s->3667s]The foundations of our next growth platforms are in place and ready to scale.\n[3667s->3674s]Sovereign AI nations are investing in AI infrastructure like they once did for electricity and internet.\n[3674s->3681s]Enterprise AI, AI must be deployable on prem and integrated with existing IT.\n[3681s->3692s]Our RTX Pro, DGX Spark, and DGX Station Enterprise AI systems are ready to modernize the $500 billion IT infrastructure on prem or in the cloud.\n[3692s->3696s]Every major IT provider is partnering with us.\n[3696s->3710s]Industrial AI from training to digital twin simulation to deployment, NVIDIA Omniverse and Isaac Groot are powering next generation factories and humanoid robotic systems worldwide.\n[3710s->3723s]The age of AI is here from AI infrastructures, inference at scale, sovereign AI, enterprise AI, and industrial AI.\n[3723s->3725s]NVIDIA is ready.\n[3725s->3740s]Join us at GTC Paris, our keynote at VivaTech on June 11, talking about quantum GPU computing, robotic factories and robots, and celebrate our partnerships building AI factories across the region.\n[3740s->3745s]The NVIDIA Band will tour France, the UK, Germany, and Belgium.\n[3745s->3753s]Thank you for joining us at the earnings call today. See you in Paris.\n[3753s->3758s]This concludes today's conference call. You may now disconnect.\n",
            "[TABLE]\n| Revenue | $ | 44,062 |  | $ | 26,044 |\n| --- | --- | --- | --- | --- | --- |\n| Cost of revenue | 17,394 |  |  | 5,638 |  |\n| Gross profit | 26,668 |  |  | 20,406 |  |\n| Operating expenses |  |  |  |  |  |\n| Research and development | 3,989 |  |  | 2,720 |  |\n| Sales, general and administrative | 1,041 |  |  | 777 |  |\n| Total operating expenses | 5,030 |  |  | 3,497 |  |\n| Operating income | 21,638 |  |  | 16,909 |  |\n| Interest income | 515 |  |  | 359 |  |\n| Interest expense | (63) |  |  | (64) |  |\n| Other income (expense), net | (180) |  |  | 75 |  |\n| Total other income (expense), net | 272 |  |  | 370 |  |\n| Income before income tax | 21,910 |  |  | 17,279 |  |\n| Income tax expense | 3,135 |  |  | 2,398 |  |\n| Net income | $ | 18,775 |  | $ | 14,881 |\n|  |  |  |  |  |  |\n| Net income per share: |  |  |  |  |  |\n| Basic | $ | 0.77 |  | $ | 0.60 |\n| Diluted | $ | 0.76 |  | $ | 0.60 |\n|  |  |  |  |  |  |\n| Weighted average shares used in per share computation: |  |  |  |  |  |\n| Basic | 24,441 |  |  | 24,620 |  |\n| Diluted | 24,611 |  |  | 24,890 |  |\n[/TABLE]\n\n该表格展示了公司收入和利润情况。主要趋势包括：总收入增长，毛利率上升，运营费用增加，净利润增长，每股收益提升。\n\nPart I. Financial Information\n\nItem 1. Financial Statements (Unaudited)\n\nNVIDIA Corporation and Subsidiaries\nCondensed Consolidated Statements of Income\n\n(In millions, except per share data)\n\nThree Months Ended\n\nApr 27, 2025\nApr 28, 2024\n\nSee accompanying Notes to Condensed Consolidated Financial Statements.\n\n3",
            "Item 2. Management's Discussion and Analysis of Financial Condition and Results of Operations\n\nForward-Looking Statements \nThis Quarterly Report on Form 10-Q contains forward-looking statements within the meaning of Section 27A of the Securities Act of 1933, as amended, and\nSection 21E of the Securities Exchange Act of 1934, as amended, which are subject to the “safe harbor” created by those sections based on management’s\nbeliefs and assumptions and on information currently available to management. In some cases, you can identify forward-looking statements by terms such as\n“may,” “will,” “should,” “could,” “goal,” “would,” “expect,” “plan,” “anticipate,” “believe,” “estimate,” “project,” “predict,” “potential” and similar expressions intended\nto identify forward-looking statements. These statements involve known and unknown risks, uncertainties and other factors, which may cause our actual results,\nperformance, time frames or achievements to be materially different from any future results, performance, time frames or achievements expressed or implied by\nthe forward-looking statements. We discuss many of these risks, uncertainties and other factors in this Quarterly Report on Form 10-Q and our Annual Report on\nForm 10-K for the fiscal year ended January 26, 2025 in greater detail under the heading “Risk Factors” of such reports. Given these risks, uncertainties, and\nother factors, you should not place undue reliance on these forward-looking statements. Also, these forward-looking statements represent our estimates and\nassumptions only as of the date of this filing. You should read this Quarterly Report on Form 10-Q completely and understand that our actual future results may\nbe materially different from what we expect. We hereby qualify our forward-looking statements by these cautionary statements. Except as required by law, we\nassume no obligation to update these forward-looking statements publicly, or to update the reasons actual results could differ materially from those anticipated in\nthese forward-looking statements, even if new information becomes available in the future.\n\nAll references to “NVIDIA,” “we,” “us,” “our” or the “Company” mean NVIDIA Corporation and its subsidiaries.\n\n© 2025 NVIDIA Corporation. All rights reserved.\n\nThe following discussion and analysis of our financial condition and results of operations should be read in conjunction with the risk factors set forth in Item 1A.\n“Risk Factors” of our Annual Report on Form 10-K for the fiscal year ended January 26, 2025 and Part II, Item 1A. “Risk Factors” of this Quarterly Report on\nForm 10-Q and our Condensed Consolidated Financial Statements and related Notes thereto, as well as other cautionary statements and risks described\nelsewhere in this Quarterly Report on Form 10-Q and our other filings with the SEC, before deciding to purchase, hold, or sell shares of our common stock.\n\nOverview\n\nOur Company and Our Businesses\n\nNVIDIA pioneered accelerated computing to help solve the most challenging computational problems. Since our original focus on PC graphics, we have\nexpanded to several other large and important computationally intensive fields. Fueled by the sustained demand for exceptional 3D graphics and the scale of the\ngaming market, NVIDIA has leveraged its GPU architecture to create platforms for scientific computing, AI, data science, AV, robotics, and digital twin\napplications.\n\nOur two operating segments are \"Compute & Networking\" and \"Graphics,\" as described in Note 13 of the Notes to Condensed Consolidated Financial\nStatements in Part I, Item 1 of this Quarterly Report on Form 10-Q.\n\nHeadquartered in Santa Clara, California, NVIDIA was incorporated in California in April 1993 and reincorporated in Delaware in April 1998.\n\nRecent Developments, Future Objectives and Challenges\n\nRevenue growth in the first quarter of fiscal year 2026 was driven by data center compute and networking platforms for accelerated computing and AI solutions.\nOur Blackwell GPU revenue ramp continued during the quarter, extending beyond large cloud service providers to include consumer internet companies and\nregional cloud service providers.\n\nOn April 9, 2025, the U.S. government, or USG, informed us that it requires a license for export to China (including Hong Kong and Macau) and D:5 countries, or\nto companies headquartered or with an ultimate parent therein, of our H20 integrated circuits and any other circuits achieving the H20’s memory bandwidth,\ninterconnect bandwidth, or combination thereof. As a result of these new requirements, we incurred a $4.5 billion charge in the first quarter of fiscal year 2026\nassociated with H20 excess inventory and purchase obligations as the demand for H20 products diminished. The $4.5 billion charge was less than what we\ninitially anticipated as we were able to re-use certain materials. Sales of our H20 products were $4.6 billion for the first quarter of fiscal year 2026 prior to the\nnew export licensing requirements. The H20 export licensing requirements have impacted our current revenue and will also negatively affect our future revenue.\nWe are still evaluating our limited options to supply Data Center compute products compliant with the USG’s export control rules. The export controls applicable\nto China are complex and address a variety of parameters, including the total processing performance of a chip, the “performance density” of a chip, the\ninterconnect bandwidth of a chip, and the memory bandwidth of a chip. We may be unable to create a competitive product for China’s data center market that\nreceives approval from the USG. In that event, we would effectively be foreclosed from competing in China's data center computing/compute market, with a\nmaterial and adverse impact on our business, operating results, and financial condition.\n\n22"
        ]
    },
    {
        "question": "Q1 FY26 数据中心业务的表现如何？收入是多少？",
        "ground_truth": "Q1 FY26 数据中心业务收入为 391.12 亿美元，同比增长 73%。这一强劲增长是由对用于大型语言模型、推荐引擎和生成式 AI 应用的加速计算平台的需求推动的。",
        "answer": "根据提供的财务报告，NVIDIA在2026财年第一季度的数据中心业务表现强劲。数据中心收入达到了390亿美元，同比增长73%。这一增长主要得益于AI工作负载向推理的过渡以及AI工厂建设的推动。此外，Blackwell产品的推出也对数据中心收入增长做出了重要贡献。Blackwell的推出速度是公司历史上最快的，推动了数据中心收入73%的年增长。因此，NVIDIA在2026财年第一季度的数据中心业务收入为390亿美元 [来源: Management's Discussion and Analysis of Financial Condition and Results of Operations, 第22页]。",
        "contexts": [
            "[0s->5s]Good afternoon. My name is Sarah and I will be your conference operator today.\n[5s->13s]At this time, I would like to welcome everyone to NVIDIA's first quarter fiscal 2026 financial results conference call.\n[13s->17s]All lines have been placed on mute to prevent any background noise.\n[17s->21s]After the speaker's remarks, there will be a question and answer session.\n[21s->27s]If you would like to ask a question during this time, simply press star one on your telephone keypad.\n[27s->32s]If you would like to withdraw your question, please press star one again. Thank you.\n[32s->36s]Toshia Hari, you may begin your conference.\n[36s->44s]Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the first quarter of fiscal 2026.\n[44s->54s]With me today from NVIDIA are Jensen Wong, President and Chief Executive Officer, and Collette Press, Executive Vice President and Chief Financial Officer.\n[54s->60s]I'd like to remind you that our call is being webcast live on NVIDIA's investor relations website.\n[60s->68s]The webcast will be available for replay until the conference call to discuss our financial results for the second quarter of fiscal 2026.\n[68s->76s]The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent.\n[76s->81s]During this call, we may make forward looking statements based on current expectations.\n[81s->88s]These are subject to a number of significant risks and uncertainties, and our actual results may differ materially.\n[88s->96s]For a discussion of factors that could affect our future financial results in business, please refer to the disclosure in today's earnings release,\n[96s->104s]our most recent forms 10-K and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission.\n[105s->112s]All our statements are made as of today, May 28, 2025, based on information currently available to us.\n[112s->118s]Except as required by law, we assume no obligation to update any such statements.\n[118s->122s]During this call, we will discuss non-GAAP financial measures.\n[122s->130s]You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website.\n[130s->133s]With that, let me turn the call over to Collette.\n[133s->135s]Thank you, Toshio.\n[135s->148s]We delivered another strong quarter with revenue of $44 billion, up 69% year-over-year, exceeding our outlook in what proved to be a challenging operating environment.\n[148s->153s]Data center revenue of $39 billion grew 73% year-on-year.\n[153s->161s]AI workloads have transitioned strongly to inference, and AI factory build-outs are driving significant revenue.\n[161s->165s]Our customers' commitments are firm.\n[165s->176s]On April 9, the U.S. government issued new export controls on H20, our data center GPU designed specifically for the China market.\n[176s->181s]We sold H20 with the approval of the previous administration.\n[181s->187s]Although our H20 has been in the market for over a year and does not have a market outside of China,\n[187s->195s]the new export controls on H20 did not provide a grace period to allow us to sell through our inventory.\n[195s->204s]In Q1, we recognized $4.6 billion in H20 revenue, which occurred prior to April 9,\n[204s->216s]but also recognized a $4.5 billion charge as we wrote down inventory and purchase obligations tied to orders we had received prior to April 9.\n[216s->226s]We were unable to ship $2.5 billion in H20 revenue in the first quarter due to the new export controls.\n[226s->234s]The $4.5 billion charge was less than what we initially anticipated as we were able to reuse certain materials.\n[234s->244s]We are still evaluating our limited options to supply data center compute products compliant with the U.S. government's revised export control rules,\n[244s->252s]losing access to the China AI accelerator market, which we believe will grow to nearly $50 billion,\n[252s->262s]would have a material adverse impact on our business going forward and benefit our foreign competitors in China and worldwide.\n[262s->271s]Our Blackwell ramp, the fastest in our company's history, drove a 73% year-on-year increase in data center revenue.\n[271s->280s]Blackwell contributed nearly 70% of data center compute revenue in the quarter, with a transition from Hopper nearly complete.\n[280s->293s]The introduction of GB200 NVL was a fundamental architectural change to enable data center scale workloads and to achieve the lowest cost per inference token.\n[293s->305s]While these systems are complex to build, we have seen a significant improvement in manufacturing yields and rack shipments are moving to strong rates to end customers.\n[305s->315s]GB200 NVL racks are now generally available for modern builders, enterprises, and sovereign customers to develop and deploy AI.\n[315s->332s]On average, major hyperscalers are each deploying nearly 1,000 NVL 72 racks or 72,000 Blackwell GPUs per week and are on track to further ramp output this quarter.\n[332s->346s]Microsoft, for example, has already deployed tens of thousands of Blackwell GPUs and is expected to ramp to hundreds of thousands of GB200s with OpenAI as one of its key customers.\n[346s->356s]Key learnings from the GB200 ramp will allow for a smooth transition to the next phase of our product roadmap, Blackwell Ultra.\n[357s->370s]Sampling of GB300 systems began earlier this month at the major CSPs, and we expect production shipments to commerce later this quarter.\n[370s->381s]GB300 will leverage the same architecture, same physical footprint, and the same electrical and mechanical specifications as GB200.\n[381s->394s]The GB300 drop-in design will allow CSPs to seamlessly transition their systems and manufacturing used for GB200 while maintaining high yields.\n[394s->407s]B300 GPUs with 50% more HBM will deliver another 50% increase in dense FP4 inference compute performance compared to the B200.\n[407s->421s]We remain committed to our annual product cadence, with our roadmap extending through 2028 tightly aligned with the multiple-year planning cycles of our customers.\n[421s->432s]We are witnessing a sharp jump in inference demand. OpenAI, Microsoft, and Google are seeing a step-function leap in token generation.\n[432s->441s]Microsoft processed over 100 trillion tokens in Q1, a five-fold increase on a year-over-year basis.\n[441s->453s]This exponential growth in Azure OpenAI is representative of strong demand for Azure AI Foundry as well as other AI services across Microsoft's platform.\n[453s->471s]Inference-serving startups are now serving models using B200, tripling their token generation rate and corresponding revenues for high-value reasoning models such as DeepSeq R1, as reported by Artificial Analysis.\n[471s->485s]NVIDIA Dynamo on Blackwell NVL72 turbocharges AI inference throughput by 30x for the new reasoning models sweeping the industry.\n[485s->497s]Developer engagements increased with adoption ranging from LLM providers such as Perplexity to financial services institutions such as Capital One,\n[497s->503s]who reduced agentic chatbox latency by 5x with Dynamo.\n[503s->511s]In the latest MLPerf inference results, we submitted our first results using GB200 NVL72,\n[511s->523s]delivering up to 30x higher inference throughput compared to our 8 GPU H200 submission on the challenging Llama 3.1 benchmark.\n[523s->535s]This feat was achieved through a combination of tripling the performance per GPU as well as 9x more GPUs, all connected on a single NVLink domain.\n[535s->547s]And while Blackwell is still early in its lifecycle, software optimizations have already improved its performance by 1.5x in the last month alone.\n[547s->555s]We expect to continue improving the performance of Blackwell through its operational life as we have done with Hopper and Amper.\n[555s->560s]For example, we increased the inference performance of Hopper by 4x over two years.\n[560s->568s]This is the benefit of NVIDIA's programmable CUDA architecture and rich ecosystem.\n[568s->578s]The pace and scale of AI factory deployments are accelerating with nearly 100 NVIDIA-powered AI factories in flight this quarter,\n[578s->588s]a two-fold increase year over year, with the average number of GPUs powering each factory also doubling in the same period.\n[588s->593s]And more AI factory projects are starting across industries and geographies.\n[593s->605s]NVIDIA's full-stack architecture is underpinning AI factory deployments as industry leaders like AT&T, BYD, Capital One, Foxconn, MediaTek, and Telenor\n[605s->614s]are strategically vital sovereign clouds like those recently announced in Saudi Arabia, Taiwan, and the UAE.\n[614s->623s]We have a line of sight to projects requiring tens of gigawatts of NVIDIA AI infrastructure in the not-too-distant future.\n[623s->639s]The transition from generative to agentic AI, AI capable of perceiving, reasoning, planning, and acting, will transform every industry, every company, and country.\n[639s->650s]We envision AI agents as a new digital workforce capable of handling tasks ranging from customer service to complex decision-making processes.\n[650s->660s]We introduced the LAMA Nemotron family of open reasoning models designed to supercharge identical AI platforms for enterprises.\n[660s->672s]Built on the LAMA architecture, these models are available as NIMS or NVIDIA inference microservices with multiple sizes to meet diverse deployment needs.\n[672s->681s]Our post-training enhancements have yielded a 20% accuracy boost and a 5x increase in inference speed.\n[681s->690s]Leading platform companies including Accenture, Cadence, Deloitte, and Microsoft are transforming work with our reasoning models.\n[690s->702s]NVIDIA NEMO microservices are generally available across industries that are being leveraged by leading enterprises to build, optimize, and scale AI applications.\n[702s->711s]With NEMO, Cisco increased model accuracy by 40% and improved response time by 10x in its code assistant.\n[711s->719s]NASDAQ realized a 30% improvement in accuracy and response time in its AI platform's search capabilities.\n[719s->727s]And Shell's Custom LLM achieved a 30% increase in accuracy when trained with NVIDIA NEMO.\n[727s->735s]NEMO's parallelism techniques accelerated model training time by 20% when compared to other frameworks.\n[735s->757s]We also announced a partnership with Yum Brands, the world's largest restaurant company, to bring NVIDIA AI to 500 of its restaurants this year and expanding to 61,000 restaurants over time to streamline order taking, optimize operations, and enhance service across its restaurants.\n[757s->779s]For AI-powered cybersecurity, leading companies like Checkpoint, CloudStrike, and Palo Alto Networks are using NVIDIA's AI security and software stack to build, optimize, and secure agentic workflows, with CloudStrike realizing 2x faster detection triage with 50% less compute cost.\n[780s->790s]Moving to networking, sequential growth in networking resumed in Q1, with revenue up 64% quarter-over-quarter to $5 billion.\n[790s->798s]Our customers continue to leverage our platform to efficiently scale up and scale out AI factory workloads.\n[798s->813s]We created the world's fastest switch, NVLink. For scale up, our NVLink compute fabric in its fifth generation offers 14x the bandwidth of PCIe Gen 5.\n[813s->825s]NVLink 72 carries 130 TB per second of bandwidth in a single rack, equivalent to the entirety of the world's peak internet traffic.\n[825s->833s]NVLink is a new growth vector and is off to a great start, with Q1 shipments exceeding $1 billion.\n[833s->848s]At Computex, we announced NVLink Fusion. Hyperscale customers can now build semi-custom CCUs and accelerators that connect directly to the NVIDIA platform with NVLink.\n[848s->868s]We are now enabling key partners, including ASIC providers such as MediaTek, Marvell, Alchip Technologies, and Astera Labs, as well as CPU suppliers such as Fujitsu and Qualcomm, to leverage NVLink Fusion to connect our respective ecosystems.\n[868s->878s]For scale out, our enhanced Ethernet offerings deliver the highest throughput, lowest latency networking for AI.\n[878s->886s]SpectrumX posted strong sequential and year-on-year growth and is now annualizing over $8 billion in revenue.\n[886s->896s]Adoption is widespread across major CSPs and consumer internet companies, including CoreWe, Microsoft Azure, Oracle Cloud, and XAI.\n[896s->903s]This quarter, we added Google Cloud and Meta to the growing list of SpectrumX customers.\n[903s->913s]We introduced SpectrumX and QuantumX, silicon photonics switches featuring the world's most advanced co-packaged optics.\n[913s->933s]These platforms will enable next level AI factory scaling to millions of GPUs through the increasingly power efficiency by 3.5x and network resiliency by 10x while accelerating customer time to market by 1.3x.\n[933s->938s]Transitioning to a quick summary of our revenue by geography.\n[938s->949s]China, as a percentage of our data center revenue, was slightly below our expectations and down sequentially due to H20 export licensing controls.\n[949s->955s]For Q2, we expect a meaningful decrease in China data center revenue.\n[955s->971s]As a reminder, while Singapore represented nearly 20% of our Q1 billed revenue, as many of our large customers use Singapore for centralized invoicing, our products are almost always sold elsewhere.\n[971s->984s]Note that over 99% of H100, H200, and Blackwell data center compute revenue billed to Singapore was for orders from US based customers.\n[984s->996s]Moving to gaming and AI PCs. Gaming revenue was a record 3.8 billion, increasing 48% sequentially and 42% year on year.\n[996s->1012s]Strong adoption by gamers, creatives, and AI enthusiasts have made Blackwell our fastest ramp ever. Against a backdrop of robust demand, we greatly improved our supply and availability in Q1 and expect to continue these efforts in Q2.\n[1012s->1017s]AI is transforming PC and creator and gamers.\n[1017s->1035s]With a 100 million user installed base, GeForce represents the largest footprint for PC developers. This quarter, we added to our AI PC laptop offerings, including models capable of running Microsoft's Copilot Plus.\n[1035s->1049s]This past quarter, we brought Blackwell architecture to mainstream gaming with its launch of GeForce RTX 5060 and 5060 Ti, starting at just $299.\n[1049s->1061s]The RTX 5060 also debuted in laptops starting at $1,099. These systems that double the frame rate and slash latency.\n[1061s->1070s]These GeForce RTX 5060 and 5060 Ti desktop GPUs and laptops are now available.\n[1070s->1090s]In console gaming, the recently unveiled Nintendo Switch 2 leverages Nvidia's neural rendering and AI technologies, including next generation custom RTX GPUs with DLSS technology, to deliver a giant leap in gaming performance to millions of players worldwide.\n[1090s->1100s]Nintendo has shipped over 150 million Switch consoles to date, making it one of the most successful gaming systems in history.\n[1100s->1108s]Moving to pro visualization. Revenue of $509 million was flat sequentially and up 19% year on year.\n[1108s->1120s]Tariff related uncertainty temporarily impacted Q1 systems and demand for AI workstations is strong and we expect sequential revenue growth to resume in Q2.\n[1120s->1130s]Nvidia DGX Spark and Station revolutionize personal computing by putting the power of an AI supercomputer in a desktop form factor.\n[1130s->1143s]DGX Spark delivers up to one petaflop of AI compute while DGX Station offers an incredible 20 petaflops and is powered by the GB300 Superchip.\n[1143s->1150s]DGX Spark will be available in Calendar Q3 and DGX Station later this year.\n[1150s->1163s]We have deepened Omniverse's integration and adoption into some of the world's leading software platforms, including Databricks, SAP, and Schneider Electric.\n[1163s->1178s]New Omniverse blueprints such as Mega for at scale robotic fleet management are being leveraged in Keyon Group, Pegatron, Accenture, and other leading companies to enhance industrial operations.\n[1178s->1189s]At Confitex, we showcased Omniverse's great traction with technology manufacturing leaders, including TSMC, Quanta, Foxconn, Pegatron.\n[1189s->1194s]Using Omniverse, TSMC saves months in work by designing fabs virtually.\n[1194s->1205s]Foxconn accelerates thermal simulations by 150x and Pegatron reduced assembly line defects rates by 67%.\n[1205s->1214s]Lastly, with our automotive group, revenue was $567 million, down 1% sequentially but up 72% year on year.\n[1214s->1222s]Year on year growth was driven by the ramp of self-driving across a number of customers and robust end demand for NEVs.\n[1222s->1232s]We are partnering with GM to build the next-gen vehicles, factories, and robots using NVIDIA AI, simulation, and accelerated computing.\n[1232s->1241s]And we are now in production with our full-stack solution for Mercedes-Benz, starting with the new CLA, hitting roads in the next few months.\n[1241s->1254s]We announced Isaac, Groot, and won the world's first open fully customizable foundation model for humanoid robots, enabling generalized reasoning and skill development.\n[1254s->1260s]We also launched new open NVIDIA Cosmo World Foundation models.\n[1260s->1268s]Leading companies include 1X, Agility Robots, Robotics, Figure AI, Uber, and Wabi.\n[1268s->1284s]We've begun integrating Cosmos into their operations for synthetic data generation, while Agility Robotics, Boston Dynamics, and XPen Robotics are harnessing Isaac simulation to advance their humanoid efforts.\n[1284s->1300s]GE Healthcare is using the new NVIDIA Isaac platform for healthcare simulation, built on NVIDIA Omniverse and using NVIDIA Cosmos, the platform speeds development of robotic imaging and surgery systems.\n[1300s->1314s]The era of robotics is here. Billions of robots, hundreds of millions of autonomous vehicles, and hundreds of thousands of robotic factories and warehouses will be developed.\n[1314s->1335s]All right, moving to the rest of the P&L. Gap gross margins and non-gap gross margins were 60.5% and 61%, respectively. Excluding the $4.5 billion charge, Q1 non-gap gross margins would have been 71.3%, slightly above our outlook at the beginning of the quarter.\n[1335s->1346s]Sequentially, gap operating expenses were up 7% and non-gap operating expenses were up 6%, reflecting higher compensation and employee growth.\n[1346s->1355s]Our investments include expanding our infrastructure capabilities and AI solutions, and we plan to grow these investments throughout the fiscal year.\n[1355s->1370s]In Q1, we returned a record $14.3 billion to shareholders in the form of share repurchases and cash dividends. Our capital return program continues to be a key element of our capital allocation strategy.\n[1370s->1384s]Let me turn to the outlook for the second quarter. Total revenue is expected to be $45 billion, plus or minus 2%. We expect modest sequential growth across all of our platforms.\n[1384s->1403s]In data center, we anticipate the continued ramp of Blackwell to be partially offset by a decline in China revenue. Note, our outlook reflects a loss in H20 revenue of approximately $8 billion for the second quarter.\n[1403s->1420s]Gap and non-gap gross margins are expected to be 71.8% and 72% respectively, plus or minus 50 basis points. We expect better Blackwell profitability to drive modest sequential improvement in gross margins.\n[1420s->1427s]We are continuing to work towards achieving gross margins in the mid-70s range late this year.\n[1427s->1444s]Gap and non-gap operating expenses are expected to be approximately $5.7 billion and $4 billion respectively, and we continue to expect full-year fiscal year 26 operating expense growth to be in the mid-30% range.\n[1444s->1457s]Gap and non-gap other income and expenses are expected to be an income of approximately $450 million, excluding gays and losses from non-marketable and publicly held equity securities.\n[1457s->1468s]Gap and non-gap tax rates are expected to be 16.5%, plus or minus 1%, excluding any discrete items.\n[1468s->1480s]Further financial details are included in the CFO commentary and other information available on our IR website, including a new financially information AI agent.\n[1480s->1488s]Let me highlight upcoming events for the financial community. We will be at the B of A Global Technology Conference in San Francisco on June 4.\n[1488s->1500s]The Rosenblatt Virtual AI Summit and NASDAQ Investor Conference in London on June 10, and GTC Paris at Viva Tech on June 11 in Paris.\n[1500s->1511s]We look forward to seeing you at these events. Our earnings call to discuss the results of our second quarter of fiscal 2026 is scheduled for August 27.\n[1511s->1517s]Well now, let me turn it over to Jensen to make some remarks.\n[1517s->1526s]Thanks, Collette. We've had a busy and productive year. Let me share my perspective on some topics we're frequently asked.\n[1526s->1536s]On export control, China is one of the world's largest AI markets and a springboard to global success.\n[1536s->1543s]With half of the world's AI researchers based there, the platform that wins China is positioned to lead globally.\n[1543s->1551s]Today, however, the $50 billion China market is effectively closed to U.S. industry.\n[1551s->1560s]The H20 export ban ended our Hopper data center business in China. We cannot reduce Hopper further to comply.\n[1561s->1568s]As a result, we are taking a multi-billion dollar write-off on inventory that cannot be sold or repurposed.\n[1568s->1575s]We are exploring limited ways to compete, but Hopper is no longer an option.\n[1575s->1584s]China's AI moves on with or without U.S. chips. It has to compute to train and deploy advanced models.\n[1585s->1590s]The question is not whether China will have AI. It already does.\n[1590s->1596s]The question is whether one of the world's largest AI markets will run on American platforms.\n[1596s->1604s]Shielding Chinese chipmakers from U.S. competition only strengthens them abroad and weakens America's position.\n[1604s->1609s]Export restrictions have spurred China's innovation in scale.\n[1609s->1616s]The AI race is not just about chips. It's about which stack the world runs on.\n[1616s->1624s]As that stack grows to include 6G and quantum, U.S. global infrastructure leadership is at stake.\n[1624s->1630s]The U.S. has based its policy on the assumption that China cannot make AI chips.\n[1630s->1635s]That assumption was always questionable, and now it's clearly wrong.\n[1635s->1639s]China has enormous manufacturing capability.\n[1639s->1645s]In the end, the platform that wins the AI developers wins AI.\n[1645s->1653s]Export controls should strengthen U.S. platforms, not drive half of the world's AI talent to rivals.\n[1654s->1657s]On DeepSeek.\n[1657s->1665s]DeepSeek and QN from China are among the best open source AI models.\n[1665s->1671s]Released freely, they've gained traction across the U.S., Europe, and beyond.\n[1671s->1680s]DeepSeek R1, like ChatGPT, introduced reasoning AI that produces better answers the longer it thinks.\n[1680s->1690s]Reasoning AI enables step-by-step problem solving, planning, and tool use, turning models into intelligent agents.\n[1690s->1705s]Reasoning is compute intensive, requires hundreds to thousands of times more tokens per task than previous one-shot inference.\n[1705s->1711s]Reasoning models are driving a step function surge in inference demand.\n[1711s->1721s]AI scaling laws remain firmly intact, not only for training, but now inference, too, requires massive scale compute.\n[1721s->1726s]DeepSeek also underscores the strategic value of open source AI.\n[1726s->1734s]When popular models are trained and optimized on U.S. platforms, it drives usage, feedback, and continuous improvement.\n[1734s->1737s]Reinforcing American leadership across the stack.\n[1737s->1743s]U.S. platforms must remain the preferred platform for open source AI.\n[1743s->1749s]That means supporting collaboration with top developers globally, including in China.\n[1749s->1757s]America wins when models like DeepSeek and QN runs best on American infrastructure.\n[1757s->1760s]Regarding onshore manufacturing.\n[1760s->1770s]President Trump has outlined a bold vision to reshore advanced manufacturing, create jobs, and strengthen national security.\n[1770s->1774s]Future plants will be highly computerized in robotics.\n[1774s->1776s]We share this vision.\n[1776s->1783s]TSMC is building six fabs and two advanced packaging plants in Arizona to make chips for Nvidia.\n[1783s->1789s]Process qualification is underway, with volume production expected by year end.\n[1790s->1798s]Spill and Amcor are also investing in Arizona, constructing packaging, assembly, and test facilities.\n[1798s->1806s]In Houston, we're partnering with Foxconn to construct a million-square-foot factory to build AI supercomputers.\n[1806s->1811s]Wistron is building a similar plant in Fort Worth, Texas.\n[1811s->1817s]To encourage and support these investments, we've made substantial long-term purchase commitments.\n[1817s->1821s]A deep investment in America's AI manufacturing future.\n[1823s->1827s]Our goal, from chip to supercomputer.\n[1827s->1830s]Built in America within a year.\n[1830s->1840s]Each GB200 and Beelink 72 racks contains 1.2 million components and weighs nearly two tons.\n[1840s->1844s]No one has produced supercomputers on this scale.\n[1844s->1847s]Our partners are doing an extraordinary job.\n[1849s->1857s]On AI diffusion rule, President Trump rescinded the AI diffusion rule, calling it counterproductive,\n[1857s->1864s]and proposed a new policy to promote US AI tech with trusted partners.\n[1864s->1868s]On his Middle East tour, he announced historic investments.\n[1868s->1875s]I was honored to join him in announcing a 500-megawatt AI infrastructure project in Saudi Arabia\n[1875s->1880s]and a 5-gigawatt AI campus in the UAE.\n[1880s->1883s]President Trump wants US tech to lead.\n[1883s->1887s]The deals he announced are wins for America.\n[1887s->1895s]Creating jobs, advancing infrastructure, generating tax revenue, and reducing the US trade deficit.\n[1895s->1902s]The US will always be Nvidia's largest market and home to the largest install base of our infrastructure.\n[1902s->1909s]Every nation now sees AI as core to the next industrial revolution.\n[1909s->1915s]A new industry that produces intelligence and essential infrastructure for every economy.\n[1915s->1922s]Countries are racing to build national AI platforms to elevate their digital capabilities.\n[1922s->1930s]At Computex, we announced Taiwan's first AI factory in partnership with Foxconn and the Taiwan government.\n[1930s->1936s]Last week, I was in Sweden to launch its first national AI infrastructure.\n[1936s->1947s]Japan, Korea, India, Canada, France, the UK, Germany, Italy, Spain, and more are now building national AI factories\n[1947s->1951s]to empower startups, industries, and societies.\n[1951s->1956s]Sovereign AI is a new growth engine for Nvidia.\n[1958s->1960s]Toshia, back to you. Thank you.\n[1960s->1966s]Operator, we will now open the call for questions. Would you please pull for questions?\n[1966s->1974s]Thank you. At this time, I would like to remind everyone in order to ask a question, press star, then the number one on your telephone keypad.\n[1974s->1978s]We'll pause for just a moment to compile the Q&A roster.\n[1982s->1988s]Your first question comes from the line of Joe Moore with Morgan Stanley. Your line is open.\n[1990s->1998s]Great. Thank you. You guys have talked about this scaling up of inference around reasoning models for at least a year now,\n[1998s->2002s]and we've really seen that come to fruition as you talked about. We've heard it from your customers.\n[2002s->2007s]Can you give us a sense for how much of that demand you're able to serve?\n[2007s->2010s]And give us a sense for maybe how big the inference business is for you guys.\n[2010s->2016s]And do we need full-on NVL-72 rack-scale solutions for reasoning inference going forward?\n[2019s->2026s]Well, we would like to serve all of it. And I think we're on track to serve most of it.\n[2026s->2038s]Grace Blackwell NVLink-72 is the ideal engine today, the ideal computer thinking machine, if you will, for reasoning AI.\n[2038s->2046s]There's a couple of reasons for that. The first reason is that the token generation amount,\n[2046s->2059s]the number of tokens reasoning goes through is a hundred, a thousand times more than a one-shot chatbot.\n[2059s->2069s]You know, it's essentially thinking to itself, breaking down a problem step by step. It might be planning multiple paths to an answer.\n[2069s->2083s]It could be using tools, reading PDFs, reading web pages, watching videos, and then producing a result, an answer.\n[2083s->2089s]The longer it thinks, the better the answer, the smarter the answer is.\n[2090s->2101s]And so what we would like to do and the reason why Grace Blackwell was designed to give such a giant step up in inference performance\n[2101s->2106s]is so that you could do all this and still get a response as quickly as possible.\n[2106s->2117s]Compared to Hopper, Grace Blackwell is some 40 times higher speed and throughput compared.\n[2117s->2132s]And so this is going to be a huge, huge benefit and driving down the cost while improving the quality of response with excellent quality of service at the same time.\n[2132s->2139s]So that's the fundamental reason. That was the core driving reason for Grace Blackwell NVLink-72.\n[2139s->2151s]Of course, in order to do that, we had to reinvent, literally redesign the entire way that these supercomputers are built.\n[2151s->2161s]But now we're in full production. It's going to be exciting. It's going to be incredibly exciting.\n[2162s->2171s]The next question comes from Vivek Arya with Bank of America Securities. Your line is open.\n[2171s->2176s]Thanks for the question. Just a clarification for Colette first.\n[2176s->2184s]So on the China impact, I think previously it was mentioned that at about $15 billion, so you had the $8 billion in Q2.\n[2184s->2190s]So is there still some left as a headwind for the remaining quarters, just Colette, how to model that?\n[2190s->2201s]And then question, Jensen, for you. Back at GTC, you had outlined a path towards almost a trillion dollars of AI spending over the next few years.\n[2201s->2212s]Where are we in that build out? And do you think it's going to be uniform that you will see every spender, whether it's ESP, sovereigns, enterprises or build out?\n[2212s->2215s]Should we expect some periods of digestion in between?\n[2215s->2221s]What are your customer discussions telling you about how to model growth for next year?\n[2223s->2232s]Yes, Vivek. Thanks so much for the question regarding H20. Yes, we recognized 4.6 H20 in Q1.\n[2232s->2241s]We were unable to ship $2.5 billion. So the total for Q1 should have been $7 billion.\n[2241s->2248s]When we look at our Q2, our Q2 is going to be meaningfully down in terms of China data center revenue.\n[2248s->2258s]And we had highlighted in terms of the amount of orders that we had planned for H20 and Q2, and that was $8 billion.\n[2258s->2266s]Now, going forward, we did have other orders going forward that we will not be able to fulfill.\n[2266s->2275s]That is what was incorporated, therefore, in the amount that we wrote down of the $4.5 billion.\n[2275s->2280s]That write down was about inventory and purchase commitments.\n[2280s->2287s]And our purchase commitments were about what we expected regarding the orders that we had received.\n[2287s->2295s]Going forward, though, it's a bigger issue regarding the amount of the market that we will not be able to serve.\n[2295s->2306s]We assess that TAM to be close to about $50 billion in the future as we don't have a product to enable for the China.\n[2310s->2318s]Vivek, probably the best way to think through it is that AI is several things.\n[2318s->2326s]Of course, we know that AI is this incredible technology that's going to transform every industry,\n[2326s->2344s]from, of course, the way we do software to health care and financial services to retail to, I guess, every industry, transportation, manufacturing.\n[2344s->2347s]And we're at the beginning of that.\n[2347s->2352s]But maybe another way to think about that is where do we need intelligence?\n[2352s->2354s]Where do we need digital intelligence?\n[2354s->2358s]And it's in every country, it's in every industry.\n[2358s->2365s]And we know because of that, we recognize that AI is also an infrastructure.\n[2365s->2371s]It's a way of delivering a technology that requires factories.\n[2371s->2374s]And these factories produce tokens.\n[2374s->2380s]And they, as I mentioned, are important to every single industry in every single country.\n[2380s->2389s]And so on that basis, we're really at the very beginning of it because the adoption of this technology is really kind of in its early stages.\n[2389s->2399s]Now, we've reached an extraordinary milestone with AIs that are reasoning, are thinking, what people call inference time scaling.\n[2399s->2413s]And of course, it created a whole new, we've entered an era where inference is going to be a significant part of the compute workload.\n[2413s->2417s]But anyhow, it's going to be a new infrastructure.\n[2417s->2421s]And we're building it out in the clouds.\n[2421s->2429s]The United States is really the early starter and available in US clouds.\n[2429s->2432s]And this is our largest market, our largest installed base.\n[2432s->2435s]And we can continue to see that happening.\n[2435s->2442s]But beyond that, we're going to have to, we're going to see AI go into enterprise, which is on prem.\n[2442s->2447s]Because so much of the data is still on prem, access control is really important.\n[2447s->2452s]It's really hard to move all of every company's data into the cloud.\n[2452s->2455s]And so we're going to move AI into the enterprise.\n[2455s->2460s]And you saw that we announced a couple of really exciting new products.\n[2460s->2468s]Our RTX Pro Enterprise AI server that runs everything enterprise and AI.\n[2468s->2476s]Our DGX Spark and DGX Station, which is designed for developers who want to work on prem.\n[2476s->2481s]And so enterprise AI is just taking off.\n[2481s->2490s]Telcos, today, a lot of the telco infrastructure will be in the future software defined and built on AI.\n[2490s->2492s]And so 6G is going to be built on AI.\n[2492s->2495s]And that infrastructure needs to be built out.\n[2495s->2498s]And they said it's very, very early stages.\n[2498s->2505s]And then, of course, every factory today that makes things will have an AI factory that sits with it.\n[2505s->2513s]And the AI factory is going to be drive creating AI and operating AI for the factory itself.\n[2513s->2518s]But also to power the products and the things that are made by the factory.\n[2518s->2523s]So it's very clear that every car company will have AI factories.\n[2523s->2527s]And very soon there'll be robotics companies, robot companies.\n[2527s->2534s]And those companies will be also building AIs to drive the robots.\n[2534s->2542s]And so we're at the beginning of all of this build out.\n[2542s->2547s]The next question comes from CJ News with Cantor Fitzgerald.\n[2547s->2549s]Your line is open.\n[2549s->2552s]Yeah, good afternoon. Thank you for taking the question.\n[2552s->2556s]There have been many large GPU cluster investment announcements in the last month.\n[2556s->2560s]And you alluded to a few of them with Saudi Arabia, the UAE.\n[2560s->2563s]And then also, you know, we heard from Oracle and XAI, just to name a few.\n[2563s->2570s]So my question, are there other that have yet to be announced of the same kind of scale and magnitude?\n[2570s->2583s]And perhaps more importantly, how are these orders impacting your lead times for Blackwell and your current visibility sitting here today, you know, almost halfway through 2025?\n[2583s->2593s]Well, we have more orders today than we did at the last time I spoke about orders at GTC.\n[2593s->2599s]However, we're also increasing our supply chain and building out our supply chain.\n[2599s->2602s]They're doing a fantastic job.\n[2602s->2613s]We're building it here on shore in the United States, but we're going to keep our supply chain quite busy for several many more years coming.\n[2613s->2625s]And with respect to further announcements, I'm going to be on the road next week through Europe.\n[2625s->2640s]And it's just about every country needs to build out AI infrastructure and their umpteen AI factories being planned.\n[2640s->2650s]I think in the remarks, Colette mentioned there's some 100 AI factories being built.\n[2650s->2653s]There's a whole bunch that haven't been announced.\n[2653s->2673s]And I think the important concept here, which makes it easier to understand, is that like other technologies that impact literally every single industry, of course, electricity was one, and it became infrastructure.\n[2673s->2683s]Of course, the information infrastructure, which we now know as the Internet, affects every single industry, every country, every society.\n[2683s->2686s]Intelligence is surely one of those things.\n[2686s->2693s]I don't know any company, industry, country who thinks that intelligence is optional.\n[2693s->2694s]It's essential infrastructure.\n[2694s->2701s]And so we've now digitalized intelligence.\n[2701s->2708s]And so I think we're clearly in the beginning of the build out of this infrastructure.\n[2708s->2712s]And every country will have it.\n[2712s->2713s]I'm certain of that.\n[2713s->2714s]Every industry will use it.\n[2714s->2716s]That I'm certain of.\n[2716s->2722s]And what's unique about this infrastructure is that it needs factories.\n[2722s->2728s]You know, it's a little bit like the energy infrastructure, electricity.\n[2728s->2730s]It needs factories.\n[2730s->2733s]We need factories to produce this intelligence.\n[2733s->2736s]And the intelligence is getting more sophisticated.\n[2736s->2743s]We were talking about earlier that we had a huge breakthrough in the last couple of years with reasoning AI.\n[2743s->2748s]And now there are agents that reason and there are super agents that use a whole bunch of tools.\n[2748s->2754s]And then there's clusters of super agents where agents are working with agents, solving problems.\n[2754s->2769s]And so you could just imagine compared to one shot chat bots and the agents that are now using AI built on these large language models, how much more compute intensive they really need to be and are.\n[2769s->2774s]And so I think we're in the beginning of the build out.\n[2774s->2782s]And there should be many, many more announcements in the future.\n[2782s->2783s]Your next question.\n[2783s->2784s]Excuse me.\n[2784s->2788s]Your next question comes from Ben Reitz's with Melius.\n[2788s->2790s]Your line is open.\n[2790s->2791s]Yeah, hi.\n[2791s->2793s]Thanks for the question.\n[2793s->2801s]I wanted to ask first to Collette just a little clarification around the guidance and maybe putting it in a different way.\n[2801s->2813s]The 8 billion for H20 just seems like it's roughly 3 billion more than most people thought with regard to what you'd be foregoing in the second quarter.\n[2813s->2823s]So that would mean that with regard to your guidance, the rest of the business in order to hit 45 is doing 2 to 3 billion or so better.\n[2824s->2828s]So I was wondering if that math made sense to you.\n[2828s->2837s]And then in terms of the guidance, that would imply the non-China business is doing a bit better than the street expected.\n[2837s->2843s]So wondering what the primary driver was there in your view.\n[2843s->2851s]And then this second part of my question, Jensen, I know you guide one quarter at a time.\n[2851s->2871s]But with regard to the AI diffusion rule being lifted and this momentum was sovereign, there's been times in your history where you guys have said on calls like this where you have more conviction and sequential growth throughout the year, et cetera.\n[2871s->2886s]And given the unleashing of demand with AI diffusion being revoked and the supply chain increasing, does the environment give you more conviction and sequential growth as we go throughout the year?\n[2886s->2891s]So first one for Collette and then next one for Jensen. Thanks so much.\n[2891s->2908s]Thanks, Ben, for the question. When we look at our Q2 guidance and our commentary that we provided that had the export controls not occurred, we would have had orders of about 8 billion for H20.\n[2908s->2918s]That's correct. That was a possibility for what we would have had in our outlook for this quarter in Q2.\n[2918s->2932s]So what we also have talked about here is the growth that we've seen in Blackwell, Blackwell across many of our customers, as well as the growth that we continue to have in terms of supply that we need for our customers.\n[2932s->2938s]So putting those together, that's where we came through with the guidance that we provided.\n[2938s->2942s]I'm going to turn the rest over to Jensen to see how he wants to.\n[2942s->2945s]Yeah, thanks. Thanks, Ben.\n[2945s->2957s]I would say compared to the beginning of the year, compared to GTC timeframe, there are four positive surprises.\n[2957s->2967s]The first positive surprise is the step function demand increase of reasoning AI.\n[2967s->2977s]I think it is fairly clear now that AI is going through an exponential growth, and reasoning AI really busts it through.\n[2977s->2986s]Concerns about hallucination or its ability to really solve problems.\n[2986s->2999s]I think a lot of people are crossing that barrier and realizing how incredibly effective agentic AI is and reasoning AI is.\n[2999s->3007s]So number one is inference reasoning and the exponential growth there, demand growth.\n[3007s->3012s]The second one, you mentioned AI diffusion.\n[3012s->3019s]It's really terrific to see that the AI diffusion rule was rescinded.\n[3019s->3032s]President Trump wants America to win, and he also realizes that we're not the only country in the race.\n[3032s->3042s]He wants the United States to win and recognizes that we have to get the American stack out to the world\n[3042s->3047s]and have the world build on top of American stacks instead of alternatives.\n[3047s->3052s]And so AI diffusion happened.\n[3052s->3062s]The rescinding of it happened at almost precisely the time that the countries around the world are awakening the importance of AI as an infrastructure,\n[3062s->3072s]not just as a technology of great curiosity and great importance, but infrastructure for their industries and startups and society.\n[3072s->3078s]Just as they had to build out infrastructure for electricity and Internet, you got to build out an infrastructure for AI.\n[3078s->3083s]I think that that's an awakening and that creates a lot of opportunity.\n[3083s->3085s]The third is enterprise AI.\n[3085s->3093s]Agents work and agents are doing these agents are really quite successful.\n[3093s->3098s]Much more than generative AI, agentic AI is game changing.\n[3098s->3113s]Agents can understand ambiguous and rather implicit instructions and able to problem solve and use tools and have memory and so on.\n[3113s->3118s]And so I think enterprise AI is ready to take off.\n[3118s->3133s]And it's taken us a few years to build a computing system that is able to integrate, run enterprise AI stacks, run enterprise IT stacks, but add AI to it.\n[3133s->3142s]And this is the RTX Pro Enterprise server that we announced at Computex just last week.\n[3142s->3149s]And just about every major IT company has joined us super excited about that.\n[3149s->3152s]And so computing is one stat, one part of it.\n[3152s->3157s]But remember, enterprise IT is really three pillars.\n[3157s->3161s]It's compute, storage and networking.\n[3161s->3164s]And we've now put all three of them together for finally.\n[3164s->3167s]And we're going to market with that.\n[3167s->3170s]And then lastly, industrial AI.\n[3170s->3186s]Remember, one of the implications of the world reordering, if you will, is regions onshore manufacturing and building plants everywhere.\n[3186s->3195s]In addition to AI factories, of course, there are new electronics manufacturing chip manufacturing being built around the world.\n[3195s->3210s]And all of these new plants and these new factories are creating exactly the right time when Omniverse and AI and all the work that we're doing with robotics is emerging.\n[3210s->3215s]And so this fourth pillar is quite important.\n[3215s->3218s]Every factory will have an AI factory associated with it.\n[3218s->3226s]And in order to create these physical AI systems, you really have to train a vast amount of data.\n[3226s->3232s]So back to more data, more training, more AIs to be created, more computers.\n[3232s->3243s]And so these four drivers are really kicking into turbocharge.\n[3243s->3248s]Your next question comes from Timothy R. Curie with UBS.\n[3248s->3250s]Your line is open.\n[3250s->3251s]Thanks a lot.\n[3251s->3253s]Jensen, I wanted to ask about China.\n[3253s->3257s]It sounds like the July guidance assumes there's no SKU replacement for the H-20.\n[3257s->3262s]But if the president wants the U.S. to win, it seems like you're going to have to be allowed to ship something into China.\n[3262s->3265s]So I guess I had two points on that.\n[3265s->3269s]First of all, have you been approved to ship a new modified version into China?\n[3269s->3272s]And you're currently building it, but you just can't ship it in fiscal Q2.\n[3272s->3277s]And then you were sort of run rating $7 to $8 billion a quarter into China.\n[3277s->3283s]Can we get back to those sorts of quarterly run rates once you get something that you're allowed to ship back into China?\n[3283s->3287s]I think we're all trying to figure out how much to add back to our models and when.\n[3287s->3289s]So whatever you can say there would be great.\n[3289s->3291s]Thanks.\n[3291s->3293s]The president has a plan.\n[3293s->3296s]He has a vision and I trust him.\n[3296s->3305s]With respect to our export controls, it's a set of limits.\n[3305s->3322s]And the new set of limits pretty much make it impossible for us to reduce Hopper any further for any productive use.\n[3322s->3330s]And so the new limits, it's kind of the end of the road for Hopper.\n[3330s->3333s]We have limited options.\n[3333s->3337s]And so the key is to understand the limits.\n[3337s->3346s]The key is to understand the limits and see if we can come up with interesting products that could continue to serve the Chinese market.\n[3346s->3350s]We don't have anything at the moment.\n[3350s->3352s]But we're considering it.\n[3352s->3354s]We're thinking about it.\n[3354s->3358s]Obviously the limits are quite stringent at the moment.\n[3358s->3363s]And we have nothing to announce today.\n[3363s->3373s]And when the time comes, you know, we'll engage the administration and discuss that.\n[3373s->3378s]Your final question comes from the line of Aaron Rakers with Wells Fargo.\n[3378s->3381s]Your line is open.\n[3381s->3382s]Hi.\n[3382s->3383s]This is Jake on for Aaron.\n[3383s->3386s]Thanks for taking the question and congrats on the great quarter.\n[3386s->3399s]I was wondering if you could give some additional color around the strength you saw within the networking business, particularly around the adoption of your Ethernet solutions at CSPs as well as your\n[3400s->3404s]any change you're seeing in network attach rates.\n[3404s->3407s]Yeah, thank you for that.\n[3407s->3415s]We now have three networking platforms, maybe four.\n[3415s->3425s]The first one is the scale up platform to turn a computer into a much larger computer.\n[3425s->3427s]Scaling up is incredibly hard to do.\n[3427s->3429s]Scaling out is easier to do.\n[3429s->3431s]But scaling up is hard to do.\n[3431s->3433s]And that platform is called NVLink.\n[3433s->3446s]And NVLink comes with it, chips and switches and, you know, NVLink spines.\n[3446s->3448s]It's really complicated.\n[3448s->3452s]But anyways, that's our new platform, scale up platform.\n[3452s->3455s]In addition to InfiniBand, we also have SpectrumX.\n[3455s->3466s]We've been fairly, fairly consistent that Ethernet was designed for a lot of traffic that are independent.\n[3466s->3471s]But in the case of AI, you have a lot of computers working together.\n[3471s->3477s]And the traffic of AI is insanely bursty.\n[3477s->3479s]Latency matters a lot.\n[3479s->3484s]Because the AI is thinking and it wants to get work done as quickly as possible.\n[3484s->3487s]And you got a whole bunch of nodes working together.\n[3487s->3499s]And so we enhanced Ethernet, added capabilities like extremely low latency, congestion control, adaptive routing,\n[3499s->3506s]the type of technologies that were available only in InfiniBand to Ethernet.\n[3506s->3511s]And as a result, we improved the utilization of Ethernet in these clusters.\n[3511s->3519s]These clusters are gigantic from as low as 50% to as high as 85%, 90%.\n[3519s->3531s]And so the difference is if you had a cluster that's $10 billion and you improved its effectiveness by 40%, that's worth $4 billion.\n[3532s->3534s]It's incredible.\n[3534s->3538s]And so SpectrumX has been really, quite frankly, a home run.\n[3538s->3553s]And this last quarter, as we said in the prepared remarks, we added two very significant CSPs to the SpectrumX adoption.\n[3553s->3558s]And then the last one, the last one is Bluefield, which is our control plane.\n[3558s->3566s]And so in those four, the control plane and network, which is used for storage, is used for security,\n[3566s->3576s]and for many of these clusters that want to achieve isolation among its users, multi-tenant clusters,\n[3576s->3583s]and still be able to use and have extremely high performance, bare metal performance,\n[3584s->3591s]Bluefield is ideal for that and is used in a lot of these cases.\n[3591s->3599s]And so we have these four networking platforms that are all growing and we're doing really well.\n[3599s->3601s]I'm very proud of the team.\n[3603s->3607s]That is all the time we have for questions.\n[3607s->3610s]Denton, I will turn the call back to you.\n[3610s->3611s]Thank you.\n[3611s->3612s]Thank you.\n[3612s->3615s]This is the start of a powerful new wave of growth.\n[3615s->3617s]Grace Blackwell is in full production.\n[3617s->3619s]We're off to the races.\n[3619s->3621s]We now have multiple significant growth engines.\n[3621s->3627s]Inference, one's delighted workload is surging with revenue generating AI services.\n[3627s->3635s]AI is growing faster and will be larger than any platform shifts before, including the internet, mobile, and cloud.\n[3635s->3645s]Blackwell is built to power the full AI lifecycle from training frontier models to running complex inference and reasoning agents at scale.\n[3645s->3655s]Training demands continues to rise with breakthroughs in post-training and like reinforcement learning and synthetic data generation.\n[3655s->3657s]But inference is exploding.\n[3657s->3661s]Reasoning AI agents require orders of magnitude more compute.\n[3661s->3667s]The foundations of our next growth platforms are in place and ready to scale.\n[3667s->3674s]Sovereign AI nations are investing in AI infrastructure like they once did for electricity and internet.\n[3674s->3681s]Enterprise AI, AI must be deployable on prem and integrated with existing IT.\n[3681s->3692s]Our RTX Pro, DGX Spark, and DGX Station Enterprise AI systems are ready to modernize the $500 billion IT infrastructure on prem or in the cloud.\n[3692s->3696s]Every major IT provider is partnering with us.\n[3696s->3710s]Industrial AI from training to digital twin simulation to deployment, NVIDIA Omniverse and Isaac Groot are powering next generation factories and humanoid robotic systems worldwide.\n[3710s->3723s]The age of AI is here from AI infrastructures, inference at scale, sovereign AI, enterprise AI, and industrial AI.\n[3723s->3725s]NVIDIA is ready.\n[3725s->3740s]Join us at GTC Paris, our keynote at VivaTech on June 11, talking about quantum GPU computing, robotic factories and robots, and celebrate our partnerships building AI factories across the region.\n[3740s->3745s]The NVIDIA Band will tour France, the UK, Germany, and Belgium.\n[3745s->3753s]Thank you for joining us at the earnings call today. See you in Paris.\n[3753s->3758s]This concludes today's conference call. You may now disconnect.\n",
            "Item 2. Management's Discussion and Analysis of Financial Condition and Results of Operations\n\nForward-Looking Statements \nThis Quarterly Report on Form 10-Q contains forward-looking statements within the meaning of Section 27A of the Securities Act of 1933, as amended, and\nSection 21E of the Securities Exchange Act of 1934, as amended, which are subject to the “safe harbor” created by those sections based on management’s\nbeliefs and assumptions and on information currently available to management. In some cases, you can identify forward-looking statements by terms such as\n“may,” “will,” “should,” “could,” “goal,” “would,” “expect,” “plan,” “anticipate,” “believe,” “estimate,” “project,” “predict,” “potential” and similar expressions intended\nto identify forward-looking statements. These statements involve known and unknown risks, uncertainties and other factors, which may cause our actual results,\nperformance, time frames or achievements to be materially different from any future results, performance, time frames or achievements expressed or implied by\nthe forward-looking statements. We discuss many of these risks, uncertainties and other factors in this Quarterly Report on Form 10-Q and our Annual Report on\nForm 10-K for the fiscal year ended January 26, 2025 in greater detail under the heading “Risk Factors” of such reports. Given these risks, uncertainties, and\nother factors, you should not place undue reliance on these forward-looking statements. Also, these forward-looking statements represent our estimates and\nassumptions only as of the date of this filing. You should read this Quarterly Report on Form 10-Q completely and understand that our actual future results may\nbe materially different from what we expect. We hereby qualify our forward-looking statements by these cautionary statements. Except as required by law, we\nassume no obligation to update these forward-looking statements publicly, or to update the reasons actual results could differ materially from those anticipated in\nthese forward-looking statements, even if new information becomes available in the future.\n\nAll references to “NVIDIA,” “we,” “us,” “our” or the “Company” mean NVIDIA Corporation and its subsidiaries.\n\n© 2025 NVIDIA Corporation. All rights reserved.\n\nThe following discussion and analysis of our financial condition and results of operations should be read in conjunction with the risk factors set forth in Item 1A.\n“Risk Factors” of our Annual Report on Form 10-K for the fiscal year ended January 26, 2025 and Part II, Item 1A. “Risk Factors” of this Quarterly Report on\nForm 10-Q and our Condensed Consolidated Financial Statements and related Notes thereto, as well as other cautionary statements and risks described\nelsewhere in this Quarterly Report on Form 10-Q and our other filings with the SEC, before deciding to purchase, hold, or sell shares of our common stock.\n\nOverview\n\nOur Company and Our Businesses\n\nNVIDIA pioneered accelerated computing to help solve the most challenging computational problems. Since our original focus on PC graphics, we have\nexpanded to several other large and important computationally intensive fields. Fueled by the sustained demand for exceptional 3D graphics and the scale of the\ngaming market, NVIDIA has leveraged its GPU architecture to create platforms for scientific computing, AI, data science, AV, robotics, and digital twin\napplications.\n\nOur two operating segments are \"Compute & Networking\" and \"Graphics,\" as described in Note 13 of the Notes to Condensed Consolidated Financial\nStatements in Part I, Item 1 of this Quarterly Report on Form 10-Q.\n\nHeadquartered in Santa Clara, California, NVIDIA was incorporated in California in April 1993 and reincorporated in Delaware in April 1998.\n\nRecent Developments, Future Objectives and Challenges\n\nRevenue growth in the first quarter of fiscal year 2026 was driven by data center compute and networking platforms for accelerated computing and AI solutions.\nOur Blackwell GPU revenue ramp continued during the quarter, extending beyond large cloud service providers to include consumer internet companies and\nregional cloud service providers.\n\nOn April 9, 2025, the U.S. government, or USG, informed us that it requires a license for export to China (including Hong Kong and Macau) and D:5 countries, or\nto companies headquartered or with an ultimate parent therein, of our H20 integrated circuits and any other circuits achieving the H20’s memory bandwidth,\ninterconnect bandwidth, or combination thereof. As a result of these new requirements, we incurred a $4.5 billion charge in the first quarter of fiscal year 2026\nassociated with H20 excess inventory and purchase obligations as the demand for H20 products diminished. The $4.5 billion charge was less than what we\ninitially anticipated as we were able to re-use certain materials. Sales of our H20 products were $4.6 billion for the first quarter of fiscal year 2026 prior to the\nnew export licensing requirements. The H20 export licensing requirements have impacted our current revenue and will also negatively affect our future revenue.\nWe are still evaluating our limited options to supply Data Center compute products compliant with the USG’s export control rules. The export controls applicable\nto China are complex and address a variety of parameters, including the total processing performance of a chip, the “performance density” of a chip, the\ninterconnect bandwidth of a chip, and the memory bandwidth of a chip. We may be unable to create a competitive product for China’s data center market that\nreceives approval from the USG. In that event, we would effectively be foreclosed from competing in China's data center computing/compute market, with a\nmaterial and adverse impact on our business, operating results, and financial condition.\n\n22",
            "Reportable segment operating income – The year over year increase in Compute & Networking segment operating income in the first quarter of fiscal year 2026\nwas driven by growth in revenue, partially offset by a $4.5 billion charge associated with H20 excess inventory and purchase obligations. The year over year\nincrease in Graphics segment operating income in the first quarter of fiscal year 2026 was driven by growth in revenue.\n\nAll Other operating loss – The year over year increase in the first quarter of fiscal year 2026 was due to an increase in stock-based compensation expense\nreflecting employee growth.\n\nConcentration of Revenue\n\nWe refer to customers who purchase products directly from NVIDIA as direct customers, such as add-in board manufacturers, distributors, ODMs, OEMs, and\nsystem integrators. We have certain customers that may purchase products directly from NVIDIA and may use either internal resources or third-party system\nintegrators to complete their build. We also have indirect customers, who purchase products through our direct customers; indirect customers include CSPs,\nconsumer internet companies, enterprises, and public sector entities.\n\nDirect Customers – Sales to one direct customer, Customer A, represented 16% of total revenue and sales to a second direct customer, Customer B,\nrepresented 14% of total revenue for the first quarter of fiscal year 2026, both of which were attributable to the Compute & Networking segment. Sales to two\ndirect customers represented 11% and 13% of total revenue for the first quarter of fiscal year 2025, both of which were attributable to the Compute & Networking\nsegment.\n\nIndirect Customers – Indirect customer revenue is an estimation based upon multiple factors including customer purchase order information, product\nspecifications, internal sales data, and other sources. Actual indirect customer revenue may differ from our estimates. For the first quarter of fiscal year 2026, two\nindirect customers which primarily purchase our products through system integrators and distributors, including through Direct Customers A and B, are estimated\nto represent 10% or more of total revenue, attributable to the Compute & Networking segment.\n\nWe estimate that in the first quarter of fiscal year 2026, an AI research and deployment company contributed to a meaningful amount of our revenue, through\none of the above indirect customers and through other indirect customers that provide cloud services.\n\nWe have experienced periods where we receive a significant amount of our revenue from a limited number of customers, and this trend may continue.\n\nRevenue by geographic region is designated based on the billing location even if the revenue may be attributable to indirect customers in a different location.\nRevenue from sales to customers outside of the United States accounted for 53% and 48% of total revenue for the first quarter of fiscal years 2026 and 2025,\nrespectively.\n\nGross Profit and Gross Margin\n\nGross profit consists of total net revenue less cost of revenue. Cost of revenue consists primarily of the cost of semiconductors, including wafer fabrication,\nassembly, testing and packaging, board and device costs, manufacturing support costs, including labor and overhead associated with such purchases, final test\nyield fallout, inventory and warranty provisions, memory and component costs, tariffs, and shipping costs. Cost of revenue also includes acquisition-related\nintangible amortization expense, costs for license and development and service arrangements, IP-related costs, and stock-based compensation related to\npersonnel associated with manufacturing operations.\n\nGross margins decreased to 60.5% for the first quarter of fiscal year 2026 compared to 78.4% for the first quarter of fiscal year 2025, primarily due to a $4.5\nbillion charge associated with H20 excess inventory and purchase obligations and the initial ramp of more sophisticated systems within Data Center.\n\nProvisions for inventory and excess inventory purchase obligations totaled $5.3 billion, including $4.5 billion associated with H20 excess inventory and purchase\nobligations, and $393 million for the first quarter of fiscal years 2026 and 2025, respectively. Sales of previously reserved inventory and settlements of excess\ninventory purchase obligations resulted in a provision release of $436 million and $114 million for the first quarter of fiscal years 2026 and 2025, respectively.\nThe net effect of provisions for inventory and excess inventory purchase obligations on our gross margin was an unfavorable impact of 11.0% and 1.1% in the\nfirst quarter of fiscal years 2026 and 2025, respectively.\n\n26"
        ]
    },
    {
        "question": "Blackwell 平台在本季度的 ramp-up 情况如何？",
        "ground_truth": "Blackwell 平台的 ramp-up 是公司历史上最快的。在 2026 财年第一季度，Blackwell 占数据中心计算收入的近 70%。",
        "answer": "Blackwell 平台在本季度的 ramp-up 情况非常好。根据财务报告，Blackwell 在数据中心计算收入中占据了近 70% 的份额，这表明其在市场中的普及率和接受度非常高。此外，报告还指出，Blackwell 的部署速度是英伟达公司历史上最快的，这进一步证明了其在市场上的强劲表现。此外，报告中提到，几乎每星期都有大约 1,000 个 NVL72 机架或 72,000 个 Blackwell GPU 被主要超大规模计算提供商部署，预计在第二季度将进一步增加产量 [来源: Q2_2026_outlook, 第 3 页]。",
        "contexts": [
            "Q2 FY26 Outlook\n\nRevenue\n\n$45.0 billion, plus or minus 2%\nExpect modest sequential growth across all our platforms\nContinued ramp of Blackwell to be partially offset by a decline in China revenue\nOutlook reflects a loss in H20 revenue of ~$8.0 billion\n\nGross Margins\n\n71.8% GAAP and 72.0% non-GAAP, plus or minus 50 basis points\nExpect better Blackwell profitability to drive a modest sequential improvement\nWorking toward achieving gross margins in the mid-70% range late this year\n\nOperating Expense\nApproximately $5.7 billion GAAP and $4.0 billion non-GAAP\nExpect full year FY26 operating expense growth to be in the mid-30% range\n\nOther Income & Expense\nIncome of approximately $450 million for GAAP and non-GAAP\nExcluding gains and losses from non-marketable and publicly-held equity securities\n\nTax Rate\n16.5% GAAP and non-GAAP, plus or minus 1%, excluding discrete items\n\nRefer to Appendix for reconciliation of Non-GAAP measures.",
            "Highlights\n\nStrong quarter exceeding our outlook in a challenging operating environment\n\n• Total revenue up 69% Y/Y to $44.1B, above outlook of $43.0B +/- 2%\n• Data Center up 73% Y/Y to $39.1B\n• Gaming up 42% Y/Y to $3.8B\n\nData Center revenue driven by the continued Blackwell ramp – fastest in our company’s history\n\n• Seeing a sharp jump in inference demand; Microsoft, Google, OpenAI all seeing a step-function leap in token generation\n• AI factory deployments accelerating; nearly 100 NVIDIA-powered AI factories in flight this quarter, up 2x y/y; average number of GPUs\n\npowering each factory also doubling in the same period\n• Have line of sight to AI factory projects requiring tens of gigawatts of NVIDIA AI infrastructure in the not-too-distant future\n\nRecord Gaming revenue due to Blackwell adoption by gamers, creatives, and AI enthusiasts\n\n• Against robust demand backdrop, we greatly improved our supply and availability in Q1 and expect to continue these efforts in Q2\n• Brought Blackwell architecture to mainstream gaming with the launch of GeForce RTX 5060 and 5060 Ti",
            "[IMAGE: Q1_2026_presentation_page7_img1.png]\n\nHighlights\n\n• Recognized $4.6B in H20 revenue; also recognized a $4.5 billion\n\ncharge – write down of inventory and purchase obligations, less than \nanticipated; unable to ship $2.5B in H20 due to export controls\n\n• Blackwell nearly 70% of DC compute revenue in Q1; Hopper\n\ntransition nearly complete\n\n• Major hyperscalers deploying nearly 1,000 NVL72 racks or 72,000\n\nBlackwell GPUs per week; on track to further ramp output in Q2\n\n• Sampling of GB300 systems began earlier in May at major CSPs;\n\nexpect production shipments to commence later in Q2\n\n• Networking grew 64% q/q\n\n• NVLink a new growth vector; Q1 shipments exceeded a billion dollars\n\n• Strong q/q & y/y Spectrum-X growth; now annualizing over $8B revenue\n\n• Spectrum-X adoption widespread: CoreWeave, Microsoft Azure, Oracle\n\nCloud, and xAI; this quarter we added Google Cloud and Meta\n\nData Center\n\nRevenue ($M)"
        ]
    },
    {
        "question": "2026财年第一季度的 GAAP 稀释每股收益（Diluted EPS）是多少？",
        "ground_truth": "截至 2025 年 4 月 27 日的 2026 财年第一季度，GAAP 稀释每股收益（Diluted EPS）为 0.76 美元。",
        "answer": "根据提供的财务报表信息，2026财年第一季度的GAAP稀释每股收益（Diluted EPS）为0.76美元 [来源: NVIDIA Corporation and Subsidiaries, Condensed Consolidated Statements of Income, Three Months Ended Apr 27, 2025]。",
        "contexts": [
            "NVIDIA Corporation and Subsidiaries\nNotes to Condensed Consolidated Financial Statements\n\nNote 1 - Summary of Significant Accounting Policies\n\nBasis of Presentation\n\nThe accompanying unaudited condensed consolidated financial statements were prepared in accordance with accounting principles generally accepted in the\nUnited States of America, or U.S. GAAP, for interim financial information and with the instructions to Form 10-Q and Article 10 of Securities and Exchange\nCommission, or SEC, Regulation S-X. The January  26, 2025 consolidated balance sheet was derived from our audited consolidated financial statements\nincluded in our Annual Report on Form 10-K for the fiscal year ended January 26, 2025, as filed with the SEC, but does not include all disclosures required by\nU.S. GAAP. In the opinion of management, all adjustments, consisting only of normal recurring adjustments considered necessary for a fair presentation of\nresults of operations and financial position, have been included. The results for the interim periods presented are not necessarily indicative of the results\nexpected for any future period. The following information should be read in conjunction with the audited consolidated financial statements and notes thereto\nincluded in our Annual Report on Form 10-K for the fiscal year ended January 26, 2025.\n\nCertain balances from the prior fiscal year have been reclassified to conform to the current period presentation.\n\nIn June 2024, we executed a ten-for-one stock split of our common stock. All share, equity award and per share amounts presented herein have been\nretrospectively adjusted to reflect the stock split.\n\nSignificant Accounting Policies\nThere have been no material changes to our significant accounting policies disclosed in Note 1 - Organization and Summary of Significant Accounting Policies,\nof the Notes to the Consolidated Financial Statements included in our Annual Report on Form 10-K for the fiscal year ended January 26, 2025.\n\nFiscal Year\n\nFiscal years 2026 and 2025 are both 52-week years ending on the last Sunday in January. The first quarters of fiscal years 2026 and 2025 were both 13-week\nquarters.\n\nPrinciples of Consolidation\n\nOur condensed consolidated financial statements include the accounts of NVIDIA Corporation and our wholly-owned subsidiaries. All intercompany balances\nand transactions have been eliminated in consolidation.\n\nUse of Estimates\n\nThe preparation of financial statements in conformity with U.S. GAAP requires management to make estimates and assumptions that affect the reported\namounts of assets and liabilities and disclosures of contingent assets and liabilities at the date of the financial statements and the reported amounts of revenue\nand expenses during the reporting period. Actual results could differ materially from our estimates. On an on-going basis, we evaluate our estimates, including\nthose related to accounts receivable, cash equivalents and marketable securities, goodwill, income taxes, inventories and product purchase commitments,\ninvestigation and settlement costs, litigation, non-marketable equity securities, other contingencies, property, plant, and equipment, restructuring and other\ncharges, revenue recognition, and stock-based compensation. These estimates are based on historical facts and various other assumptions that we believe are\nreasonable.\n\nRecently Issued Accounting Pronouncements\n\nRecent Accounting Pronouncements Not Yet Adopted\n\nIn December 2023, the FASB issued a new accounting standard which includes new and updated income tax disclosures, including disaggregation of\ninformation in the rate reconciliation and income taxes paid. We will adopt this standard in our fiscal year 2026 annual report. We are currently assessing the\neffect of the adoption of this standard on our disclosures that will be included in our Form 10-K for the year ending January 25, 2026.\n\nIn November 2024, the FASB issued a new accounting standard requiring disclosures of certain additional expense information on an annual and interim basis,\nincluding, among other items, the amounts of purchases of inventory, employee compensation, depreciation and intangible asset amortization included within\neach income statement expense caption, as applicable. We will adopt this standard in our fiscal year 2028 annual report. We do not expect the adoption of this\nstandard to have a material impact on our Consolidated Financial Statements other than additional disclosures.\n\n8",
            "Item 2. Management's Discussion and Analysis of Financial Condition and Results of Operations\n\nForward-Looking Statements \nThis Quarterly Report on Form 10-Q contains forward-looking statements within the meaning of Section 27A of the Securities Act of 1933, as amended, and\nSection 21E of the Securities Exchange Act of 1934, as amended, which are subject to the “safe harbor” created by those sections based on management’s\nbeliefs and assumptions and on information currently available to management. In some cases, you can identify forward-looking statements by terms such as\n“may,” “will,” “should,” “could,” “goal,” “would,” “expect,” “plan,” “anticipate,” “believe,” “estimate,” “project,” “predict,” “potential” and similar expressions intended\nto identify forward-looking statements. These statements involve known and unknown risks, uncertainties and other factors, which may cause our actual results,\nperformance, time frames or achievements to be materially different from any future results, performance, time frames or achievements expressed or implied by\nthe forward-looking statements. We discuss many of these risks, uncertainties and other factors in this Quarterly Report on Form 10-Q and our Annual Report on\nForm 10-K for the fiscal year ended January 26, 2025 in greater detail under the heading “Risk Factors” of such reports. Given these risks, uncertainties, and\nother factors, you should not place undue reliance on these forward-looking statements. Also, these forward-looking statements represent our estimates and\nassumptions only as of the date of this filing. You should read this Quarterly Report on Form 10-Q completely and understand that our actual future results may\nbe materially different from what we expect. We hereby qualify our forward-looking statements by these cautionary statements. Except as required by law, we\nassume no obligation to update these forward-looking statements publicly, or to update the reasons actual results could differ materially from those anticipated in\nthese forward-looking statements, even if new information becomes available in the future.\n\nAll references to “NVIDIA,” “we,” “us,” “our” or the “Company” mean NVIDIA Corporation and its subsidiaries.\n\n© 2025 NVIDIA Corporation. All rights reserved.\n\nThe following discussion and analysis of our financial condition and results of operations should be read in conjunction with the risk factors set forth in Item 1A.\n“Risk Factors” of our Annual Report on Form 10-K for the fiscal year ended January 26, 2025 and Part II, Item 1A. “Risk Factors” of this Quarterly Report on\nForm 10-Q and our Condensed Consolidated Financial Statements and related Notes thereto, as well as other cautionary statements and risks described\nelsewhere in this Quarterly Report on Form 10-Q and our other filings with the SEC, before deciding to purchase, hold, or sell shares of our common stock.\n\nOverview\n\nOur Company and Our Businesses\n\nNVIDIA pioneered accelerated computing to help solve the most challenging computational problems. Since our original focus on PC graphics, we have\nexpanded to several other large and important computationally intensive fields. Fueled by the sustained demand for exceptional 3D graphics and the scale of the\ngaming market, NVIDIA has leveraged its GPU architecture to create platforms for scientific computing, AI, data science, AV, robotics, and digital twin\napplications.\n\nOur two operating segments are \"Compute & Networking\" and \"Graphics,\" as described in Note 13 of the Notes to Condensed Consolidated Financial\nStatements in Part I, Item 1 of this Quarterly Report on Form 10-Q.\n\nHeadquartered in Santa Clara, California, NVIDIA was incorporated in California in April 1993 and reincorporated in Delaware in April 1998.\n\nRecent Developments, Future Objectives and Challenges\n\nRevenue growth in the first quarter of fiscal year 2026 was driven by data center compute and networking platforms for accelerated computing and AI solutions.\nOur Blackwell GPU revenue ramp continued during the quarter, extending beyond large cloud service providers to include consumer internet companies and\nregional cloud service providers.\n\nOn April 9, 2025, the U.S. government, or USG, informed us that it requires a license for export to China (including Hong Kong and Macau) and D:5 countries, or\nto companies headquartered or with an ultimate parent therein, of our H20 integrated circuits and any other circuits achieving the H20’s memory bandwidth,\ninterconnect bandwidth, or combination thereof. As a result of these new requirements, we incurred a $4.5 billion charge in the first quarter of fiscal year 2026\nassociated with H20 excess inventory and purchase obligations as the demand for H20 products diminished. The $4.5 billion charge was less than what we\ninitially anticipated as we were able to re-use certain materials. Sales of our H20 products were $4.6 billion for the first quarter of fiscal year 2026 prior to the\nnew export licensing requirements. The H20 export licensing requirements have impacted our current revenue and will also negatively affect our future revenue.\nWe are still evaluating our limited options to supply Data Center compute products compliant with the USG’s export control rules. The export controls applicable\nto China are complex and address a variety of parameters, including the total processing performance of a chip, the “performance density” of a chip, the\ninterconnect bandwidth of a chip, and the memory bandwidth of a chip. We may be unable to create a competitive product for China’s data center market that\nreceives approval from the USG. In that event, we would effectively be foreclosed from competing in China's data center computing/compute market, with a\nmaterial and adverse impact on our business, operating results, and financial condition.\n\n22",
            "[TABLE]\n| Revenue | $ | 44,062 |  | $ | 26,044 |\n| --- | --- | --- | --- | --- | --- |\n| Cost of revenue | 17,394 |  |  | 5,638 |  |\n| Gross profit | 26,668 |  |  | 20,406 |  |\n| Operating expenses |  |  |  |  |  |\n| Research and development | 3,989 |  |  | 2,720 |  |\n| Sales, general and administrative | 1,041 |  |  | 777 |  |\n| Total operating expenses | 5,030 |  |  | 3,497 |  |\n| Operating income | 21,638 |  |  | 16,909 |  |\n| Interest income | 515 |  |  | 359 |  |\n| Interest expense | (63) |  |  | (64) |  |\n| Other income (expense), net | (180) |  |  | 75 |  |\n| Total other income (expense), net | 272 |  |  | 370 |  |\n| Income before income tax | 21,910 |  |  | 17,279 |  |\n| Income tax expense | 3,135 |  |  | 2,398 |  |\n| Net income | $ | 18,775 |  | $ | 14,881 |\n|  |  |  |  |  |  |\n| Net income per share: |  |  |  |  |  |\n| Basic | $ | 0.77 |  | $ | 0.60 |\n| Diluted | $ | 0.76 |  | $ | 0.60 |\n|  |  |  |  |  |  |\n| Weighted average shares used in per share computation: |  |  |  |  |  |\n| Basic | 24,441 |  |  | 24,620 |  |\n| Diluted | 24,611 |  |  | 24,890 |  |\n[/TABLE]\n\n该表格展示了公司收入和利润情况。主要趋势包括：总收入增长，毛利率上升，运营费用增加，净利润增长，每股收益提升。\n\nPart I. Financial Information\n\nItem 1. Financial Statements (Unaudited)\n\nNVIDIA Corporation and Subsidiaries\nCondensed Consolidated Statements of Income\n\n(In millions, except per share data)\n\nThree Months Ended\n\nApr 27, 2025\nApr 28, 2024\n\nSee accompanying Notes to Condensed Consolidated Financial Statements.\n\n3"
        ]
    },
    {
        "question": "针对 H20 产品的出口管制对公司 Q1 的财务有什么具体影响？",
        "ground_truth": "由于美国政府 2025 年 4 月 9 日的出口管制新规，H20 产品需求减少，导致公司在 Q1 计提了 45 亿美元的过剩库存和采购义务费用。在新规生效前，H20 在 Q1 的销售额为 46 亿美元。",
        "answer": "针对 H20 产品的出口管制对公司 Q1 的财务有以下具体影响：\n\n1. **收入影响**：由于新的出口管制要求，公司无法销售 H20 产品，导致第一季度 H20 产品收入为 46 亿美元，这一数字是在 2025 财年 4 月 9 日之前实现的。\n\n2. **库存和采购义务减记**：由于 H20 产品需求下降，公司对 H20 过剩库存和采购义务进行了减记，产生了 45 亿美元的支出。这一支出低于公司最初的预期，因为公司能够重新利用某些材料。\n\n3. **无法发货**：由于新的出口管制，公司无法发货 25 亿美元的 H20 产品收入。\n\n4. **对数据中心收入的影响**：由于 H20 的出口管制，中国数据中心收入低于预期，预计第二季度中国数据中心收入将显著下降。\n\n这些因素共同导致公司 Q1 的财务表现受到影响，包括收入减少和支出增加。 [来源: Q1_2026_presentation, 第 22-23 页]",
        "contexts": [
            "Item 2. Management's Discussion and Analysis of Financial Condition and Results of Operations\n\nForward-Looking Statements \nThis Quarterly Report on Form 10-Q contains forward-looking statements within the meaning of Section 27A of the Securities Act of 1933, as amended, and\nSection 21E of the Securities Exchange Act of 1934, as amended, which are subject to the “safe harbor” created by those sections based on management’s\nbeliefs and assumptions and on information currently available to management. In some cases, you can identify forward-looking statements by terms such as\n“may,” “will,” “should,” “could,” “goal,” “would,” “expect,” “plan,” “anticipate,” “believe,” “estimate,” “project,” “predict,” “potential” and similar expressions intended\nto identify forward-looking statements. These statements involve known and unknown risks, uncertainties and other factors, which may cause our actual results,\nperformance, time frames or achievements to be materially different from any future results, performance, time frames or achievements expressed or implied by\nthe forward-looking statements. We discuss many of these risks, uncertainties and other factors in this Quarterly Report on Form 10-Q and our Annual Report on\nForm 10-K for the fiscal year ended January 26, 2025 in greater detail under the heading “Risk Factors” of such reports. Given these risks, uncertainties, and\nother factors, you should not place undue reliance on these forward-looking statements. Also, these forward-looking statements represent our estimates and\nassumptions only as of the date of this filing. You should read this Quarterly Report on Form 10-Q completely and understand that our actual future results may\nbe materially different from what we expect. We hereby qualify our forward-looking statements by these cautionary statements. Except as required by law, we\nassume no obligation to update these forward-looking statements publicly, or to update the reasons actual results could differ materially from those anticipated in\nthese forward-looking statements, even if new information becomes available in the future.\n\nAll references to “NVIDIA,” “we,” “us,” “our” or the “Company” mean NVIDIA Corporation and its subsidiaries.\n\n© 2025 NVIDIA Corporation. All rights reserved.\n\nThe following discussion and analysis of our financial condition and results of operations should be read in conjunction with the risk factors set forth in Item 1A.\n“Risk Factors” of our Annual Report on Form 10-K for the fiscal year ended January 26, 2025 and Part II, Item 1A. “Risk Factors” of this Quarterly Report on\nForm 10-Q and our Condensed Consolidated Financial Statements and related Notes thereto, as well as other cautionary statements and risks described\nelsewhere in this Quarterly Report on Form 10-Q and our other filings with the SEC, before deciding to purchase, hold, or sell shares of our common stock.\n\nOverview\n\nOur Company and Our Businesses\n\nNVIDIA pioneered accelerated computing to help solve the most challenging computational problems. Since our original focus on PC graphics, we have\nexpanded to several other large and important computationally intensive fields. Fueled by the sustained demand for exceptional 3D graphics and the scale of the\ngaming market, NVIDIA has leveraged its GPU architecture to create platforms for scientific computing, AI, data science, AV, robotics, and digital twin\napplications.\n\nOur two operating segments are \"Compute & Networking\" and \"Graphics,\" as described in Note 13 of the Notes to Condensed Consolidated Financial\nStatements in Part I, Item 1 of this Quarterly Report on Form 10-Q.\n\nHeadquartered in Santa Clara, California, NVIDIA was incorporated in California in April 1993 and reincorporated in Delaware in April 1998.\n\nRecent Developments, Future Objectives and Challenges\n\nRevenue growth in the first quarter of fiscal year 2026 was driven by data center compute and networking platforms for accelerated computing and AI solutions.\nOur Blackwell GPU revenue ramp continued during the quarter, extending beyond large cloud service providers to include consumer internet companies and\nregional cloud service providers.\n\nOn April 9, 2025, the U.S. government, or USG, informed us that it requires a license for export to China (including Hong Kong and Macau) and D:5 countries, or\nto companies headquartered or with an ultimate parent therein, of our H20 integrated circuits and any other circuits achieving the H20’s memory bandwidth,\ninterconnect bandwidth, or combination thereof. As a result of these new requirements, we incurred a $4.5 billion charge in the first quarter of fiscal year 2026\nassociated with H20 excess inventory and purchase obligations as the demand for H20 products diminished. The $4.5 billion charge was less than what we\ninitially anticipated as we were able to re-use certain materials. Sales of our H20 products were $4.6 billion for the first quarter of fiscal year 2026 prior to the\nnew export licensing requirements. The H20 export licensing requirements have impacted our current revenue and will also negatively affect our future revenue.\nWe are still evaluating our limited options to supply Data Center compute products compliant with the USG’s export control rules. The export controls applicable\nto China are complex and address a variety of parameters, including the total processing performance of a chip, the “performance density” of a chip, the\ninterconnect bandwidth of a chip, and the memory bandwidth of a chip. We may be unable to create a competitive product for China’s data center market that\nreceives approval from the USG. In that event, we would effectively be foreclosed from competing in China's data center computing/compute market, with a\nmaterial and adverse impact on our business, operating results, and financial condition.\n\n22",
            "In addition to controls targeting D:4 and D:5 countries, the USG has also imposed worldwide export controls impacting our products, and may impose additional\ncontrols in the future. On January 15, 2025, the USG published the “AI Diffusion” IFR in the Federal Register. After a 120-day delayed compliance period, the\nIFR would have imposed a worldwide licensing requirement on all products classified under Export Control Classification Numbers, or ECCNs, 3A090.a,\n4A090.a, or corresponding .z ECCNs, including all related software and technology. The licensing requirement would have applied to our most popular data\ncenter products, such as our H200 and GB200. The AI Diffusion IFR would have divided the world into three tiers, relegating most countries to “Tier 2” status,\nand would have created a complex and burdensome scheme for licensing approvals.\n\nIn May 2025, the USG announced that it would rescind the AI Diffusion IFR and implement a replacement rule. The scope, timing, and requirements of the\nforthcoming rule remain uncertain. The replacement rule may impose new restrictions on our products or operations and/or add license requirements that could\nhave a material impact on our business, operating results, and financial condition.\n\nOur competitive position has been harmed by export controls, and our competitive position and future results may be further harmed, over the long term, if there\nare further changes in the USG’s export controls, including further expansion of the geographic, customer, or product scope of the controls, if customers\npurchase product from competitors, if customers develop their own internal solution, if we are unable to provide contractual warranty or other extended service\nobligations, if the USG does not grant licenses in a timely manner or denies licenses to significant customers or if we incur significant transition costs. Even if the\nUSG grants any requested licenses, the licenses may be temporary or impose burdensome conditions that we or our customers or end users cannot or choose\nnot to fulfill. The licensing requirements may benefit certain of our competitors, as the licensing process will make our pre-sale and post-sale technical support\nefforts more cumbersome and less certain and encourage customers in China to pursue alternatives to our products, including semiconductor suppliers based in\nChina, Europe, and Israel.\n\nGiven the increasing strategic importance of AI and rising geopolitical tensions, the USG has changed and may again change the export control rules at any time\nand further subject a wider range of our products to export restrictions and licensing requirements, negatively impacting our business and financial results. In the\nevent of such change, we may be unable to sell our inventory of such products and may be unable to develop replacement products not subject to the licensing\nrequirements.\n\nFor example, the USG has already imposed conditions to limit the ability of foreign firms to create and offer as a service large-scale GPU clusters, for example\nby imposing license conditions on the use of products to be exported to certain countries, and may impose additional conditions such as requiring chip tracking\nand throttling mechanisms that could disable or impair GPUs if certain events, including unauthorized system configuration, use, or location, are detected. Such\ngovernment mandates in chip designs could introduce system vulnerabilities and expose us to significant risk and potential liability, negatively impact demand for\nour products, and could have a material impact on our business, operating results, and financial condition.\n\nOpen-source foundation models are rapidly growing in popularity with developers worldwide. Any regulatory control or other restriction that limits our ability to\nprovide products and services that support third-party applications and models, including applications built on foundation models originating in China such as\nDeepSeek or Qwen, could have a material impact on our business, operating results, and financial condition.\n\nThe USG has already imposed export controls restricting certain gaming GPUs, and if the USG expands such controls to restrict additional gaming products, it\nmay disrupt a significant portion of our supply and distribution chain and negatively impact sales of such products to markets outside China, including the U.S.\nand Europe. In addition, as the performance of the gaming GPUs increases over time, export controls may have a greater impact on our ability to compete in\nmarkets subject to those controls. Export controls may disrupt our supply and distribution chain for a substantial portion of our products, which are warehoused\nin and distributed from Hong Kong.\n\nExport controls restricting our ability to sell data center GPUs may also negatively impact demand for our networking products used in servers containing our\nGPUs. The USG may also impose export controls on our networking products, such as high-speed network interconnects, to limit the ability of downstream\nparties to create large clusters for frontier model training.\n\nAny new control that impacts a wider range of our products would likely have a disproportionate impact on NVIDIA and may disadvantage us against certain of\nour competitors that sell chips that are outside the scope of such control. Export controls have already and may in the future encourage customers outside China\nand other impacted regions to “design-out” certain U.S. semiconductors from their products to reduce the compliance burden and risk, and to ensure that they\nare able to serve markets worldwide. Export controls have already encouraged and may in the future encourage overseas governments to request that our\ncustomers purchase from our competitors rather than NVIDIA or other U.S. firms, harming our business, market position, and financial results.\n\nAs a result, export controls may negatively impact demand for our products and services not only in China, but also in other markets, such as Europe, Latin\nAmerica, and Southeast Asia. Export controls increase the risk of investing in U.S.\n\n35",
            "[0s->5s]Good afternoon. My name is Sarah and I will be your conference operator today.\n[5s->13s]At this time, I would like to welcome everyone to NVIDIA's first quarter fiscal 2026 financial results conference call.\n[13s->17s]All lines have been placed on mute to prevent any background noise.\n[17s->21s]After the speaker's remarks, there will be a question and answer session.\n[21s->27s]If you would like to ask a question during this time, simply press star one on your telephone keypad.\n[27s->32s]If you would like to withdraw your question, please press star one again. Thank you.\n[32s->36s]Toshia Hari, you may begin your conference.\n[36s->44s]Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the first quarter of fiscal 2026.\n[44s->54s]With me today from NVIDIA are Jensen Wong, President and Chief Executive Officer, and Collette Press, Executive Vice President and Chief Financial Officer.\n[54s->60s]I'd like to remind you that our call is being webcast live on NVIDIA's investor relations website.\n[60s->68s]The webcast will be available for replay until the conference call to discuss our financial results for the second quarter of fiscal 2026.\n[68s->76s]The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent.\n[76s->81s]During this call, we may make forward looking statements based on current expectations.\n[81s->88s]These are subject to a number of significant risks and uncertainties, and our actual results may differ materially.\n[88s->96s]For a discussion of factors that could affect our future financial results in business, please refer to the disclosure in today's earnings release,\n[96s->104s]our most recent forms 10-K and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission.\n[105s->112s]All our statements are made as of today, May 28, 2025, based on information currently available to us.\n[112s->118s]Except as required by law, we assume no obligation to update any such statements.\n[118s->122s]During this call, we will discuss non-GAAP financial measures.\n[122s->130s]You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website.\n[130s->133s]With that, let me turn the call over to Collette.\n[133s->135s]Thank you, Toshio.\n[135s->148s]We delivered another strong quarter with revenue of $44 billion, up 69% year-over-year, exceeding our outlook in what proved to be a challenging operating environment.\n[148s->153s]Data center revenue of $39 billion grew 73% year-on-year.\n[153s->161s]AI workloads have transitioned strongly to inference, and AI factory build-outs are driving significant revenue.\n[161s->165s]Our customers' commitments are firm.\n[165s->176s]On April 9, the U.S. government issued new export controls on H20, our data center GPU designed specifically for the China market.\n[176s->181s]We sold H20 with the approval of the previous administration.\n[181s->187s]Although our H20 has been in the market for over a year and does not have a market outside of China,\n[187s->195s]the new export controls on H20 did not provide a grace period to allow us to sell through our inventory.\n[195s->204s]In Q1, we recognized $4.6 billion in H20 revenue, which occurred prior to April 9,\n[204s->216s]but also recognized a $4.5 billion charge as we wrote down inventory and purchase obligations tied to orders we had received prior to April 9.\n[216s->226s]We were unable to ship $2.5 billion in H20 revenue in the first quarter due to the new export controls.\n[226s->234s]The $4.5 billion charge was less than what we initially anticipated as we were able to reuse certain materials.\n[234s->244s]We are still evaluating our limited options to supply data center compute products compliant with the U.S. government's revised export control rules,\n[244s->252s]losing access to the China AI accelerator market, which we believe will grow to nearly $50 billion,\n[252s->262s]would have a material adverse impact on our business going forward and benefit our foreign competitors in China and worldwide.\n[262s->271s]Our Blackwell ramp, the fastest in our company's history, drove a 73% year-on-year increase in data center revenue.\n[271s->280s]Blackwell contributed nearly 70% of data center compute revenue in the quarter, with a transition from Hopper nearly complete.\n[280s->293s]The introduction of GB200 NVL was a fundamental architectural change to enable data center scale workloads and to achieve the lowest cost per inference token.\n[293s->305s]While these systems are complex to build, we have seen a significant improvement in manufacturing yields and rack shipments are moving to strong rates to end customers.\n[305s->315s]GB200 NVL racks are now generally available for modern builders, enterprises, and sovereign customers to develop and deploy AI.\n[315s->332s]On average, major hyperscalers are each deploying nearly 1,000 NVL 72 racks or 72,000 Blackwell GPUs per week and are on track to further ramp output this quarter.\n[332s->346s]Microsoft, for example, has already deployed tens of thousands of Blackwell GPUs and is expected to ramp to hundreds of thousands of GB200s with OpenAI as one of its key customers.\n[346s->356s]Key learnings from the GB200 ramp will allow for a smooth transition to the next phase of our product roadmap, Blackwell Ultra.\n[357s->370s]Sampling of GB300 systems began earlier this month at the major CSPs, and we expect production shipments to commerce later this quarter.\n[370s->381s]GB300 will leverage the same architecture, same physical footprint, and the same electrical and mechanical specifications as GB200.\n[381s->394s]The GB300 drop-in design will allow CSPs to seamlessly transition their systems and manufacturing used for GB200 while maintaining high yields.\n[394s->407s]B300 GPUs with 50% more HBM will deliver another 50% increase in dense FP4 inference compute performance compared to the B200.\n[407s->421s]We remain committed to our annual product cadence, with our roadmap extending through 2028 tightly aligned with the multiple-year planning cycles of our customers.\n[421s->432s]We are witnessing a sharp jump in inference demand. OpenAI, Microsoft, and Google are seeing a step-function leap in token generation.\n[432s->441s]Microsoft processed over 100 trillion tokens in Q1, a five-fold increase on a year-over-year basis.\n[441s->453s]This exponential growth in Azure OpenAI is representative of strong demand for Azure AI Foundry as well as other AI services across Microsoft's platform.\n[453s->471s]Inference-serving startups are now serving models using B200, tripling their token generation rate and corresponding revenues for high-value reasoning models such as DeepSeq R1, as reported by Artificial Analysis.\n[471s->485s]NVIDIA Dynamo on Blackwell NVL72 turbocharges AI inference throughput by 30x for the new reasoning models sweeping the industry.\n[485s->497s]Developer engagements increased with adoption ranging from LLM providers such as Perplexity to financial services institutions such as Capital One,\n[497s->503s]who reduced agentic chatbox latency by 5x with Dynamo.\n[503s->511s]In the latest MLPerf inference results, we submitted our first results using GB200 NVL72,\n[511s->523s]delivering up to 30x higher inference throughput compared to our 8 GPU H200 submission on the challenging Llama 3.1 benchmark.\n[523s->535s]This feat was achieved through a combination of tripling the performance per GPU as well as 9x more GPUs, all connected on a single NVLink domain.\n[535s->547s]And while Blackwell is still early in its lifecycle, software optimizations have already improved its performance by 1.5x in the last month alone.\n[547s->555s]We expect to continue improving the performance of Blackwell through its operational life as we have done with Hopper and Amper.\n[555s->560s]For example, we increased the inference performance of Hopper by 4x over two years.\n[560s->568s]This is the benefit of NVIDIA's programmable CUDA architecture and rich ecosystem.\n[568s->578s]The pace and scale of AI factory deployments are accelerating with nearly 100 NVIDIA-powered AI factories in flight this quarter,\n[578s->588s]a two-fold increase year over year, with the average number of GPUs powering each factory also doubling in the same period.\n[588s->593s]And more AI factory projects are starting across industries and geographies.\n[593s->605s]NVIDIA's full-stack architecture is underpinning AI factory deployments as industry leaders like AT&T, BYD, Capital One, Foxconn, MediaTek, and Telenor\n[605s->614s]are strategically vital sovereign clouds like those recently announced in Saudi Arabia, Taiwan, and the UAE.\n[614s->623s]We have a line of sight to projects requiring tens of gigawatts of NVIDIA AI infrastructure in the not-too-distant future.\n[623s->639s]The transition from generative to agentic AI, AI capable of perceiving, reasoning, planning, and acting, will transform every industry, every company, and country.\n[639s->650s]We envision AI agents as a new digital workforce capable of handling tasks ranging from customer service to complex decision-making processes.\n[650s->660s]We introduced the LAMA Nemotron family of open reasoning models designed to supercharge identical AI platforms for enterprises.\n[660s->672s]Built on the LAMA architecture, these models are available as NIMS or NVIDIA inference microservices with multiple sizes to meet diverse deployment needs.\n[672s->681s]Our post-training enhancements have yielded a 20% accuracy boost and a 5x increase in inference speed.\n[681s->690s]Leading platform companies including Accenture, Cadence, Deloitte, and Microsoft are transforming work with our reasoning models.\n[690s->702s]NVIDIA NEMO microservices are generally available across industries that are being leveraged by leading enterprises to build, optimize, and scale AI applications.\n[702s->711s]With NEMO, Cisco increased model accuracy by 40% and improved response time by 10x in its code assistant.\n[711s->719s]NASDAQ realized a 30% improvement in accuracy and response time in its AI platform's search capabilities.\n[719s->727s]And Shell's Custom LLM achieved a 30% increase in accuracy when trained with NVIDIA NEMO.\n[727s->735s]NEMO's parallelism techniques accelerated model training time by 20% when compared to other frameworks.\n[735s->757s]We also announced a partnership with Yum Brands, the world's largest restaurant company, to bring NVIDIA AI to 500 of its restaurants this year and expanding to 61,000 restaurants over time to streamline order taking, optimize operations, and enhance service across its restaurants.\n[757s->779s]For AI-powered cybersecurity, leading companies like Checkpoint, CloudStrike, and Palo Alto Networks are using NVIDIA's AI security and software stack to build, optimize, and secure agentic workflows, with CloudStrike realizing 2x faster detection triage with 50% less compute cost.\n[780s->790s]Moving to networking, sequential growth in networking resumed in Q1, with revenue up 64% quarter-over-quarter to $5 billion.\n[790s->798s]Our customers continue to leverage our platform to efficiently scale up and scale out AI factory workloads.\n[798s->813s]We created the world's fastest switch, NVLink. For scale up, our NVLink compute fabric in its fifth generation offers 14x the bandwidth of PCIe Gen 5.\n[813s->825s]NVLink 72 carries 130 TB per second of bandwidth in a single rack, equivalent to the entirety of the world's peak internet traffic.\n[825s->833s]NVLink is a new growth vector and is off to a great start, with Q1 shipments exceeding $1 billion.\n[833s->848s]At Computex, we announced NVLink Fusion. Hyperscale customers can now build semi-custom CCUs and accelerators that connect directly to the NVIDIA platform with NVLink.\n[848s->868s]We are now enabling key partners, including ASIC providers such as MediaTek, Marvell, Alchip Technologies, and Astera Labs, as well as CPU suppliers such as Fujitsu and Qualcomm, to leverage NVLink Fusion to connect our respective ecosystems.\n[868s->878s]For scale out, our enhanced Ethernet offerings deliver the highest throughput, lowest latency networking for AI.\n[878s->886s]SpectrumX posted strong sequential and year-on-year growth and is now annualizing over $8 billion in revenue.\n[886s->896s]Adoption is widespread across major CSPs and consumer internet companies, including CoreWe, Microsoft Azure, Oracle Cloud, and XAI.\n[896s->903s]This quarter, we added Google Cloud and Meta to the growing list of SpectrumX customers.\n[903s->913s]We introduced SpectrumX and QuantumX, silicon photonics switches featuring the world's most advanced co-packaged optics.\n[913s->933s]These platforms will enable next level AI factory scaling to millions of GPUs through the increasingly power efficiency by 3.5x and network resiliency by 10x while accelerating customer time to market by 1.3x.\n[933s->938s]Transitioning to a quick summary of our revenue by geography.\n[938s->949s]China, as a percentage of our data center revenue, was slightly below our expectations and down sequentially due to H20 export licensing controls.\n[949s->955s]For Q2, we expect a meaningful decrease in China data center revenue.\n[955s->971s]As a reminder, while Singapore represented nearly 20% of our Q1 billed revenue, as many of our large customers use Singapore for centralized invoicing, our products are almost always sold elsewhere.\n[971s->984s]Note that over 99% of H100, H200, and Blackwell data center compute revenue billed to Singapore was for orders from US based customers.\n[984s->996s]Moving to gaming and AI PCs. Gaming revenue was a record 3.8 billion, increasing 48% sequentially and 42% year on year.\n[996s->1012s]Strong adoption by gamers, creatives, and AI enthusiasts have made Blackwell our fastest ramp ever. Against a backdrop of robust demand, we greatly improved our supply and availability in Q1 and expect to continue these efforts in Q2.\n[1012s->1017s]AI is transforming PC and creator and gamers.\n[1017s->1035s]With a 100 million user installed base, GeForce represents the largest footprint for PC developers. This quarter, we added to our AI PC laptop offerings, including models capable of running Microsoft's Copilot Plus.\n[1035s->1049s]This past quarter, we brought Blackwell architecture to mainstream gaming with its launch of GeForce RTX 5060 and 5060 Ti, starting at just $299.\n[1049s->1061s]The RTX 5060 also debuted in laptops starting at $1,099. These systems that double the frame rate and slash latency.\n[1061s->1070s]These GeForce RTX 5060 and 5060 Ti desktop GPUs and laptops are now available.\n[1070s->1090s]In console gaming, the recently unveiled Nintendo Switch 2 leverages Nvidia's neural rendering and AI technologies, including next generation custom RTX GPUs with DLSS technology, to deliver a giant leap in gaming performance to millions of players worldwide.\n[1090s->1100s]Nintendo has shipped over 150 million Switch consoles to date, making it one of the most successful gaming systems in history.\n[1100s->1108s]Moving to pro visualization. Revenue of $509 million was flat sequentially and up 19% year on year.\n[1108s->1120s]Tariff related uncertainty temporarily impacted Q1 systems and demand for AI workstations is strong and we expect sequential revenue growth to resume in Q2.\n[1120s->1130s]Nvidia DGX Spark and Station revolutionize personal computing by putting the power of an AI supercomputer in a desktop form factor.\n[1130s->1143s]DGX Spark delivers up to one petaflop of AI compute while DGX Station offers an incredible 20 petaflops and is powered by the GB300 Superchip.\n[1143s->1150s]DGX Spark will be available in Calendar Q3 and DGX Station later this year.\n[1150s->1163s]We have deepened Omniverse's integration and adoption into some of the world's leading software platforms, including Databricks, SAP, and Schneider Electric.\n[1163s->1178s]New Omniverse blueprints such as Mega for at scale robotic fleet management are being leveraged in Keyon Group, Pegatron, Accenture, and other leading companies to enhance industrial operations.\n[1178s->1189s]At Confitex, we showcased Omniverse's great traction with technology manufacturing leaders, including TSMC, Quanta, Foxconn, Pegatron.\n[1189s->1194s]Using Omniverse, TSMC saves months in work by designing fabs virtually.\n[1194s->1205s]Foxconn accelerates thermal simulations by 150x and Pegatron reduced assembly line defects rates by 67%.\n[1205s->1214s]Lastly, with our automotive group, revenue was $567 million, down 1% sequentially but up 72% year on year.\n[1214s->1222s]Year on year growth was driven by the ramp of self-driving across a number of customers and robust end demand for NEVs.\n[1222s->1232s]We are partnering with GM to build the next-gen vehicles, factories, and robots using NVIDIA AI, simulation, and accelerated computing.\n[1232s->1241s]And we are now in production with our full-stack solution for Mercedes-Benz, starting with the new CLA, hitting roads in the next few months.\n[1241s->1254s]We announced Isaac, Groot, and won the world's first open fully customizable foundation model for humanoid robots, enabling generalized reasoning and skill development.\n[1254s->1260s]We also launched new open NVIDIA Cosmo World Foundation models.\n[1260s->1268s]Leading companies include 1X, Agility Robots, Robotics, Figure AI, Uber, and Wabi.\n[1268s->1284s]We've begun integrating Cosmos into their operations for synthetic data generation, while Agility Robotics, Boston Dynamics, and XPen Robotics are harnessing Isaac simulation to advance their humanoid efforts.\n[1284s->1300s]GE Healthcare is using the new NVIDIA Isaac platform for healthcare simulation, built on NVIDIA Omniverse and using NVIDIA Cosmos, the platform speeds development of robotic imaging and surgery systems.\n[1300s->1314s]The era of robotics is here. Billions of robots, hundreds of millions of autonomous vehicles, and hundreds of thousands of robotic factories and warehouses will be developed.\n[1314s->1335s]All right, moving to the rest of the P&L. Gap gross margins and non-gap gross margins were 60.5% and 61%, respectively. Excluding the $4.5 billion charge, Q1 non-gap gross margins would have been 71.3%, slightly above our outlook at the beginning of the quarter.\n[1335s->1346s]Sequentially, gap operating expenses were up 7% and non-gap operating expenses were up 6%, reflecting higher compensation and employee growth.\n[1346s->1355s]Our investments include expanding our infrastructure capabilities and AI solutions, and we plan to grow these investments throughout the fiscal year.\n[1355s->1370s]In Q1, we returned a record $14.3 billion to shareholders in the form of share repurchases and cash dividends. Our capital return program continues to be a key element of our capital allocation strategy.\n[1370s->1384s]Let me turn to the outlook for the second quarter. Total revenue is expected to be $45 billion, plus or minus 2%. We expect modest sequential growth across all of our platforms.\n[1384s->1403s]In data center, we anticipate the continued ramp of Blackwell to be partially offset by a decline in China revenue. Note, our outlook reflects a loss in H20 revenue of approximately $8 billion for the second quarter.\n[1403s->1420s]Gap and non-gap gross margins are expected to be 71.8% and 72% respectively, plus or minus 50 basis points. We expect better Blackwell profitability to drive modest sequential improvement in gross margins.\n[1420s->1427s]We are continuing to work towards achieving gross margins in the mid-70s range late this year.\n[1427s->1444s]Gap and non-gap operating expenses are expected to be approximately $5.7 billion and $4 billion respectively, and we continue to expect full-year fiscal year 26 operating expense growth to be in the mid-30% range.\n[1444s->1457s]Gap and non-gap other income and expenses are expected to be an income of approximately $450 million, excluding gays and losses from non-marketable and publicly held equity securities.\n[1457s->1468s]Gap and non-gap tax rates are expected to be 16.5%, plus or minus 1%, excluding any discrete items.\n[1468s->1480s]Further financial details are included in the CFO commentary and other information available on our IR website, including a new financially information AI agent.\n[1480s->1488s]Let me highlight upcoming events for the financial community. We will be at the B of A Global Technology Conference in San Francisco on June 4.\n[1488s->1500s]The Rosenblatt Virtual AI Summit and NASDAQ Investor Conference in London on June 10, and GTC Paris at Viva Tech on June 11 in Paris.\n[1500s->1511s]We look forward to seeing you at these events. Our earnings call to discuss the results of our second quarter of fiscal 2026 is scheduled for August 27.\n[1511s->1517s]Well now, let me turn it over to Jensen to make some remarks.\n[1517s->1526s]Thanks, Collette. We've had a busy and productive year. Let me share my perspective on some topics we're frequently asked.\n[1526s->1536s]On export control, China is one of the world's largest AI markets and a springboard to global success.\n[1536s->1543s]With half of the world's AI researchers based there, the platform that wins China is positioned to lead globally.\n[1543s->1551s]Today, however, the $50 billion China market is effectively closed to U.S. industry.\n[1551s->1560s]The H20 export ban ended our Hopper data center business in China. We cannot reduce Hopper further to comply.\n[1561s->1568s]As a result, we are taking a multi-billion dollar write-off on inventory that cannot be sold or repurposed.\n[1568s->1575s]We are exploring limited ways to compete, but Hopper is no longer an option.\n[1575s->1584s]China's AI moves on with or without U.S. chips. It has to compute to train and deploy advanced models.\n[1585s->1590s]The question is not whether China will have AI. It already does.\n[1590s->1596s]The question is whether one of the world's largest AI markets will run on American platforms.\n[1596s->1604s]Shielding Chinese chipmakers from U.S. competition only strengthens them abroad and weakens America's position.\n[1604s->1609s]Export restrictions have spurred China's innovation in scale.\n[1609s->1616s]The AI race is not just about chips. It's about which stack the world runs on.\n[1616s->1624s]As that stack grows to include 6G and quantum, U.S. global infrastructure leadership is at stake.\n[1624s->1630s]The U.S. has based its policy on the assumption that China cannot make AI chips.\n[1630s->1635s]That assumption was always questionable, and now it's clearly wrong.\n[1635s->1639s]China has enormous manufacturing capability.\n[1639s->1645s]In the end, the platform that wins the AI developers wins AI.\n[1645s->1653s]Export controls should strengthen U.S. platforms, not drive half of the world's AI talent to rivals.\n[1654s->1657s]On DeepSeek.\n[1657s->1665s]DeepSeek and QN from China are among the best open source AI models.\n[1665s->1671s]Released freely, they've gained traction across the U.S., Europe, and beyond.\n[1671s->1680s]DeepSeek R1, like ChatGPT, introduced reasoning AI that produces better answers the longer it thinks.\n[1680s->1690s]Reasoning AI enables step-by-step problem solving, planning, and tool use, turning models into intelligent agents.\n[1690s->1705s]Reasoning is compute intensive, requires hundreds to thousands of times more tokens per task than previous one-shot inference.\n[1705s->1711s]Reasoning models are driving a step function surge in inference demand.\n[1711s->1721s]AI scaling laws remain firmly intact, not only for training, but now inference, too, requires massive scale compute.\n[1721s->1726s]DeepSeek also underscores the strategic value of open source AI.\n[1726s->1734s]When popular models are trained and optimized on U.S. platforms, it drives usage, feedback, and continuous improvement.\n[1734s->1737s]Reinforcing American leadership across the stack.\n[1737s->1743s]U.S. platforms must remain the preferred platform for open source AI.\n[1743s->1749s]That means supporting collaboration with top developers globally, including in China.\n[1749s->1757s]America wins when models like DeepSeek and QN runs best on American infrastructure.\n[1757s->1760s]Regarding onshore manufacturing.\n[1760s->1770s]President Trump has outlined a bold vision to reshore advanced manufacturing, create jobs, and strengthen national security.\n[1770s->1774s]Future plants will be highly computerized in robotics.\n[1774s->1776s]We share this vision.\n[1776s->1783s]TSMC is building six fabs and two advanced packaging plants in Arizona to make chips for Nvidia.\n[1783s->1789s]Process qualification is underway, with volume production expected by year end.\n[1790s->1798s]Spill and Amcor are also investing in Arizona, constructing packaging, assembly, and test facilities.\n[1798s->1806s]In Houston, we're partnering with Foxconn to construct a million-square-foot factory to build AI supercomputers.\n[1806s->1811s]Wistron is building a similar plant in Fort Worth, Texas.\n[1811s->1817s]To encourage and support these investments, we've made substantial long-term purchase commitments.\n[1817s->1821s]A deep investment in America's AI manufacturing future.\n[1823s->1827s]Our goal, from chip to supercomputer.\n[1827s->1830s]Built in America within a year.\n[1830s->1840s]Each GB200 and Beelink 72 racks contains 1.2 million components and weighs nearly two tons.\n[1840s->1844s]No one has produced supercomputers on this scale.\n[1844s->1847s]Our partners are doing an extraordinary job.\n[1849s->1857s]On AI diffusion rule, President Trump rescinded the AI diffusion rule, calling it counterproductive,\n[1857s->1864s]and proposed a new policy to promote US AI tech with trusted partners.\n[1864s->1868s]On his Middle East tour, he announced historic investments.\n[1868s->1875s]I was honored to join him in announcing a 500-megawatt AI infrastructure project in Saudi Arabia\n[1875s->1880s]and a 5-gigawatt AI campus in the UAE.\n[1880s->1883s]President Trump wants US tech to lead.\n[1883s->1887s]The deals he announced are wins for America.\n[1887s->1895s]Creating jobs, advancing infrastructure, generating tax revenue, and reducing the US trade deficit.\n[1895s->1902s]The US will always be Nvidia's largest market and home to the largest install base of our infrastructure.\n[1902s->1909s]Every nation now sees AI as core to the next industrial revolution.\n[1909s->1915s]A new industry that produces intelligence and essential infrastructure for every economy.\n[1915s->1922s]Countries are racing to build national AI platforms to elevate their digital capabilities.\n[1922s->1930s]At Computex, we announced Taiwan's first AI factory in partnership with Foxconn and the Taiwan government.\n[1930s->1936s]Last week, I was in Sweden to launch its first national AI infrastructure.\n[1936s->1947s]Japan, Korea, India, Canada, France, the UK, Germany, Italy, Spain, and more are now building national AI factories\n[1947s->1951s]to empower startups, industries, and societies.\n[1951s->1956s]Sovereign AI is a new growth engine for Nvidia.\n[1958s->1960s]Toshia, back to you. Thank you.\n[1960s->1966s]Operator, we will now open the call for questions. Would you please pull for questions?\n[1966s->1974s]Thank you. At this time, I would like to remind everyone in order to ask a question, press star, then the number one on your telephone keypad.\n[1974s->1978s]We'll pause for just a moment to compile the Q&A roster.\n[1982s->1988s]Your first question comes from the line of Joe Moore with Morgan Stanley. Your line is open.\n[1990s->1998s]Great. Thank you. You guys have talked about this scaling up of inference around reasoning models for at least a year now,\n[1998s->2002s]and we've really seen that come to fruition as you talked about. We've heard it from your customers.\n[2002s->2007s]Can you give us a sense for how much of that demand you're able to serve?\n[2007s->2010s]And give us a sense for maybe how big the inference business is for you guys.\n[2010s->2016s]And do we need full-on NVL-72 rack-scale solutions for reasoning inference going forward?\n[2019s->2026s]Well, we would like to serve all of it. And I think we're on track to serve most of it.\n[2026s->2038s]Grace Blackwell NVLink-72 is the ideal engine today, the ideal computer thinking machine, if you will, for reasoning AI.\n[2038s->2046s]There's a couple of reasons for that. The first reason is that the token generation amount,\n[2046s->2059s]the number of tokens reasoning goes through is a hundred, a thousand times more than a one-shot chatbot.\n[2059s->2069s]You know, it's essentially thinking to itself, breaking down a problem step by step. It might be planning multiple paths to an answer.\n[2069s->2083s]It could be using tools, reading PDFs, reading web pages, watching videos, and then producing a result, an answer.\n[2083s->2089s]The longer it thinks, the better the answer, the smarter the answer is.\n[2090s->2101s]And so what we would like to do and the reason why Grace Blackwell was designed to give such a giant step up in inference performance\n[2101s->2106s]is so that you could do all this and still get a response as quickly as possible.\n[2106s->2117s]Compared to Hopper, Grace Blackwell is some 40 times higher speed and throughput compared.\n[2117s->2132s]And so this is going to be a huge, huge benefit and driving down the cost while improving the quality of response with excellent quality of service at the same time.\n[2132s->2139s]So that's the fundamental reason. That was the core driving reason for Grace Blackwell NVLink-72.\n[2139s->2151s]Of course, in order to do that, we had to reinvent, literally redesign the entire way that these supercomputers are built.\n[2151s->2161s]But now we're in full production. It's going to be exciting. It's going to be incredibly exciting.\n[2162s->2171s]The next question comes from Vivek Arya with Bank of America Securities. Your line is open.\n[2171s->2176s]Thanks for the question. Just a clarification for Colette first.\n[2176s->2184s]So on the China impact, I think previously it was mentioned that at about $15 billion, so you had the $8 billion in Q2.\n[2184s->2190s]So is there still some left as a headwind for the remaining quarters, just Colette, how to model that?\n[2190s->2201s]And then question, Jensen, for you. Back at GTC, you had outlined a path towards almost a trillion dollars of AI spending over the next few years.\n[2201s->2212s]Where are we in that build out? And do you think it's going to be uniform that you will see every spender, whether it's ESP, sovereigns, enterprises or build out?\n[2212s->2215s]Should we expect some periods of digestion in between?\n[2215s->2221s]What are your customer discussions telling you about how to model growth for next year?\n[2223s->2232s]Yes, Vivek. Thanks so much for the question regarding H20. Yes, we recognized 4.6 H20 in Q1.\n[2232s->2241s]We were unable to ship $2.5 billion. So the total for Q1 should have been $7 billion.\n[2241s->2248s]When we look at our Q2, our Q2 is going to be meaningfully down in terms of China data center revenue.\n[2248s->2258s]And we had highlighted in terms of the amount of orders that we had planned for H20 and Q2, and that was $8 billion.\n[2258s->2266s]Now, going forward, we did have other orders going forward that we will not be able to fulfill.\n[2266s->2275s]That is what was incorporated, therefore, in the amount that we wrote down of the $4.5 billion.\n[2275s->2280s]That write down was about inventory and purchase commitments.\n[2280s->2287s]And our purchase commitments were about what we expected regarding the orders that we had received.\n[2287s->2295s]Going forward, though, it's a bigger issue regarding the amount of the market that we will not be able to serve.\n[2295s->2306s]We assess that TAM to be close to about $50 billion in the future as we don't have a product to enable for the China.\n[2310s->2318s]Vivek, probably the best way to think through it is that AI is several things.\n[2318s->2326s]Of course, we know that AI is this incredible technology that's going to transform every industry,\n[2326s->2344s]from, of course, the way we do software to health care and financial services to retail to, I guess, every industry, transportation, manufacturing.\n[2344s->2347s]And we're at the beginning of that.\n[2347s->2352s]But maybe another way to think about that is where do we need intelligence?\n[2352s->2354s]Where do we need digital intelligence?\n[2354s->2358s]And it's in every country, it's in every industry.\n[2358s->2365s]And we know because of that, we recognize that AI is also an infrastructure.\n[2365s->2371s]It's a way of delivering a technology that requires factories.\n[2371s->2374s]And these factories produce tokens.\n[2374s->2380s]And they, as I mentioned, are important to every single industry in every single country.\n[2380s->2389s]And so on that basis, we're really at the very beginning of it because the adoption of this technology is really kind of in its early stages.\n[2389s->2399s]Now, we've reached an extraordinary milestone with AIs that are reasoning, are thinking, what people call inference time scaling.\n[2399s->2413s]And of course, it created a whole new, we've entered an era where inference is going to be a significant part of the compute workload.\n[2413s->2417s]But anyhow, it's going to be a new infrastructure.\n[2417s->2421s]And we're building it out in the clouds.\n[2421s->2429s]The United States is really the early starter and available in US clouds.\n[2429s->2432s]And this is our largest market, our largest installed base.\n[2432s->2435s]And we can continue to see that happening.\n[2435s->2442s]But beyond that, we're going to have to, we're going to see AI go into enterprise, which is on prem.\n[2442s->2447s]Because so much of the data is still on prem, access control is really important.\n[2447s->2452s]It's really hard to move all of every company's data into the cloud.\n[2452s->2455s]And so we're going to move AI into the enterprise.\n[2455s->2460s]And you saw that we announced a couple of really exciting new products.\n[2460s->2468s]Our RTX Pro Enterprise AI server that runs everything enterprise and AI.\n[2468s->2476s]Our DGX Spark and DGX Station, which is designed for developers who want to work on prem.\n[2476s->2481s]And so enterprise AI is just taking off.\n[2481s->2490s]Telcos, today, a lot of the telco infrastructure will be in the future software defined and built on AI.\n[2490s->2492s]And so 6G is going to be built on AI.\n[2492s->2495s]And that infrastructure needs to be built out.\n[2495s->2498s]And they said it's very, very early stages.\n[2498s->2505s]And then, of course, every factory today that makes things will have an AI factory that sits with it.\n[2505s->2513s]And the AI factory is going to be drive creating AI and operating AI for the factory itself.\n[2513s->2518s]But also to power the products and the things that are made by the factory.\n[2518s->2523s]So it's very clear that every car company will have AI factories.\n[2523s->2527s]And very soon there'll be robotics companies, robot companies.\n[2527s->2534s]And those companies will be also building AIs to drive the robots.\n[2534s->2542s]And so we're at the beginning of all of this build out.\n[2542s->2547s]The next question comes from CJ News with Cantor Fitzgerald.\n[2547s->2549s]Your line is open.\n[2549s->2552s]Yeah, good afternoon. Thank you for taking the question.\n[2552s->2556s]There have been many large GPU cluster investment announcements in the last month.\n[2556s->2560s]And you alluded to a few of them with Saudi Arabia, the UAE.\n[2560s->2563s]And then also, you know, we heard from Oracle and XAI, just to name a few.\n[2563s->2570s]So my question, are there other that have yet to be announced of the same kind of scale and magnitude?\n[2570s->2583s]And perhaps more importantly, how are these orders impacting your lead times for Blackwell and your current visibility sitting here today, you know, almost halfway through 2025?\n[2583s->2593s]Well, we have more orders today than we did at the last time I spoke about orders at GTC.\n[2593s->2599s]However, we're also increasing our supply chain and building out our supply chain.\n[2599s->2602s]They're doing a fantastic job.\n[2602s->2613s]We're building it here on shore in the United States, but we're going to keep our supply chain quite busy for several many more years coming.\n[2613s->2625s]And with respect to further announcements, I'm going to be on the road next week through Europe.\n[2625s->2640s]And it's just about every country needs to build out AI infrastructure and their umpteen AI factories being planned.\n[2640s->2650s]I think in the remarks, Colette mentioned there's some 100 AI factories being built.\n[2650s->2653s]There's a whole bunch that haven't been announced.\n[2653s->2673s]And I think the important concept here, which makes it easier to understand, is that like other technologies that impact literally every single industry, of course, electricity was one, and it became infrastructure.\n[2673s->2683s]Of course, the information infrastructure, which we now know as the Internet, affects every single industry, every country, every society.\n[2683s->2686s]Intelligence is surely one of those things.\n[2686s->2693s]I don't know any company, industry, country who thinks that intelligence is optional.\n[2693s->2694s]It's essential infrastructure.\n[2694s->2701s]And so we've now digitalized intelligence.\n[2701s->2708s]And so I think we're clearly in the beginning of the build out of this infrastructure.\n[2708s->2712s]And every country will have it.\n[2712s->2713s]I'm certain of that.\n[2713s->2714s]Every industry will use it.\n[2714s->2716s]That I'm certain of.\n[2716s->2722s]And what's unique about this infrastructure is that it needs factories.\n[2722s->2728s]You know, it's a little bit like the energy infrastructure, electricity.\n[2728s->2730s]It needs factories.\n[2730s->2733s]We need factories to produce this intelligence.\n[2733s->2736s]And the intelligence is getting more sophisticated.\n[2736s->2743s]We were talking about earlier that we had a huge breakthrough in the last couple of years with reasoning AI.\n[2743s->2748s]And now there are agents that reason and there are super agents that use a whole bunch of tools.\n[2748s->2754s]And then there's clusters of super agents where agents are working with agents, solving problems.\n[2754s->2769s]And so you could just imagine compared to one shot chat bots and the agents that are now using AI built on these large language models, how much more compute intensive they really need to be and are.\n[2769s->2774s]And so I think we're in the beginning of the build out.\n[2774s->2782s]And there should be many, many more announcements in the future.\n[2782s->2783s]Your next question.\n[2783s->2784s]Excuse me.\n[2784s->2788s]Your next question comes from Ben Reitz's with Melius.\n[2788s->2790s]Your line is open.\n[2790s->2791s]Yeah, hi.\n[2791s->2793s]Thanks for the question.\n[2793s->2801s]I wanted to ask first to Collette just a little clarification around the guidance and maybe putting it in a different way.\n[2801s->2813s]The 8 billion for H20 just seems like it's roughly 3 billion more than most people thought with regard to what you'd be foregoing in the second quarter.\n[2813s->2823s]So that would mean that with regard to your guidance, the rest of the business in order to hit 45 is doing 2 to 3 billion or so better.\n[2824s->2828s]So I was wondering if that math made sense to you.\n[2828s->2837s]And then in terms of the guidance, that would imply the non-China business is doing a bit better than the street expected.\n[2837s->2843s]So wondering what the primary driver was there in your view.\n[2843s->2851s]And then this second part of my question, Jensen, I know you guide one quarter at a time.\n[2851s->2871s]But with regard to the AI diffusion rule being lifted and this momentum was sovereign, there's been times in your history where you guys have said on calls like this where you have more conviction and sequential growth throughout the year, et cetera.\n[2871s->2886s]And given the unleashing of demand with AI diffusion being revoked and the supply chain increasing, does the environment give you more conviction and sequential growth as we go throughout the year?\n[2886s->2891s]So first one for Collette and then next one for Jensen. Thanks so much.\n[2891s->2908s]Thanks, Ben, for the question. When we look at our Q2 guidance and our commentary that we provided that had the export controls not occurred, we would have had orders of about 8 billion for H20.\n[2908s->2918s]That's correct. That was a possibility for what we would have had in our outlook for this quarter in Q2.\n[2918s->2932s]So what we also have talked about here is the growth that we've seen in Blackwell, Blackwell across many of our customers, as well as the growth that we continue to have in terms of supply that we need for our customers.\n[2932s->2938s]So putting those together, that's where we came through with the guidance that we provided.\n[2938s->2942s]I'm going to turn the rest over to Jensen to see how he wants to.\n[2942s->2945s]Yeah, thanks. Thanks, Ben.\n[2945s->2957s]I would say compared to the beginning of the year, compared to GTC timeframe, there are four positive surprises.\n[2957s->2967s]The first positive surprise is the step function demand increase of reasoning AI.\n[2967s->2977s]I think it is fairly clear now that AI is going through an exponential growth, and reasoning AI really busts it through.\n[2977s->2986s]Concerns about hallucination or its ability to really solve problems.\n[2986s->2999s]I think a lot of people are crossing that barrier and realizing how incredibly effective agentic AI is and reasoning AI is.\n[2999s->3007s]So number one is inference reasoning and the exponential growth there, demand growth.\n[3007s->3012s]The second one, you mentioned AI diffusion.\n[3012s->3019s]It's really terrific to see that the AI diffusion rule was rescinded.\n[3019s->3032s]President Trump wants America to win, and he also realizes that we're not the only country in the race.\n[3032s->3042s]He wants the United States to win and recognizes that we have to get the American stack out to the world\n[3042s->3047s]and have the world build on top of American stacks instead of alternatives.\n[3047s->3052s]And so AI diffusion happened.\n[3052s->3062s]The rescinding of it happened at almost precisely the time that the countries around the world are awakening the importance of AI as an infrastructure,\n[3062s->3072s]not just as a technology of great curiosity and great importance, but infrastructure for their industries and startups and society.\n[3072s->3078s]Just as they had to build out infrastructure for electricity and Internet, you got to build out an infrastructure for AI.\n[3078s->3083s]I think that that's an awakening and that creates a lot of opportunity.\n[3083s->3085s]The third is enterprise AI.\n[3085s->3093s]Agents work and agents are doing these agents are really quite successful.\n[3093s->3098s]Much more than generative AI, agentic AI is game changing.\n[3098s->3113s]Agents can understand ambiguous and rather implicit instructions and able to problem solve and use tools and have memory and so on.\n[3113s->3118s]And so I think enterprise AI is ready to take off.\n[3118s->3133s]And it's taken us a few years to build a computing system that is able to integrate, run enterprise AI stacks, run enterprise IT stacks, but add AI to it.\n[3133s->3142s]And this is the RTX Pro Enterprise server that we announced at Computex just last week.\n[3142s->3149s]And just about every major IT company has joined us super excited about that.\n[3149s->3152s]And so computing is one stat, one part of it.\n[3152s->3157s]But remember, enterprise IT is really three pillars.\n[3157s->3161s]It's compute, storage and networking.\n[3161s->3164s]And we've now put all three of them together for finally.\n[3164s->3167s]And we're going to market with that.\n[3167s->3170s]And then lastly, industrial AI.\n[3170s->3186s]Remember, one of the implications of the world reordering, if you will, is regions onshore manufacturing and building plants everywhere.\n[3186s->3195s]In addition to AI factories, of course, there are new electronics manufacturing chip manufacturing being built around the world.\n[3195s->3210s]And all of these new plants and these new factories are creating exactly the right time when Omniverse and AI and all the work that we're doing with robotics is emerging.\n[3210s->3215s]And so this fourth pillar is quite important.\n[3215s->3218s]Every factory will have an AI factory associated with it.\n[3218s->3226s]And in order to create these physical AI systems, you really have to train a vast amount of data.\n[3226s->3232s]So back to more data, more training, more AIs to be created, more computers.\n[3232s->3243s]And so these four drivers are really kicking into turbocharge.\n[3243s->3248s]Your next question comes from Timothy R. Curie with UBS.\n[3248s->3250s]Your line is open.\n[3250s->3251s]Thanks a lot.\n[3251s->3253s]Jensen, I wanted to ask about China.\n[3253s->3257s]It sounds like the July guidance assumes there's no SKU replacement for the H-20.\n[3257s->3262s]But if the president wants the U.S. to win, it seems like you're going to have to be allowed to ship something into China.\n[3262s->3265s]So I guess I had two points on that.\n[3265s->3269s]First of all, have you been approved to ship a new modified version into China?\n[3269s->3272s]And you're currently building it, but you just can't ship it in fiscal Q2.\n[3272s->3277s]And then you were sort of run rating $7 to $8 billion a quarter into China.\n[3277s->3283s]Can we get back to those sorts of quarterly run rates once you get something that you're allowed to ship back into China?\n[3283s->3287s]I think we're all trying to figure out how much to add back to our models and when.\n[3287s->3289s]So whatever you can say there would be great.\n[3289s->3291s]Thanks.\n[3291s->3293s]The president has a plan.\n[3293s->3296s]He has a vision and I trust him.\n[3296s->3305s]With respect to our export controls, it's a set of limits.\n[3305s->3322s]And the new set of limits pretty much make it impossible for us to reduce Hopper any further for any productive use.\n[3322s->3330s]And so the new limits, it's kind of the end of the road for Hopper.\n[3330s->3333s]We have limited options.\n[3333s->3337s]And so the key is to understand the limits.\n[3337s->3346s]The key is to understand the limits and see if we can come up with interesting products that could continue to serve the Chinese market.\n[3346s->3350s]We don't have anything at the moment.\n[3350s->3352s]But we're considering it.\n[3352s->3354s]We're thinking about it.\n[3354s->3358s]Obviously the limits are quite stringent at the moment.\n[3358s->3363s]And we have nothing to announce today.\n[3363s->3373s]And when the time comes, you know, we'll engage the administration and discuss that.\n[3373s->3378s]Your final question comes from the line of Aaron Rakers with Wells Fargo.\n[3378s->3381s]Your line is open.\n[3381s->3382s]Hi.\n[3382s->3383s]This is Jake on for Aaron.\n[3383s->3386s]Thanks for taking the question and congrats on the great quarter.\n[3386s->3399s]I was wondering if you could give some additional color around the strength you saw within the networking business, particularly around the adoption of your Ethernet solutions at CSPs as well as your\n[3400s->3404s]any change you're seeing in network attach rates.\n[3404s->3407s]Yeah, thank you for that.\n[3407s->3415s]We now have three networking platforms, maybe four.\n[3415s->3425s]The first one is the scale up platform to turn a computer into a much larger computer.\n[3425s->3427s]Scaling up is incredibly hard to do.\n[3427s->3429s]Scaling out is easier to do.\n[3429s->3431s]But scaling up is hard to do.\n[3431s->3433s]And that platform is called NVLink.\n[3433s->3446s]And NVLink comes with it, chips and switches and, you know, NVLink spines.\n[3446s->3448s]It's really complicated.\n[3448s->3452s]But anyways, that's our new platform, scale up platform.\n[3452s->3455s]In addition to InfiniBand, we also have SpectrumX.\n[3455s->3466s]We've been fairly, fairly consistent that Ethernet was designed for a lot of traffic that are independent.\n[3466s->3471s]But in the case of AI, you have a lot of computers working together.\n[3471s->3477s]And the traffic of AI is insanely bursty.\n[3477s->3479s]Latency matters a lot.\n[3479s->3484s]Because the AI is thinking and it wants to get work done as quickly as possible.\n[3484s->3487s]And you got a whole bunch of nodes working together.\n[3487s->3499s]And so we enhanced Ethernet, added capabilities like extremely low latency, congestion control, adaptive routing,\n[3499s->3506s]the type of technologies that were available only in InfiniBand to Ethernet.\n[3506s->3511s]And as a result, we improved the utilization of Ethernet in these clusters.\n[3511s->3519s]These clusters are gigantic from as low as 50% to as high as 85%, 90%.\n[3519s->3531s]And so the difference is if you had a cluster that's $10 billion and you improved its effectiveness by 40%, that's worth $4 billion.\n[3532s->3534s]It's incredible.\n[3534s->3538s]And so SpectrumX has been really, quite frankly, a home run.\n[3538s->3553s]And this last quarter, as we said in the prepared remarks, we added two very significant CSPs to the SpectrumX adoption.\n[3553s->3558s]And then the last one, the last one is Bluefield, which is our control plane.\n[3558s->3566s]And so in those four, the control plane and network, which is used for storage, is used for security,\n[3566s->3576s]and for many of these clusters that want to achieve isolation among its users, multi-tenant clusters,\n[3576s->3583s]and still be able to use and have extremely high performance, bare metal performance,\n[3584s->3591s]Bluefield is ideal for that and is used in a lot of these cases.\n[3591s->3599s]And so we have these four networking platforms that are all growing and we're doing really well.\n[3599s->3601s]I'm very proud of the team.\n[3603s->3607s]That is all the time we have for questions.\n[3607s->3610s]Denton, I will turn the call back to you.\n[3610s->3611s]Thank you.\n[3611s->3612s]Thank you.\n[3612s->3615s]This is the start of a powerful new wave of growth.\n[3615s->3617s]Grace Blackwell is in full production.\n[3617s->3619s]We're off to the races.\n[3619s->3621s]We now have multiple significant growth engines.\n[3621s->3627s]Inference, one's delighted workload is surging with revenue generating AI services.\n[3627s->3635s]AI is growing faster and will be larger than any platform shifts before, including the internet, mobile, and cloud.\n[3635s->3645s]Blackwell is built to power the full AI lifecycle from training frontier models to running complex inference and reasoning agents at scale.\n[3645s->3655s]Training demands continues to rise with breakthroughs in post-training and like reinforcement learning and synthetic data generation.\n[3655s->3657s]But inference is exploding.\n[3657s->3661s]Reasoning AI agents require orders of magnitude more compute.\n[3661s->3667s]The foundations of our next growth platforms are in place and ready to scale.\n[3667s->3674s]Sovereign AI nations are investing in AI infrastructure like they once did for electricity and internet.\n[3674s->3681s]Enterprise AI, AI must be deployable on prem and integrated with existing IT.\n[3681s->3692s]Our RTX Pro, DGX Spark, and DGX Station Enterprise AI systems are ready to modernize the $500 billion IT infrastructure on prem or in the cloud.\n[3692s->3696s]Every major IT provider is partnering with us.\n[3696s->3710s]Industrial AI from training to digital twin simulation to deployment, NVIDIA Omniverse and Isaac Groot are powering next generation factories and humanoid robotic systems worldwide.\n[3710s->3723s]The age of AI is here from AI infrastructures, inference at scale, sovereign AI, enterprise AI, and industrial AI.\n[3723s->3725s]NVIDIA is ready.\n[3725s->3740s]Join us at GTC Paris, our keynote at VivaTech on June 11, talking about quantum GPU computing, robotic factories and robots, and celebrate our partnerships building AI factories across the region.\n[3740s->3745s]The NVIDIA Band will tour France, the UK, Germany, and Belgium.\n[3745s->3753s]Thank you for joining us at the earnings call today. See you in Paris.\n[3753s->3758s]This concludes today's conference call. You may now disconnect.\n"
        ]
    },
    {
        "question": "GB200 NVL72 系统在 MLPerf Inference 基准测试中的表现如何？",
        "ground_truth": "在最新的 MLPerf Inference 结果中，NVIDIA GB200 NVL72 系统在 Llama 3.1 405B 基准测试上，实现了比 NVIDIA H200 NVL8 高出 30 倍的吞吐量。",
        "answer": "GB200 NVL72 系统在 MLPerf Inference 基准测试中表现出色。具体来说，该系统在 Llama 3.1 405B 基准测试中实现了比 NVIDIA H200 NVL8 高达 30 倍的吞吐量。这是 NVIDIA 首次使用其专为 AI 推理设计的 rack-scale NVIDIA GB200 NVL72 系统提交的 MLPerf 提交结果。此外，NVIDIA DGX B200 系统在配备八个 Blackwell GPU 的情况下，其性能比八个 H200 GPU 在更具挑战性的 Llama 2 70B 基准测试中提升了 3 倍 [来源: Q1_2026_presentation, 第 5 页]。",
        "contexts": [
            "[IMAGE: Q1_2026_presentation_page24_img1.png]\n\nNVIDIA Blackwell Takes Pole Position \nin Latest MLPerf Inference Results\n\n• Marked NVIDIA’s first MLPerf submission using the rack-scale NVIDIA\n\nGB200 NVL72 system designed for AI reasoning\n\n• NVIDIA Blackwell set new MLPerf inference records:\n\n•\nGB200 NVL72 system delivered up to 30x higher throughput on the Llama \n3.1 405B benchmark vs. NVIDIA H200 NVL8. Only NVIDIA and its partners \nsubmitted results on this benchmark\n\n•\nNVIDIA DGX B200 system performance with eight Blackwell GPUs jumped \n3x vs. eight H200 GPUs on the more challenging version of the Llama 2 \n70B benchmark\n\n• Three years after launch, NVIDIA Hopper AI factory value continues\n\nincreasing:\n\n•\nOn the Llama 2 70B benchmark, H100 GPU throughput has increased by \n1.5x vs. a year ago. The H200 GPU with larger and faster GPU memory, \nextends that increase to 1.6x\n\n•\nHopper also ran every benchmark – Hopper can run a wide range of \nworkloads and keep pace as models and usage scenarios grow more \nchallenging\n\n• 15 partners submitted stellar results on the NVIDIA platform –\n\nincluding CoreWeave, Dell, Google Cloud, Oracle Cloud Infrastructure – \nreflecting the reach of the NVIDIA platform",
            "[IMAGE: Q1_2026_presentation_page14_img1.png]\n\n[IMAGE: Q1_2026_presentation_page14_img2.png]\n\nNVIDIA Blackwell Ultra Engineered \nfor the Era of AI Reasoning\n\n• Built on the Blackwell architecture, Blackwell Ultra boosts training\n\nand test-time inference scaling for agentic and physical AI\n\n• Configurations include:\n\n•\nNVIDIA GB300 NVL72; a rack-scale system delivering 1.5x more AI \nperformance vs. NVIDIA GB200, and increases Blackwell’s revenue \nopportunity by 50x for AI factories compared to Hopper. It connects 72 \nBlackwell Ultra GPUs + 36 Grace CPUs to act as a single GPU\n\n•\nNVIDIA HGX B300 NVL16 – baseboard features 11x faster inference on \nlarge language models, 7x more compute and 4x more memory compared \nto Hopper\n\n• AWS, Google Cloud, Microsoft Azure, Oracle Cloud Infrastructure and\n\nGPU cloud providers including CoreWeave, will be among the first to \noffer Blackwell Ultra instances.\n\n• Sampling of GB300 systems began in May at the major CSPs.\n\nProduction shipments expected to commence in Q2.",
            "Item 4. Controls and Procedures\n\nControls and Procedures\n\nDisclosure Controls and Procedures\n\nBased on their evaluation as of April  27, 2025, our management, including our Chief Executive Officer and Chief Financial Officer, has concluded that our\ndisclosure controls and procedures (as defined in Rule 13a-15(e) under the Exchange Act) were effective to provide reasonable assurance that the information\nwe are required to disclose in reports that we file or submit under the Exchange Act is recorded, processed, summarized and reported within the time periods\nspecified in the SEC rules and forms, and that such information is accumulated and communicated to our management, including our Chief Executive Officer\nand our Chief Financial Officer, as appropriate, to allow timely decisions regarding required disclosures.\n\nChanges in Internal Control Over Financial Reporting\n\nThere have been no changes in our internal control over financial reporting during the first quarter of fiscal year 2026 that have materially affected, or are\nreasonably likely to materially affect, our internal control over financial reporting. We are continuing a phased upgrade of our enterprise resource planning, or\nERP, system to update our existing core financial systems. The ERP system is designed to accurately maintain our financial records used to report operating\nresults. We will continue to evaluate each quarter whether there are changes that materially affect our internal control over financial reporting.\n\nInherent Limitations on Effectiveness of Controls\n\nOur management, including our Chief Executive Officer and Chief Financial Officer, does not expect that our disclosure controls and procedures or our internal\ncontrols, will prevent all error and all fraud. A control system, no matter how well conceived and operated, can provide only reasonable, not absolute, assurance\nthat the objectives of the control system are met. Further, the design of a control system must reflect the fact that there are resource constraints, and the benefits\nof controls must be considered relative to their costs. Because of the inherent limitations in all control systems, no evaluation of controls can provide absolute\nassurance that all control issues and instances of fraud, if any, within NVIDIA have been detected.\n\nPart II. Other Information\n\nItem 1. Legal Proceedings\n\nRefer to Part I, Item 1, Note 11  of the Notes to Condensed Consolidated Financial Statements for a discussion of significant developments in our legal\nproceedings since January 26, 2025. Also refer to Item 3, “Legal Proceedings” in our Annual Report on Form 10-K for the fiscal year ended January 26, 2025 for\na prior discussion of our legal proceedings.\n\nItem 1A. Risk Factors\n\nOther than the risk factors listed below, there have been no material changes from the risk factors previously described under Item 1A of our Annual Report on\nForm 10-K for the fiscal year ended January 26, 2025.\n\nPurchasing or owning NVIDIA common stock involves investment risks including, but not limited to, the risks described in Item 1A of our Annual Report on Form\n10-K for the fiscal year ended January 26, 2025, and below. Any one of those risks could harm our business, financial condition and results of operations or\nreputation, which could cause our stock price to decline. Additional risks, trends and uncertainties not presently known to us or that we currently believe are\nimmaterial may also harm our business, financial condition, results of operations or reputation.\n\nLong manufacturing lead times and uncertain supply and capacity availability, combined with a failure to estimate customer demand accurately, has\nled and could lead to mismatches between supply and demand.\n\nWe have long manufacturing lead times and build finished products and maintain inventory in advance of anticipated demand. In periods of shortages impacting\nthe semiconductor industry and/or limited supply or capacity in our supply chain, the lead times for certain supply may be extended. We have previously\nexperienced and may continue to experience extended lead times of more than 12 months. To secure future supply and capacity, we have paid premiums,\nprovided deposits, and entered into long-term supply agreements and capacity commitments, which have increased our product costs and this may continue.\nWe may still be unable to secure sufficient commitments for capacity to address our business needs.\n\nIf we inaccurately estimate demand, or our customers change orders, as we have experienced in the past, we may not be able to reduce our supply\ncommitments in time, at the same rate, or at all. Significant mismatches between supply and demand have varied across our market platforms, resulted in both\nproduct shortages and excess inventory, significantly harmed our financial results and could reoccur. If we underestimate demand, and our foundry partners and\ncontract manufacturers are unable to increase production or provide sufficient supply, we may not be able to meet increased\n\n30"
        ]
    },
    {
        "question": "公司在 Q1 FY26 通过股票回购和股息向股东回报了多少现金？",
        "ground_truth": "在 2026 财年第一季度，公司斥资 145 亿美元回购了 1.26 亿股股票，并支付了 2.44 亿美元的现金股息。",
        "answer": "英伟达在 Q1 FY26 通过股票回购和股息向股东回报了共计 $14.3 亿美元。其中，股票回购金额为 $14.1 亿美元，现金股息为 $244 百万美元 [来源: Q1_2026_presentation, 第 11 页]。",
        "contexts": [
            "Q1 FY26\nEarnings Summary",
            "[IMAGE: Q1_2026_presentation_page11_img1.png]\n\nHighlights\n\n• Y/Y increase reflects higher revenue and timing of cash\n\ncollections\n\n• Q/Q increase was driven by higher revenue, timing of cash\n\ncollections, and lower cash taxes\n\n• Expect a substantial increase in cash taxes in Q2 related to\n\nestimated federal and state cash tax payments\n\n• Utilized cash of $14.3B towards shareholder returns, including\n\n$14.1B in share repurchases and $244M in cash dividends\n\n• Invested $1.3B in capex (includes principal payments on PP&E)\n\n• Ended the quarter with $53.7B in gross cash and $8.5B in debt;\n\n$42.5B in net cash\n\nSources & Uses of Cash\n\nCash Flow from Operations ($M)\n\nGross cash is defined as cash/cash equivalents & marketable securities.\nNet cash is defined as gross cash less debt.\nDebt is defined as principal value of debt.",
            "[TABLE]\n|  | Non-GAAP | Acquisition-Related and Other Costs (A) | Stock-Based Compensation (B) | Other (C) | Tax Impact of Adjustments | GAAP |\n| --- | --- | --- | --- | --- | --- | --- |\n|  |  |  |  |  |  |  |\n|  | $26,858 | (123) | (64) | (3) | — |  |\n|  | 61.0% | (0.3) | (0.2) | — | — |  |\n|  | $23,275 | (160) | (1,474) | (3) | — |  |\n|  | $19,894 | (160) | (1,474) | (179) | 694 |  |\n|  | 24,611 | — | — | — | — |  |\n|  | $0.81 | (0.01) | (0.06) | (0.01) | 0.03 |  |\n[/TABLE]\n\n该表格展示了公司非GAAP和GAAP利润的对比，主要趋势包括：非GAAP利润为$26,858，主要受收购相关成本和股票补偿影响；GAAP利润为$19,894，调整税后为$24,611，显示公司盈利能力受特定成本调整影响。\n\nReconciliation of Non-GAAP to GAAP Financial Measures\n\n*Q1 FY26 included a $4.5 billion charge associated with H20 excess inventory and purchase obligations\nA. Consists of amortization of intangible assets, transaction costs, and certain compensation charges.\nB. Stock-based compensation charge was allocated to cost of goods sold, research and development expense, and sales, general and administrative expense. \nC. Other consists of legal settlement cost, losses from non-marketable equity securities and publicly-held equity securities, net, and interest expense related to amortization of debt discount."
        ]
    },
    {
        "question": "Q1 FY26 游戏（Gaming）业务的收入和同比增长率是多少？",
        "ground_truth": "2026 财年第一季度，游戏（Gaming）业务收入为 37.63 亿美元，同比增长 42%。",
        "answer": "根据提供的财务报告，英伟达在2026财年第一季度的游戏（Gaming）业务收入为38亿美元，同比增长42%。[来源: NVIDIA Q1_2026 financial report, 第 9 页]",
        "contexts": [
            "[0s->5s]Good afternoon. My name is Sarah and I will be your conference operator today.\n[5s->13s]At this time, I would like to welcome everyone to NVIDIA's first quarter fiscal 2026 financial results conference call.\n[13s->17s]All lines have been placed on mute to prevent any background noise.\n[17s->21s]After the speaker's remarks, there will be a question and answer session.\n[21s->27s]If you would like to ask a question during this time, simply press star one on your telephone keypad.\n[27s->32s]If you would like to withdraw your question, please press star one again. Thank you.\n[32s->36s]Toshia Hari, you may begin your conference.\n[36s->44s]Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the first quarter of fiscal 2026.\n[44s->54s]With me today from NVIDIA are Jensen Wong, President and Chief Executive Officer, and Collette Press, Executive Vice President and Chief Financial Officer.\n[54s->60s]I'd like to remind you that our call is being webcast live on NVIDIA's investor relations website.\n[60s->68s]The webcast will be available for replay until the conference call to discuss our financial results for the second quarter of fiscal 2026.\n[68s->76s]The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent.\n[76s->81s]During this call, we may make forward looking statements based on current expectations.\n[81s->88s]These are subject to a number of significant risks and uncertainties, and our actual results may differ materially.\n[88s->96s]For a discussion of factors that could affect our future financial results in business, please refer to the disclosure in today's earnings release,\n[96s->104s]our most recent forms 10-K and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission.\n[105s->112s]All our statements are made as of today, May 28, 2025, based on information currently available to us.\n[112s->118s]Except as required by law, we assume no obligation to update any such statements.\n[118s->122s]During this call, we will discuss non-GAAP financial measures.\n[122s->130s]You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website.\n[130s->133s]With that, let me turn the call over to Collette.\n[133s->135s]Thank you, Toshio.\n[135s->148s]We delivered another strong quarter with revenue of $44 billion, up 69% year-over-year, exceeding our outlook in what proved to be a challenging operating environment.\n[148s->153s]Data center revenue of $39 billion grew 73% year-on-year.\n[153s->161s]AI workloads have transitioned strongly to inference, and AI factory build-outs are driving significant revenue.\n[161s->165s]Our customers' commitments are firm.\n[165s->176s]On April 9, the U.S. government issued new export controls on H20, our data center GPU designed specifically for the China market.\n[176s->181s]We sold H20 with the approval of the previous administration.\n[181s->187s]Although our H20 has been in the market for over a year and does not have a market outside of China,\n[187s->195s]the new export controls on H20 did not provide a grace period to allow us to sell through our inventory.\n[195s->204s]In Q1, we recognized $4.6 billion in H20 revenue, which occurred prior to April 9,\n[204s->216s]but also recognized a $4.5 billion charge as we wrote down inventory and purchase obligations tied to orders we had received prior to April 9.\n[216s->226s]We were unable to ship $2.5 billion in H20 revenue in the first quarter due to the new export controls.\n[226s->234s]The $4.5 billion charge was less than what we initially anticipated as we were able to reuse certain materials.\n[234s->244s]We are still evaluating our limited options to supply data center compute products compliant with the U.S. government's revised export control rules,\n[244s->252s]losing access to the China AI accelerator market, which we believe will grow to nearly $50 billion,\n[252s->262s]would have a material adverse impact on our business going forward and benefit our foreign competitors in China and worldwide.\n[262s->271s]Our Blackwell ramp, the fastest in our company's history, drove a 73% year-on-year increase in data center revenue.\n[271s->280s]Blackwell contributed nearly 70% of data center compute revenue in the quarter, with a transition from Hopper nearly complete.\n[280s->293s]The introduction of GB200 NVL was a fundamental architectural change to enable data center scale workloads and to achieve the lowest cost per inference token.\n[293s->305s]While these systems are complex to build, we have seen a significant improvement in manufacturing yields and rack shipments are moving to strong rates to end customers.\n[305s->315s]GB200 NVL racks are now generally available for modern builders, enterprises, and sovereign customers to develop and deploy AI.\n[315s->332s]On average, major hyperscalers are each deploying nearly 1,000 NVL 72 racks or 72,000 Blackwell GPUs per week and are on track to further ramp output this quarter.\n[332s->346s]Microsoft, for example, has already deployed tens of thousands of Blackwell GPUs and is expected to ramp to hundreds of thousands of GB200s with OpenAI as one of its key customers.\n[346s->356s]Key learnings from the GB200 ramp will allow for a smooth transition to the next phase of our product roadmap, Blackwell Ultra.\n[357s->370s]Sampling of GB300 systems began earlier this month at the major CSPs, and we expect production shipments to commerce later this quarter.\n[370s->381s]GB300 will leverage the same architecture, same physical footprint, and the same electrical and mechanical specifications as GB200.\n[381s->394s]The GB300 drop-in design will allow CSPs to seamlessly transition their systems and manufacturing used for GB200 while maintaining high yields.\n[394s->407s]B300 GPUs with 50% more HBM will deliver another 50% increase in dense FP4 inference compute performance compared to the B200.\n[407s->421s]We remain committed to our annual product cadence, with our roadmap extending through 2028 tightly aligned with the multiple-year planning cycles of our customers.\n[421s->432s]We are witnessing a sharp jump in inference demand. OpenAI, Microsoft, and Google are seeing a step-function leap in token generation.\n[432s->441s]Microsoft processed over 100 trillion tokens in Q1, a five-fold increase on a year-over-year basis.\n[441s->453s]This exponential growth in Azure OpenAI is representative of strong demand for Azure AI Foundry as well as other AI services across Microsoft's platform.\n[453s->471s]Inference-serving startups are now serving models using B200, tripling their token generation rate and corresponding revenues for high-value reasoning models such as DeepSeq R1, as reported by Artificial Analysis.\n[471s->485s]NVIDIA Dynamo on Blackwell NVL72 turbocharges AI inference throughput by 30x for the new reasoning models sweeping the industry.\n[485s->497s]Developer engagements increased with adoption ranging from LLM providers such as Perplexity to financial services institutions such as Capital One,\n[497s->503s]who reduced agentic chatbox latency by 5x with Dynamo.\n[503s->511s]In the latest MLPerf inference results, we submitted our first results using GB200 NVL72,\n[511s->523s]delivering up to 30x higher inference throughput compared to our 8 GPU H200 submission on the challenging Llama 3.1 benchmark.\n[523s->535s]This feat was achieved through a combination of tripling the performance per GPU as well as 9x more GPUs, all connected on a single NVLink domain.\n[535s->547s]And while Blackwell is still early in its lifecycle, software optimizations have already improved its performance by 1.5x in the last month alone.\n[547s->555s]We expect to continue improving the performance of Blackwell through its operational life as we have done with Hopper and Amper.\n[555s->560s]For example, we increased the inference performance of Hopper by 4x over two years.\n[560s->568s]This is the benefit of NVIDIA's programmable CUDA architecture and rich ecosystem.\n[568s->578s]The pace and scale of AI factory deployments are accelerating with nearly 100 NVIDIA-powered AI factories in flight this quarter,\n[578s->588s]a two-fold increase year over year, with the average number of GPUs powering each factory also doubling in the same period.\n[588s->593s]And more AI factory projects are starting across industries and geographies.\n[593s->605s]NVIDIA's full-stack architecture is underpinning AI factory deployments as industry leaders like AT&T, BYD, Capital One, Foxconn, MediaTek, and Telenor\n[605s->614s]are strategically vital sovereign clouds like those recently announced in Saudi Arabia, Taiwan, and the UAE.\n[614s->623s]We have a line of sight to projects requiring tens of gigawatts of NVIDIA AI infrastructure in the not-too-distant future.\n[623s->639s]The transition from generative to agentic AI, AI capable of perceiving, reasoning, planning, and acting, will transform every industry, every company, and country.\n[639s->650s]We envision AI agents as a new digital workforce capable of handling tasks ranging from customer service to complex decision-making processes.\n[650s->660s]We introduced the LAMA Nemotron family of open reasoning models designed to supercharge identical AI platforms for enterprises.\n[660s->672s]Built on the LAMA architecture, these models are available as NIMS or NVIDIA inference microservices with multiple sizes to meet diverse deployment needs.\n[672s->681s]Our post-training enhancements have yielded a 20% accuracy boost and a 5x increase in inference speed.\n[681s->690s]Leading platform companies including Accenture, Cadence, Deloitte, and Microsoft are transforming work with our reasoning models.\n[690s->702s]NVIDIA NEMO microservices are generally available across industries that are being leveraged by leading enterprises to build, optimize, and scale AI applications.\n[702s->711s]With NEMO, Cisco increased model accuracy by 40% and improved response time by 10x in its code assistant.\n[711s->719s]NASDAQ realized a 30% improvement in accuracy and response time in its AI platform's search capabilities.\n[719s->727s]And Shell's Custom LLM achieved a 30% increase in accuracy when trained with NVIDIA NEMO.\n[727s->735s]NEMO's parallelism techniques accelerated model training time by 20% when compared to other frameworks.\n[735s->757s]We also announced a partnership with Yum Brands, the world's largest restaurant company, to bring NVIDIA AI to 500 of its restaurants this year and expanding to 61,000 restaurants over time to streamline order taking, optimize operations, and enhance service across its restaurants.\n[757s->779s]For AI-powered cybersecurity, leading companies like Checkpoint, CloudStrike, and Palo Alto Networks are using NVIDIA's AI security and software stack to build, optimize, and secure agentic workflows, with CloudStrike realizing 2x faster detection triage with 50% less compute cost.\n[780s->790s]Moving to networking, sequential growth in networking resumed in Q1, with revenue up 64% quarter-over-quarter to $5 billion.\n[790s->798s]Our customers continue to leverage our platform to efficiently scale up and scale out AI factory workloads.\n[798s->813s]We created the world's fastest switch, NVLink. For scale up, our NVLink compute fabric in its fifth generation offers 14x the bandwidth of PCIe Gen 5.\n[813s->825s]NVLink 72 carries 130 TB per second of bandwidth in a single rack, equivalent to the entirety of the world's peak internet traffic.\n[825s->833s]NVLink is a new growth vector and is off to a great start, with Q1 shipments exceeding $1 billion.\n[833s->848s]At Computex, we announced NVLink Fusion. Hyperscale customers can now build semi-custom CCUs and accelerators that connect directly to the NVIDIA platform with NVLink.\n[848s->868s]We are now enabling key partners, including ASIC providers such as MediaTek, Marvell, Alchip Technologies, and Astera Labs, as well as CPU suppliers such as Fujitsu and Qualcomm, to leverage NVLink Fusion to connect our respective ecosystems.\n[868s->878s]For scale out, our enhanced Ethernet offerings deliver the highest throughput, lowest latency networking for AI.\n[878s->886s]SpectrumX posted strong sequential and year-on-year growth and is now annualizing over $8 billion in revenue.\n[886s->896s]Adoption is widespread across major CSPs and consumer internet companies, including CoreWe, Microsoft Azure, Oracle Cloud, and XAI.\n[896s->903s]This quarter, we added Google Cloud and Meta to the growing list of SpectrumX customers.\n[903s->913s]We introduced SpectrumX and QuantumX, silicon photonics switches featuring the world's most advanced co-packaged optics.\n[913s->933s]These platforms will enable next level AI factory scaling to millions of GPUs through the increasingly power efficiency by 3.5x and network resiliency by 10x while accelerating customer time to market by 1.3x.\n[933s->938s]Transitioning to a quick summary of our revenue by geography.\n[938s->949s]China, as a percentage of our data center revenue, was slightly below our expectations and down sequentially due to H20 export licensing controls.\n[949s->955s]For Q2, we expect a meaningful decrease in China data center revenue.\n[955s->971s]As a reminder, while Singapore represented nearly 20% of our Q1 billed revenue, as many of our large customers use Singapore for centralized invoicing, our products are almost always sold elsewhere.\n[971s->984s]Note that over 99% of H100, H200, and Blackwell data center compute revenue billed to Singapore was for orders from US based customers.\n[984s->996s]Moving to gaming and AI PCs. Gaming revenue was a record 3.8 billion, increasing 48% sequentially and 42% year on year.\n[996s->1012s]Strong adoption by gamers, creatives, and AI enthusiasts have made Blackwell our fastest ramp ever. Against a backdrop of robust demand, we greatly improved our supply and availability in Q1 and expect to continue these efforts in Q2.\n[1012s->1017s]AI is transforming PC and creator and gamers.\n[1017s->1035s]With a 100 million user installed base, GeForce represents the largest footprint for PC developers. This quarter, we added to our AI PC laptop offerings, including models capable of running Microsoft's Copilot Plus.\n[1035s->1049s]This past quarter, we brought Blackwell architecture to mainstream gaming with its launch of GeForce RTX 5060 and 5060 Ti, starting at just $299.\n[1049s->1061s]The RTX 5060 also debuted in laptops starting at $1,099. These systems that double the frame rate and slash latency.\n[1061s->1070s]These GeForce RTX 5060 and 5060 Ti desktop GPUs and laptops are now available.\n[1070s->1090s]In console gaming, the recently unveiled Nintendo Switch 2 leverages Nvidia's neural rendering and AI technologies, including next generation custom RTX GPUs with DLSS technology, to deliver a giant leap in gaming performance to millions of players worldwide.\n[1090s->1100s]Nintendo has shipped over 150 million Switch consoles to date, making it one of the most successful gaming systems in history.\n[1100s->1108s]Moving to pro visualization. Revenue of $509 million was flat sequentially and up 19% year on year.\n[1108s->1120s]Tariff related uncertainty temporarily impacted Q1 systems and demand for AI workstations is strong and we expect sequential revenue growth to resume in Q2.\n[1120s->1130s]Nvidia DGX Spark and Station revolutionize personal computing by putting the power of an AI supercomputer in a desktop form factor.\n[1130s->1143s]DGX Spark delivers up to one petaflop of AI compute while DGX Station offers an incredible 20 petaflops and is powered by the GB300 Superchip.\n[1143s->1150s]DGX Spark will be available in Calendar Q3 and DGX Station later this year.\n[1150s->1163s]We have deepened Omniverse's integration and adoption into some of the world's leading software platforms, including Databricks, SAP, and Schneider Electric.\n[1163s->1178s]New Omniverse blueprints such as Mega for at scale robotic fleet management are being leveraged in Keyon Group, Pegatron, Accenture, and other leading companies to enhance industrial operations.\n[1178s->1189s]At Confitex, we showcased Omniverse's great traction with technology manufacturing leaders, including TSMC, Quanta, Foxconn, Pegatron.\n[1189s->1194s]Using Omniverse, TSMC saves months in work by designing fabs virtually.\n[1194s->1205s]Foxconn accelerates thermal simulations by 150x and Pegatron reduced assembly line defects rates by 67%.\n[1205s->1214s]Lastly, with our automotive group, revenue was $567 million, down 1% sequentially but up 72% year on year.\n[1214s->1222s]Year on year growth was driven by the ramp of self-driving across a number of customers and robust end demand for NEVs.\n[1222s->1232s]We are partnering with GM to build the next-gen vehicles, factories, and robots using NVIDIA AI, simulation, and accelerated computing.\n[1232s->1241s]And we are now in production with our full-stack solution for Mercedes-Benz, starting with the new CLA, hitting roads in the next few months.\n[1241s->1254s]We announced Isaac, Groot, and won the world's first open fully customizable foundation model for humanoid robots, enabling generalized reasoning and skill development.\n[1254s->1260s]We also launched new open NVIDIA Cosmo World Foundation models.\n[1260s->1268s]Leading companies include 1X, Agility Robots, Robotics, Figure AI, Uber, and Wabi.\n[1268s->1284s]We've begun integrating Cosmos into their operations for synthetic data generation, while Agility Robotics, Boston Dynamics, and XPen Robotics are harnessing Isaac simulation to advance their humanoid efforts.\n[1284s->1300s]GE Healthcare is using the new NVIDIA Isaac platform for healthcare simulation, built on NVIDIA Omniverse and using NVIDIA Cosmos, the platform speeds development of robotic imaging and surgery systems.\n[1300s->1314s]The era of robotics is here. Billions of robots, hundreds of millions of autonomous vehicles, and hundreds of thousands of robotic factories and warehouses will be developed.\n[1314s->1335s]All right, moving to the rest of the P&L. Gap gross margins and non-gap gross margins were 60.5% and 61%, respectively. Excluding the $4.5 billion charge, Q1 non-gap gross margins would have been 71.3%, slightly above our outlook at the beginning of the quarter.\n[1335s->1346s]Sequentially, gap operating expenses were up 7% and non-gap operating expenses were up 6%, reflecting higher compensation and employee growth.\n[1346s->1355s]Our investments include expanding our infrastructure capabilities and AI solutions, and we plan to grow these investments throughout the fiscal year.\n[1355s->1370s]In Q1, we returned a record $14.3 billion to shareholders in the form of share repurchases and cash dividends. Our capital return program continues to be a key element of our capital allocation strategy.\n[1370s->1384s]Let me turn to the outlook for the second quarter. Total revenue is expected to be $45 billion, plus or minus 2%. We expect modest sequential growth across all of our platforms.\n[1384s->1403s]In data center, we anticipate the continued ramp of Blackwell to be partially offset by a decline in China revenue. Note, our outlook reflects a loss in H20 revenue of approximately $8 billion for the second quarter.\n[1403s->1420s]Gap and non-gap gross margins are expected to be 71.8% and 72% respectively, plus or minus 50 basis points. We expect better Blackwell profitability to drive modest sequential improvement in gross margins.\n[1420s->1427s]We are continuing to work towards achieving gross margins in the mid-70s range late this year.\n[1427s->1444s]Gap and non-gap operating expenses are expected to be approximately $5.7 billion and $4 billion respectively, and we continue to expect full-year fiscal year 26 operating expense growth to be in the mid-30% range.\n[1444s->1457s]Gap and non-gap other income and expenses are expected to be an income of approximately $450 million, excluding gays and losses from non-marketable and publicly held equity securities.\n[1457s->1468s]Gap and non-gap tax rates are expected to be 16.5%, plus or minus 1%, excluding any discrete items.\n[1468s->1480s]Further financial details are included in the CFO commentary and other information available on our IR website, including a new financially information AI agent.\n[1480s->1488s]Let me highlight upcoming events for the financial community. We will be at the B of A Global Technology Conference in San Francisco on June 4.\n[1488s->1500s]The Rosenblatt Virtual AI Summit and NASDAQ Investor Conference in London on June 10, and GTC Paris at Viva Tech on June 11 in Paris.\n[1500s->1511s]We look forward to seeing you at these events. Our earnings call to discuss the results of our second quarter of fiscal 2026 is scheduled for August 27.\n[1511s->1517s]Well now, let me turn it over to Jensen to make some remarks.\n[1517s->1526s]Thanks, Collette. We've had a busy and productive year. Let me share my perspective on some topics we're frequently asked.\n[1526s->1536s]On export control, China is one of the world's largest AI markets and a springboard to global success.\n[1536s->1543s]With half of the world's AI researchers based there, the platform that wins China is positioned to lead globally.\n[1543s->1551s]Today, however, the $50 billion China market is effectively closed to U.S. industry.\n[1551s->1560s]The H20 export ban ended our Hopper data center business in China. We cannot reduce Hopper further to comply.\n[1561s->1568s]As a result, we are taking a multi-billion dollar write-off on inventory that cannot be sold or repurposed.\n[1568s->1575s]We are exploring limited ways to compete, but Hopper is no longer an option.\n[1575s->1584s]China's AI moves on with or without U.S. chips. It has to compute to train and deploy advanced models.\n[1585s->1590s]The question is not whether China will have AI. It already does.\n[1590s->1596s]The question is whether one of the world's largest AI markets will run on American platforms.\n[1596s->1604s]Shielding Chinese chipmakers from U.S. competition only strengthens them abroad and weakens America's position.\n[1604s->1609s]Export restrictions have spurred China's innovation in scale.\n[1609s->1616s]The AI race is not just about chips. It's about which stack the world runs on.\n[1616s->1624s]As that stack grows to include 6G and quantum, U.S. global infrastructure leadership is at stake.\n[1624s->1630s]The U.S. has based its policy on the assumption that China cannot make AI chips.\n[1630s->1635s]That assumption was always questionable, and now it's clearly wrong.\n[1635s->1639s]China has enormous manufacturing capability.\n[1639s->1645s]In the end, the platform that wins the AI developers wins AI.\n[1645s->1653s]Export controls should strengthen U.S. platforms, not drive half of the world's AI talent to rivals.\n[1654s->1657s]On DeepSeek.\n[1657s->1665s]DeepSeek and QN from China are among the best open source AI models.\n[1665s->1671s]Released freely, they've gained traction across the U.S., Europe, and beyond.\n[1671s->1680s]DeepSeek R1, like ChatGPT, introduced reasoning AI that produces better answers the longer it thinks.\n[1680s->1690s]Reasoning AI enables step-by-step problem solving, planning, and tool use, turning models into intelligent agents.\n[1690s->1705s]Reasoning is compute intensive, requires hundreds to thousands of times more tokens per task than previous one-shot inference.\n[1705s->1711s]Reasoning models are driving a step function surge in inference demand.\n[1711s->1721s]AI scaling laws remain firmly intact, not only for training, but now inference, too, requires massive scale compute.\n[1721s->1726s]DeepSeek also underscores the strategic value of open source AI.\n[1726s->1734s]When popular models are trained and optimized on U.S. platforms, it drives usage, feedback, and continuous improvement.\n[1734s->1737s]Reinforcing American leadership across the stack.\n[1737s->1743s]U.S. platforms must remain the preferred platform for open source AI.\n[1743s->1749s]That means supporting collaboration with top developers globally, including in China.\n[1749s->1757s]America wins when models like DeepSeek and QN runs best on American infrastructure.\n[1757s->1760s]Regarding onshore manufacturing.\n[1760s->1770s]President Trump has outlined a bold vision to reshore advanced manufacturing, create jobs, and strengthen national security.\n[1770s->1774s]Future plants will be highly computerized in robotics.\n[1774s->1776s]We share this vision.\n[1776s->1783s]TSMC is building six fabs and two advanced packaging plants in Arizona to make chips for Nvidia.\n[1783s->1789s]Process qualification is underway, with volume production expected by year end.\n[1790s->1798s]Spill and Amcor are also investing in Arizona, constructing packaging, assembly, and test facilities.\n[1798s->1806s]In Houston, we're partnering with Foxconn to construct a million-square-foot factory to build AI supercomputers.\n[1806s->1811s]Wistron is building a similar plant in Fort Worth, Texas.\n[1811s->1817s]To encourage and support these investments, we've made substantial long-term purchase commitments.\n[1817s->1821s]A deep investment in America's AI manufacturing future.\n[1823s->1827s]Our goal, from chip to supercomputer.\n[1827s->1830s]Built in America within a year.\n[1830s->1840s]Each GB200 and Beelink 72 racks contains 1.2 million components and weighs nearly two tons.\n[1840s->1844s]No one has produced supercomputers on this scale.\n[1844s->1847s]Our partners are doing an extraordinary job.\n[1849s->1857s]On AI diffusion rule, President Trump rescinded the AI diffusion rule, calling it counterproductive,\n[1857s->1864s]and proposed a new policy to promote US AI tech with trusted partners.\n[1864s->1868s]On his Middle East tour, he announced historic investments.\n[1868s->1875s]I was honored to join him in announcing a 500-megawatt AI infrastructure project in Saudi Arabia\n[1875s->1880s]and a 5-gigawatt AI campus in the UAE.\n[1880s->1883s]President Trump wants US tech to lead.\n[1883s->1887s]The deals he announced are wins for America.\n[1887s->1895s]Creating jobs, advancing infrastructure, generating tax revenue, and reducing the US trade deficit.\n[1895s->1902s]The US will always be Nvidia's largest market and home to the largest install base of our infrastructure.\n[1902s->1909s]Every nation now sees AI as core to the next industrial revolution.\n[1909s->1915s]A new industry that produces intelligence and essential infrastructure for every economy.\n[1915s->1922s]Countries are racing to build national AI platforms to elevate their digital capabilities.\n[1922s->1930s]At Computex, we announced Taiwan's first AI factory in partnership with Foxconn and the Taiwan government.\n[1930s->1936s]Last week, I was in Sweden to launch its first national AI infrastructure.\n[1936s->1947s]Japan, Korea, India, Canada, France, the UK, Germany, Italy, Spain, and more are now building national AI factories\n[1947s->1951s]to empower startups, industries, and societies.\n[1951s->1956s]Sovereign AI is a new growth engine for Nvidia.\n[1958s->1960s]Toshia, back to you. Thank you.\n[1960s->1966s]Operator, we will now open the call for questions. Would you please pull for questions?\n[1966s->1974s]Thank you. At this time, I would like to remind everyone in order to ask a question, press star, then the number one on your telephone keypad.\n[1974s->1978s]We'll pause for just a moment to compile the Q&A roster.\n[1982s->1988s]Your first question comes from the line of Joe Moore with Morgan Stanley. Your line is open.\n[1990s->1998s]Great. Thank you. You guys have talked about this scaling up of inference around reasoning models for at least a year now,\n[1998s->2002s]and we've really seen that come to fruition as you talked about. We've heard it from your customers.\n[2002s->2007s]Can you give us a sense for how much of that demand you're able to serve?\n[2007s->2010s]And give us a sense for maybe how big the inference business is for you guys.\n[2010s->2016s]And do we need full-on NVL-72 rack-scale solutions for reasoning inference going forward?\n[2019s->2026s]Well, we would like to serve all of it. And I think we're on track to serve most of it.\n[2026s->2038s]Grace Blackwell NVLink-72 is the ideal engine today, the ideal computer thinking machine, if you will, for reasoning AI.\n[2038s->2046s]There's a couple of reasons for that. The first reason is that the token generation amount,\n[2046s->2059s]the number of tokens reasoning goes through is a hundred, a thousand times more than a one-shot chatbot.\n[2059s->2069s]You know, it's essentially thinking to itself, breaking down a problem step by step. It might be planning multiple paths to an answer.\n[2069s->2083s]It could be using tools, reading PDFs, reading web pages, watching videos, and then producing a result, an answer.\n[2083s->2089s]The longer it thinks, the better the answer, the smarter the answer is.\n[2090s->2101s]And so what we would like to do and the reason why Grace Blackwell was designed to give such a giant step up in inference performance\n[2101s->2106s]is so that you could do all this and still get a response as quickly as possible.\n[2106s->2117s]Compared to Hopper, Grace Blackwell is some 40 times higher speed and throughput compared.\n[2117s->2132s]And so this is going to be a huge, huge benefit and driving down the cost while improving the quality of response with excellent quality of service at the same time.\n[2132s->2139s]So that's the fundamental reason. That was the core driving reason for Grace Blackwell NVLink-72.\n[2139s->2151s]Of course, in order to do that, we had to reinvent, literally redesign the entire way that these supercomputers are built.\n[2151s->2161s]But now we're in full production. It's going to be exciting. It's going to be incredibly exciting.\n[2162s->2171s]The next question comes from Vivek Arya with Bank of America Securities. Your line is open.\n[2171s->2176s]Thanks for the question. Just a clarification for Colette first.\n[2176s->2184s]So on the China impact, I think previously it was mentioned that at about $15 billion, so you had the $8 billion in Q2.\n[2184s->2190s]So is there still some left as a headwind for the remaining quarters, just Colette, how to model that?\n[2190s->2201s]And then question, Jensen, for you. Back at GTC, you had outlined a path towards almost a trillion dollars of AI spending over the next few years.\n[2201s->2212s]Where are we in that build out? And do you think it's going to be uniform that you will see every spender, whether it's ESP, sovereigns, enterprises or build out?\n[2212s->2215s]Should we expect some periods of digestion in between?\n[2215s->2221s]What are your customer discussions telling you about how to model growth for next year?\n[2223s->2232s]Yes, Vivek. Thanks so much for the question regarding H20. Yes, we recognized 4.6 H20 in Q1.\n[2232s->2241s]We were unable to ship $2.5 billion. So the total for Q1 should have been $7 billion.\n[2241s->2248s]When we look at our Q2, our Q2 is going to be meaningfully down in terms of China data center revenue.\n[2248s->2258s]And we had highlighted in terms of the amount of orders that we had planned for H20 and Q2, and that was $8 billion.\n[2258s->2266s]Now, going forward, we did have other orders going forward that we will not be able to fulfill.\n[2266s->2275s]That is what was incorporated, therefore, in the amount that we wrote down of the $4.5 billion.\n[2275s->2280s]That write down was about inventory and purchase commitments.\n[2280s->2287s]And our purchase commitments were about what we expected regarding the orders that we had received.\n[2287s->2295s]Going forward, though, it's a bigger issue regarding the amount of the market that we will not be able to serve.\n[2295s->2306s]We assess that TAM to be close to about $50 billion in the future as we don't have a product to enable for the China.\n[2310s->2318s]Vivek, probably the best way to think through it is that AI is several things.\n[2318s->2326s]Of course, we know that AI is this incredible technology that's going to transform every industry,\n[2326s->2344s]from, of course, the way we do software to health care and financial services to retail to, I guess, every industry, transportation, manufacturing.\n[2344s->2347s]And we're at the beginning of that.\n[2347s->2352s]But maybe another way to think about that is where do we need intelligence?\n[2352s->2354s]Where do we need digital intelligence?\n[2354s->2358s]And it's in every country, it's in every industry.\n[2358s->2365s]And we know because of that, we recognize that AI is also an infrastructure.\n[2365s->2371s]It's a way of delivering a technology that requires factories.\n[2371s->2374s]And these factories produce tokens.\n[2374s->2380s]And they, as I mentioned, are important to every single industry in every single country.\n[2380s->2389s]And so on that basis, we're really at the very beginning of it because the adoption of this technology is really kind of in its early stages.\n[2389s->2399s]Now, we've reached an extraordinary milestone with AIs that are reasoning, are thinking, what people call inference time scaling.\n[2399s->2413s]And of course, it created a whole new, we've entered an era where inference is going to be a significant part of the compute workload.\n[2413s->2417s]But anyhow, it's going to be a new infrastructure.\n[2417s->2421s]And we're building it out in the clouds.\n[2421s->2429s]The United States is really the early starter and available in US clouds.\n[2429s->2432s]And this is our largest market, our largest installed base.\n[2432s->2435s]And we can continue to see that happening.\n[2435s->2442s]But beyond that, we're going to have to, we're going to see AI go into enterprise, which is on prem.\n[2442s->2447s]Because so much of the data is still on prem, access control is really important.\n[2447s->2452s]It's really hard to move all of every company's data into the cloud.\n[2452s->2455s]And so we're going to move AI into the enterprise.\n[2455s->2460s]And you saw that we announced a couple of really exciting new products.\n[2460s->2468s]Our RTX Pro Enterprise AI server that runs everything enterprise and AI.\n[2468s->2476s]Our DGX Spark and DGX Station, which is designed for developers who want to work on prem.\n[2476s->2481s]And so enterprise AI is just taking off.\n[2481s->2490s]Telcos, today, a lot of the telco infrastructure will be in the future software defined and built on AI.\n[2490s->2492s]And so 6G is going to be built on AI.\n[2492s->2495s]And that infrastructure needs to be built out.\n[2495s->2498s]And they said it's very, very early stages.\n[2498s->2505s]And then, of course, every factory today that makes things will have an AI factory that sits with it.\n[2505s->2513s]And the AI factory is going to be drive creating AI and operating AI for the factory itself.\n[2513s->2518s]But also to power the products and the things that are made by the factory.\n[2518s->2523s]So it's very clear that every car company will have AI factories.\n[2523s->2527s]And very soon there'll be robotics companies, robot companies.\n[2527s->2534s]And those companies will be also building AIs to drive the robots.\n[2534s->2542s]And so we're at the beginning of all of this build out.\n[2542s->2547s]The next question comes from CJ News with Cantor Fitzgerald.\n[2547s->2549s]Your line is open.\n[2549s->2552s]Yeah, good afternoon. Thank you for taking the question.\n[2552s->2556s]There have been many large GPU cluster investment announcements in the last month.\n[2556s->2560s]And you alluded to a few of them with Saudi Arabia, the UAE.\n[2560s->2563s]And then also, you know, we heard from Oracle and XAI, just to name a few.\n[2563s->2570s]So my question, are there other that have yet to be announced of the same kind of scale and magnitude?\n[2570s->2583s]And perhaps more importantly, how are these orders impacting your lead times for Blackwell and your current visibility sitting here today, you know, almost halfway through 2025?\n[2583s->2593s]Well, we have more orders today than we did at the last time I spoke about orders at GTC.\n[2593s->2599s]However, we're also increasing our supply chain and building out our supply chain.\n[2599s->2602s]They're doing a fantastic job.\n[2602s->2613s]We're building it here on shore in the United States, but we're going to keep our supply chain quite busy for several many more years coming.\n[2613s->2625s]And with respect to further announcements, I'm going to be on the road next week through Europe.\n[2625s->2640s]And it's just about every country needs to build out AI infrastructure and their umpteen AI factories being planned.\n[2640s->2650s]I think in the remarks, Colette mentioned there's some 100 AI factories being built.\n[2650s->2653s]There's a whole bunch that haven't been announced.\n[2653s->2673s]And I think the important concept here, which makes it easier to understand, is that like other technologies that impact literally every single industry, of course, electricity was one, and it became infrastructure.\n[2673s->2683s]Of course, the information infrastructure, which we now know as the Internet, affects every single industry, every country, every society.\n[2683s->2686s]Intelligence is surely one of those things.\n[2686s->2693s]I don't know any company, industry, country who thinks that intelligence is optional.\n[2693s->2694s]It's essential infrastructure.\n[2694s->2701s]And so we've now digitalized intelligence.\n[2701s->2708s]And so I think we're clearly in the beginning of the build out of this infrastructure.\n[2708s->2712s]And every country will have it.\n[2712s->2713s]I'm certain of that.\n[2713s->2714s]Every industry will use it.\n[2714s->2716s]That I'm certain of.\n[2716s->2722s]And what's unique about this infrastructure is that it needs factories.\n[2722s->2728s]You know, it's a little bit like the energy infrastructure, electricity.\n[2728s->2730s]It needs factories.\n[2730s->2733s]We need factories to produce this intelligence.\n[2733s->2736s]And the intelligence is getting more sophisticated.\n[2736s->2743s]We were talking about earlier that we had a huge breakthrough in the last couple of years with reasoning AI.\n[2743s->2748s]And now there are agents that reason and there are super agents that use a whole bunch of tools.\n[2748s->2754s]And then there's clusters of super agents where agents are working with agents, solving problems.\n[2754s->2769s]And so you could just imagine compared to one shot chat bots and the agents that are now using AI built on these large language models, how much more compute intensive they really need to be and are.\n[2769s->2774s]And so I think we're in the beginning of the build out.\n[2774s->2782s]And there should be many, many more announcements in the future.\n[2782s->2783s]Your next question.\n[2783s->2784s]Excuse me.\n[2784s->2788s]Your next question comes from Ben Reitz's with Melius.\n[2788s->2790s]Your line is open.\n[2790s->2791s]Yeah, hi.\n[2791s->2793s]Thanks for the question.\n[2793s->2801s]I wanted to ask first to Collette just a little clarification around the guidance and maybe putting it in a different way.\n[2801s->2813s]The 8 billion for H20 just seems like it's roughly 3 billion more than most people thought with regard to what you'd be foregoing in the second quarter.\n[2813s->2823s]So that would mean that with regard to your guidance, the rest of the business in order to hit 45 is doing 2 to 3 billion or so better.\n[2824s->2828s]So I was wondering if that math made sense to you.\n[2828s->2837s]And then in terms of the guidance, that would imply the non-China business is doing a bit better than the street expected.\n[2837s->2843s]So wondering what the primary driver was there in your view.\n[2843s->2851s]And then this second part of my question, Jensen, I know you guide one quarter at a time.\n[2851s->2871s]But with regard to the AI diffusion rule being lifted and this momentum was sovereign, there's been times in your history where you guys have said on calls like this where you have more conviction and sequential growth throughout the year, et cetera.\n[2871s->2886s]And given the unleashing of demand with AI diffusion being revoked and the supply chain increasing, does the environment give you more conviction and sequential growth as we go throughout the year?\n[2886s->2891s]So first one for Collette and then next one for Jensen. Thanks so much.\n[2891s->2908s]Thanks, Ben, for the question. When we look at our Q2 guidance and our commentary that we provided that had the export controls not occurred, we would have had orders of about 8 billion for H20.\n[2908s->2918s]That's correct. That was a possibility for what we would have had in our outlook for this quarter in Q2.\n[2918s->2932s]So what we also have talked about here is the growth that we've seen in Blackwell, Blackwell across many of our customers, as well as the growth that we continue to have in terms of supply that we need for our customers.\n[2932s->2938s]So putting those together, that's where we came through with the guidance that we provided.\n[2938s->2942s]I'm going to turn the rest over to Jensen to see how he wants to.\n[2942s->2945s]Yeah, thanks. Thanks, Ben.\n[2945s->2957s]I would say compared to the beginning of the year, compared to GTC timeframe, there are four positive surprises.\n[2957s->2967s]The first positive surprise is the step function demand increase of reasoning AI.\n[2967s->2977s]I think it is fairly clear now that AI is going through an exponential growth, and reasoning AI really busts it through.\n[2977s->2986s]Concerns about hallucination or its ability to really solve problems.\n[2986s->2999s]I think a lot of people are crossing that barrier and realizing how incredibly effective agentic AI is and reasoning AI is.\n[2999s->3007s]So number one is inference reasoning and the exponential growth there, demand growth.\n[3007s->3012s]The second one, you mentioned AI diffusion.\n[3012s->3019s]It's really terrific to see that the AI diffusion rule was rescinded.\n[3019s->3032s]President Trump wants America to win, and he also realizes that we're not the only country in the race.\n[3032s->3042s]He wants the United States to win and recognizes that we have to get the American stack out to the world\n[3042s->3047s]and have the world build on top of American stacks instead of alternatives.\n[3047s->3052s]And so AI diffusion happened.\n[3052s->3062s]The rescinding of it happened at almost precisely the time that the countries around the world are awakening the importance of AI as an infrastructure,\n[3062s->3072s]not just as a technology of great curiosity and great importance, but infrastructure for their industries and startups and society.\n[3072s->3078s]Just as they had to build out infrastructure for electricity and Internet, you got to build out an infrastructure for AI.\n[3078s->3083s]I think that that's an awakening and that creates a lot of opportunity.\n[3083s->3085s]The third is enterprise AI.\n[3085s->3093s]Agents work and agents are doing these agents are really quite successful.\n[3093s->3098s]Much more than generative AI, agentic AI is game changing.\n[3098s->3113s]Agents can understand ambiguous and rather implicit instructions and able to problem solve and use tools and have memory and so on.\n[3113s->3118s]And so I think enterprise AI is ready to take off.\n[3118s->3133s]And it's taken us a few years to build a computing system that is able to integrate, run enterprise AI stacks, run enterprise IT stacks, but add AI to it.\n[3133s->3142s]And this is the RTX Pro Enterprise server that we announced at Computex just last week.\n[3142s->3149s]And just about every major IT company has joined us super excited about that.\n[3149s->3152s]And so computing is one stat, one part of it.\n[3152s->3157s]But remember, enterprise IT is really three pillars.\n[3157s->3161s]It's compute, storage and networking.\n[3161s->3164s]And we've now put all three of them together for finally.\n[3164s->3167s]And we're going to market with that.\n[3167s->3170s]And then lastly, industrial AI.\n[3170s->3186s]Remember, one of the implications of the world reordering, if you will, is regions onshore manufacturing and building plants everywhere.\n[3186s->3195s]In addition to AI factories, of course, there are new electronics manufacturing chip manufacturing being built around the world.\n[3195s->3210s]And all of these new plants and these new factories are creating exactly the right time when Omniverse and AI and all the work that we're doing with robotics is emerging.\n[3210s->3215s]And so this fourth pillar is quite important.\n[3215s->3218s]Every factory will have an AI factory associated with it.\n[3218s->3226s]And in order to create these physical AI systems, you really have to train a vast amount of data.\n[3226s->3232s]So back to more data, more training, more AIs to be created, more computers.\n[3232s->3243s]And so these four drivers are really kicking into turbocharge.\n[3243s->3248s]Your next question comes from Timothy R. Curie with UBS.\n[3248s->3250s]Your line is open.\n[3250s->3251s]Thanks a lot.\n[3251s->3253s]Jensen, I wanted to ask about China.\n[3253s->3257s]It sounds like the July guidance assumes there's no SKU replacement for the H-20.\n[3257s->3262s]But if the president wants the U.S. to win, it seems like you're going to have to be allowed to ship something into China.\n[3262s->3265s]So I guess I had two points on that.\n[3265s->3269s]First of all, have you been approved to ship a new modified version into China?\n[3269s->3272s]And you're currently building it, but you just can't ship it in fiscal Q2.\n[3272s->3277s]And then you were sort of run rating $7 to $8 billion a quarter into China.\n[3277s->3283s]Can we get back to those sorts of quarterly run rates once you get something that you're allowed to ship back into China?\n[3283s->3287s]I think we're all trying to figure out how much to add back to our models and when.\n[3287s->3289s]So whatever you can say there would be great.\n[3289s->3291s]Thanks.\n[3291s->3293s]The president has a plan.\n[3293s->3296s]He has a vision and I trust him.\n[3296s->3305s]With respect to our export controls, it's a set of limits.\n[3305s->3322s]And the new set of limits pretty much make it impossible for us to reduce Hopper any further for any productive use.\n[3322s->3330s]And so the new limits, it's kind of the end of the road for Hopper.\n[3330s->3333s]We have limited options.\n[3333s->3337s]And so the key is to understand the limits.\n[3337s->3346s]The key is to understand the limits and see if we can come up with interesting products that could continue to serve the Chinese market.\n[3346s->3350s]We don't have anything at the moment.\n[3350s->3352s]But we're considering it.\n[3352s->3354s]We're thinking about it.\n[3354s->3358s]Obviously the limits are quite stringent at the moment.\n[3358s->3363s]And we have nothing to announce today.\n[3363s->3373s]And when the time comes, you know, we'll engage the administration and discuss that.\n[3373s->3378s]Your final question comes from the line of Aaron Rakers with Wells Fargo.\n[3378s->3381s]Your line is open.\n[3381s->3382s]Hi.\n[3382s->3383s]This is Jake on for Aaron.\n[3383s->3386s]Thanks for taking the question and congrats on the great quarter.\n[3386s->3399s]I was wondering if you could give some additional color around the strength you saw within the networking business, particularly around the adoption of your Ethernet solutions at CSPs as well as your\n[3400s->3404s]any change you're seeing in network attach rates.\n[3404s->3407s]Yeah, thank you for that.\n[3407s->3415s]We now have three networking platforms, maybe four.\n[3415s->3425s]The first one is the scale up platform to turn a computer into a much larger computer.\n[3425s->3427s]Scaling up is incredibly hard to do.\n[3427s->3429s]Scaling out is easier to do.\n[3429s->3431s]But scaling up is hard to do.\n[3431s->3433s]And that platform is called NVLink.\n[3433s->3446s]And NVLink comes with it, chips and switches and, you know, NVLink spines.\n[3446s->3448s]It's really complicated.\n[3448s->3452s]But anyways, that's our new platform, scale up platform.\n[3452s->3455s]In addition to InfiniBand, we also have SpectrumX.\n[3455s->3466s]We've been fairly, fairly consistent that Ethernet was designed for a lot of traffic that are independent.\n[3466s->3471s]But in the case of AI, you have a lot of computers working together.\n[3471s->3477s]And the traffic of AI is insanely bursty.\n[3477s->3479s]Latency matters a lot.\n[3479s->3484s]Because the AI is thinking and it wants to get work done as quickly as possible.\n[3484s->3487s]And you got a whole bunch of nodes working together.\n[3487s->3499s]And so we enhanced Ethernet, added capabilities like extremely low latency, congestion control, adaptive routing,\n[3499s->3506s]the type of technologies that were available only in InfiniBand to Ethernet.\n[3506s->3511s]And as a result, we improved the utilization of Ethernet in these clusters.\n[3511s->3519s]These clusters are gigantic from as low as 50% to as high as 85%, 90%.\n[3519s->3531s]And so the difference is if you had a cluster that's $10 billion and you improved its effectiveness by 40%, that's worth $4 billion.\n[3532s->3534s]It's incredible.\n[3534s->3538s]And so SpectrumX has been really, quite frankly, a home run.\n[3538s->3553s]And this last quarter, as we said in the prepared remarks, we added two very significant CSPs to the SpectrumX adoption.\n[3553s->3558s]And then the last one, the last one is Bluefield, which is our control plane.\n[3558s->3566s]And so in those four, the control plane and network, which is used for storage, is used for security,\n[3566s->3576s]and for many of these clusters that want to achieve isolation among its users, multi-tenant clusters,\n[3576s->3583s]and still be able to use and have extremely high performance, bare metal performance,\n[3584s->3591s]Bluefield is ideal for that and is used in a lot of these cases.\n[3591s->3599s]And so we have these four networking platforms that are all growing and we're doing really well.\n[3599s->3601s]I'm very proud of the team.\n[3603s->3607s]That is all the time we have for questions.\n[3607s->3610s]Denton, I will turn the call back to you.\n[3610s->3611s]Thank you.\n[3611s->3612s]Thank you.\n[3612s->3615s]This is the start of a powerful new wave of growth.\n[3615s->3617s]Grace Blackwell is in full production.\n[3617s->3619s]We're off to the races.\n[3619s->3621s]We now have multiple significant growth engines.\n[3621s->3627s]Inference, one's delighted workload is surging with revenue generating AI services.\n[3627s->3635s]AI is growing faster and will be larger than any platform shifts before, including the internet, mobile, and cloud.\n[3635s->3645s]Blackwell is built to power the full AI lifecycle from training frontier models to running complex inference and reasoning agents at scale.\n[3645s->3655s]Training demands continues to rise with breakthroughs in post-training and like reinforcement learning and synthetic data generation.\n[3655s->3657s]But inference is exploding.\n[3657s->3661s]Reasoning AI agents require orders of magnitude more compute.\n[3661s->3667s]The foundations of our next growth platforms are in place and ready to scale.\n[3667s->3674s]Sovereign AI nations are investing in AI infrastructure like they once did for electricity and internet.\n[3674s->3681s]Enterprise AI, AI must be deployable on prem and integrated with existing IT.\n[3681s->3692s]Our RTX Pro, DGX Spark, and DGX Station Enterprise AI systems are ready to modernize the $500 billion IT infrastructure on prem or in the cloud.\n[3692s->3696s]Every major IT provider is partnering with us.\n[3696s->3710s]Industrial AI from training to digital twin simulation to deployment, NVIDIA Omniverse and Isaac Groot are powering next generation factories and humanoid robotic systems worldwide.\n[3710s->3723s]The age of AI is here from AI infrastructures, inference at scale, sovereign AI, enterprise AI, and industrial AI.\n[3723s->3725s]NVIDIA is ready.\n[3725s->3740s]Join us at GTC Paris, our keynote at VivaTech on June 11, talking about quantum GPU computing, robotic factories and robots, and celebrate our partnerships building AI factories across the region.\n[3740s->3745s]The NVIDIA Band will tour France, the UK, Germany, and Belgium.\n[3745s->3753s]Thank you for joining us at the earnings call today. See you in Paris.\n[3753s->3758s]This concludes today's conference call. You may now disconnect.\n",
            "On April 9, 2025, we were informed by the USG that a license is required for exports of our H20 products into the China market. As a result of these new\nrequirements, we incurred a $4.5 billion charge in the first quarter of fiscal year 2026 associated with H20 excess inventory and purchase obligations as the\ndemand for H20 products diminished. The $4.5 billion charge was less than what we initially anticipated as we were able to re-use certain materials. Sales of our\nH20 products were $4.6 billion for the first quarter of fiscal year 2026 prior to the new export licensing requirements. The H20 export licensing requirements have\nimpacted our current revenue and will also negatively affect our future revenue.\n\nRevenue was $44.1 billion, up 69% from a year ago and up 12% sequentially.\n\nData Center revenue was $39.1 billion, up 73% from a year ago and up 10% sequentially. The strong year-on-year and sequential growth was driven by demand\nfor our accelerated computing platform used for large language models, recommendation engines, and generative and agentic AI applications. We saw our\nBlackwell architecture ramp expand to all customer categories, while large cloud service providers remained our largest at just under 50% of Data Center\nrevenue. Data Center compute revenue was $34.2 billion, up 76% from a year ago and up 5% sequentially. Networking revenue was $5.0 billion, up 56% from a\nyear ago and up 64% sequentially, driven by the growth of NVLink compute fabric in our GB200 systems and continued adoption of Ethernet for AI solutions at\ncloud service providers and consumer internet companies.\n\nGaming revenue was up 42% from a year ago and up 48% sequentially, driven by sales of our Blackwell architecture.\n\nProfessional Visualization revenue was up 19% from a year ago and flat sequentially. The increase from a year ago was driven by broader adoption of Ada RTX\nworkstation GPUs, addressing workflows in AI acceleration, real-time graphics rendering and data simulation.\n\nAutomotive revenue was up 72% from a year ago and down 1% sequentially. The increase from a year ago was driven by sales of our self-driving platforms.\n\nGross margin decreased from a year ago and sequentially, primarily due to a $4.5 billion charge associated with H20 excess inventory and purchase obligations\nand the initial ramp of more sophisticated systems within Data Center.\n\nOperating expenses were up 44% from a year ago and up 7% sequentially. The year-on-year increase was primarily driven by higher compensation and benefits\nexpenses due to employee growth and compensation increases, and compute, infrastructure and engineering development costs for new product introductions.\nThe sequential increase was primarily driven by higher compensation and benefits due to compensation increases and employee growth.\n\nFinancial Information by Business Segment and Geographic Data\n\nRefer to Note 13 of the Notes to the Condensed Consolidated Financial Statements for disclosure regarding segment information.\n\nCritical Accounting Policies and Estimates\n\nRefer to Part II, Item 7, \"Critical Accounting Policies and Estimates\" of our Annual Report on Form 10-K for the fiscal year ended January 26, 2025. There have\nbeen no material changes to our Critical Accounting Policies and Estimates.\n\n24",
            "Reportable segment operating income – The year over year increase in Compute & Networking segment operating income in the first quarter of fiscal year 2026\nwas driven by growth in revenue, partially offset by a $4.5 billion charge associated with H20 excess inventory and purchase obligations. The year over year\nincrease in Graphics segment operating income in the first quarter of fiscal year 2026 was driven by growth in revenue.\n\nAll Other operating loss – The year over year increase in the first quarter of fiscal year 2026 was due to an increase in stock-based compensation expense\nreflecting employee growth.\n\nConcentration of Revenue\n\nWe refer to customers who purchase products directly from NVIDIA as direct customers, such as add-in board manufacturers, distributors, ODMs, OEMs, and\nsystem integrators. We have certain customers that may purchase products directly from NVIDIA and may use either internal resources or third-party system\nintegrators to complete their build. We also have indirect customers, who purchase products through our direct customers; indirect customers include CSPs,\nconsumer internet companies, enterprises, and public sector entities.\n\nDirect Customers – Sales to one direct customer, Customer A, represented 16% of total revenue and sales to a second direct customer, Customer B,\nrepresented 14% of total revenue for the first quarter of fiscal year 2026, both of which were attributable to the Compute & Networking segment. Sales to two\ndirect customers represented 11% and 13% of total revenue for the first quarter of fiscal year 2025, both of which were attributable to the Compute & Networking\nsegment.\n\nIndirect Customers – Indirect customer revenue is an estimation based upon multiple factors including customer purchase order information, product\nspecifications, internal sales data, and other sources. Actual indirect customer revenue may differ from our estimates. For the first quarter of fiscal year 2026, two\nindirect customers which primarily purchase our products through system integrators and distributors, including through Direct Customers A and B, are estimated\nto represent 10% or more of total revenue, attributable to the Compute & Networking segment.\n\nWe estimate that in the first quarter of fiscal year 2026, an AI research and deployment company contributed to a meaningful amount of our revenue, through\none of the above indirect customers and through other indirect customers that provide cloud services.\n\nWe have experienced periods where we receive a significant amount of our revenue from a limited number of customers, and this trend may continue.\n\nRevenue by geographic region is designated based on the billing location even if the revenue may be attributable to indirect customers in a different location.\nRevenue from sales to customers outside of the United States accounted for 53% and 48% of total revenue for the first quarter of fiscal years 2026 and 2025,\nrespectively.\n\nGross Profit and Gross Margin\n\nGross profit consists of total net revenue less cost of revenue. Cost of revenue consists primarily of the cost of semiconductors, including wafer fabrication,\nassembly, testing and packaging, board and device costs, manufacturing support costs, including labor and overhead associated with such purchases, final test\nyield fallout, inventory and warranty provisions, memory and component costs, tariffs, and shipping costs. Cost of revenue also includes acquisition-related\nintangible amortization expense, costs for license and development and service arrangements, IP-related costs, and stock-based compensation related to\npersonnel associated with manufacturing operations.\n\nGross margins decreased to 60.5% for the first quarter of fiscal year 2026 compared to 78.4% for the first quarter of fiscal year 2025, primarily due to a $4.5\nbillion charge associated with H20 excess inventory and purchase obligations and the initial ramp of more sophisticated systems within Data Center.\n\nProvisions for inventory and excess inventory purchase obligations totaled $5.3 billion, including $4.5 billion associated with H20 excess inventory and purchase\nobligations, and $393 million for the first quarter of fiscal years 2026 and 2025, respectively. Sales of previously reserved inventory and settlements of excess\ninventory purchase obligations resulted in a provision release of $436 million and $114 million for the first quarter of fiscal years 2026 and 2025, respectively.\nThe net effect of provisions for inventory and excess inventory purchase obligations on our gross margin was an unfavorable impact of 11.0% and 1.1% in the\nfirst quarter of fiscal years 2026 and 2025, respectively.\n\n26"
        ]
    },
    {
        "question": "公司对 2026 财年第二季度的收入展望（Outlook）是多少？",
        "ground_truth": "公司对 2026 财年第二季度的收入展望是 450 亿美元，正负 2%。",
        "answer": "根据提供的财务报告，英伟达对2026财年第二季度的收入展望为45亿美元，上下浮动2%。公司预计在所有平台上实现适度增长，其中Blackwell的持续推广将部分抵消中国收入下降的影响。展望中反映了H20收入约8亿美元的损失 [来源: Q1_2026_presentation, 第 9 页]。",
        "contexts": [
            "[0s->5s]Good afternoon. My name is Sarah and I will be your conference operator today.\n[5s->13s]At this time, I would like to welcome everyone to NVIDIA's first quarter fiscal 2026 financial results conference call.\n[13s->17s]All lines have been placed on mute to prevent any background noise.\n[17s->21s]After the speaker's remarks, there will be a question and answer session.\n[21s->27s]If you would like to ask a question during this time, simply press star one on your telephone keypad.\n[27s->32s]If you would like to withdraw your question, please press star one again. Thank you.\n[32s->36s]Toshia Hari, you may begin your conference.\n[36s->44s]Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the first quarter of fiscal 2026.\n[44s->54s]With me today from NVIDIA are Jensen Wong, President and Chief Executive Officer, and Collette Press, Executive Vice President and Chief Financial Officer.\n[54s->60s]I'd like to remind you that our call is being webcast live on NVIDIA's investor relations website.\n[60s->68s]The webcast will be available for replay until the conference call to discuss our financial results for the second quarter of fiscal 2026.\n[68s->76s]The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent.\n[76s->81s]During this call, we may make forward looking statements based on current expectations.\n[81s->88s]These are subject to a number of significant risks and uncertainties, and our actual results may differ materially.\n[88s->96s]For a discussion of factors that could affect our future financial results in business, please refer to the disclosure in today's earnings release,\n[96s->104s]our most recent forms 10-K and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission.\n[105s->112s]All our statements are made as of today, May 28, 2025, based on information currently available to us.\n[112s->118s]Except as required by law, we assume no obligation to update any such statements.\n[118s->122s]During this call, we will discuss non-GAAP financial measures.\n[122s->130s]You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website.\n[130s->133s]With that, let me turn the call over to Collette.\n[133s->135s]Thank you, Toshio.\n[135s->148s]We delivered another strong quarter with revenue of $44 billion, up 69% year-over-year, exceeding our outlook in what proved to be a challenging operating environment.\n[148s->153s]Data center revenue of $39 billion grew 73% year-on-year.\n[153s->161s]AI workloads have transitioned strongly to inference, and AI factory build-outs are driving significant revenue.\n[161s->165s]Our customers' commitments are firm.\n[165s->176s]On April 9, the U.S. government issued new export controls on H20, our data center GPU designed specifically for the China market.\n[176s->181s]We sold H20 with the approval of the previous administration.\n[181s->187s]Although our H20 has been in the market for over a year and does not have a market outside of China,\n[187s->195s]the new export controls on H20 did not provide a grace period to allow us to sell through our inventory.\n[195s->204s]In Q1, we recognized $4.6 billion in H20 revenue, which occurred prior to April 9,\n[204s->216s]but also recognized a $4.5 billion charge as we wrote down inventory and purchase obligations tied to orders we had received prior to April 9.\n[216s->226s]We were unable to ship $2.5 billion in H20 revenue in the first quarter due to the new export controls.\n[226s->234s]The $4.5 billion charge was less than what we initially anticipated as we were able to reuse certain materials.\n[234s->244s]We are still evaluating our limited options to supply data center compute products compliant with the U.S. government's revised export control rules,\n[244s->252s]losing access to the China AI accelerator market, which we believe will grow to nearly $50 billion,\n[252s->262s]would have a material adverse impact on our business going forward and benefit our foreign competitors in China and worldwide.\n[262s->271s]Our Blackwell ramp, the fastest in our company's history, drove a 73% year-on-year increase in data center revenue.\n[271s->280s]Blackwell contributed nearly 70% of data center compute revenue in the quarter, with a transition from Hopper nearly complete.\n[280s->293s]The introduction of GB200 NVL was a fundamental architectural change to enable data center scale workloads and to achieve the lowest cost per inference token.\n[293s->305s]While these systems are complex to build, we have seen a significant improvement in manufacturing yields and rack shipments are moving to strong rates to end customers.\n[305s->315s]GB200 NVL racks are now generally available for modern builders, enterprises, and sovereign customers to develop and deploy AI.\n[315s->332s]On average, major hyperscalers are each deploying nearly 1,000 NVL 72 racks or 72,000 Blackwell GPUs per week and are on track to further ramp output this quarter.\n[332s->346s]Microsoft, for example, has already deployed tens of thousands of Blackwell GPUs and is expected to ramp to hundreds of thousands of GB200s with OpenAI as one of its key customers.\n[346s->356s]Key learnings from the GB200 ramp will allow for a smooth transition to the next phase of our product roadmap, Blackwell Ultra.\n[357s->370s]Sampling of GB300 systems began earlier this month at the major CSPs, and we expect production shipments to commerce later this quarter.\n[370s->381s]GB300 will leverage the same architecture, same physical footprint, and the same electrical and mechanical specifications as GB200.\n[381s->394s]The GB300 drop-in design will allow CSPs to seamlessly transition their systems and manufacturing used for GB200 while maintaining high yields.\n[394s->407s]B300 GPUs with 50% more HBM will deliver another 50% increase in dense FP4 inference compute performance compared to the B200.\n[407s->421s]We remain committed to our annual product cadence, with our roadmap extending through 2028 tightly aligned with the multiple-year planning cycles of our customers.\n[421s->432s]We are witnessing a sharp jump in inference demand. OpenAI, Microsoft, and Google are seeing a step-function leap in token generation.\n[432s->441s]Microsoft processed over 100 trillion tokens in Q1, a five-fold increase on a year-over-year basis.\n[441s->453s]This exponential growth in Azure OpenAI is representative of strong demand for Azure AI Foundry as well as other AI services across Microsoft's platform.\n[453s->471s]Inference-serving startups are now serving models using B200, tripling their token generation rate and corresponding revenues for high-value reasoning models such as DeepSeq R1, as reported by Artificial Analysis.\n[471s->485s]NVIDIA Dynamo on Blackwell NVL72 turbocharges AI inference throughput by 30x for the new reasoning models sweeping the industry.\n[485s->497s]Developer engagements increased with adoption ranging from LLM providers such as Perplexity to financial services institutions such as Capital One,\n[497s->503s]who reduced agentic chatbox latency by 5x with Dynamo.\n[503s->511s]In the latest MLPerf inference results, we submitted our first results using GB200 NVL72,\n[511s->523s]delivering up to 30x higher inference throughput compared to our 8 GPU H200 submission on the challenging Llama 3.1 benchmark.\n[523s->535s]This feat was achieved through a combination of tripling the performance per GPU as well as 9x more GPUs, all connected on a single NVLink domain.\n[535s->547s]And while Blackwell is still early in its lifecycle, software optimizations have already improved its performance by 1.5x in the last month alone.\n[547s->555s]We expect to continue improving the performance of Blackwell through its operational life as we have done with Hopper and Amper.\n[555s->560s]For example, we increased the inference performance of Hopper by 4x over two years.\n[560s->568s]This is the benefit of NVIDIA's programmable CUDA architecture and rich ecosystem.\n[568s->578s]The pace and scale of AI factory deployments are accelerating with nearly 100 NVIDIA-powered AI factories in flight this quarter,\n[578s->588s]a two-fold increase year over year, with the average number of GPUs powering each factory also doubling in the same period.\n[588s->593s]And more AI factory projects are starting across industries and geographies.\n[593s->605s]NVIDIA's full-stack architecture is underpinning AI factory deployments as industry leaders like AT&T, BYD, Capital One, Foxconn, MediaTek, and Telenor\n[605s->614s]are strategically vital sovereign clouds like those recently announced in Saudi Arabia, Taiwan, and the UAE.\n[614s->623s]We have a line of sight to projects requiring tens of gigawatts of NVIDIA AI infrastructure in the not-too-distant future.\n[623s->639s]The transition from generative to agentic AI, AI capable of perceiving, reasoning, planning, and acting, will transform every industry, every company, and country.\n[639s->650s]We envision AI agents as a new digital workforce capable of handling tasks ranging from customer service to complex decision-making processes.\n[650s->660s]We introduced the LAMA Nemotron family of open reasoning models designed to supercharge identical AI platforms for enterprises.\n[660s->672s]Built on the LAMA architecture, these models are available as NIMS or NVIDIA inference microservices with multiple sizes to meet diverse deployment needs.\n[672s->681s]Our post-training enhancements have yielded a 20% accuracy boost and a 5x increase in inference speed.\n[681s->690s]Leading platform companies including Accenture, Cadence, Deloitte, and Microsoft are transforming work with our reasoning models.\n[690s->702s]NVIDIA NEMO microservices are generally available across industries that are being leveraged by leading enterprises to build, optimize, and scale AI applications.\n[702s->711s]With NEMO, Cisco increased model accuracy by 40% and improved response time by 10x in its code assistant.\n[711s->719s]NASDAQ realized a 30% improvement in accuracy and response time in its AI platform's search capabilities.\n[719s->727s]And Shell's Custom LLM achieved a 30% increase in accuracy when trained with NVIDIA NEMO.\n[727s->735s]NEMO's parallelism techniques accelerated model training time by 20% when compared to other frameworks.\n[735s->757s]We also announced a partnership with Yum Brands, the world's largest restaurant company, to bring NVIDIA AI to 500 of its restaurants this year and expanding to 61,000 restaurants over time to streamline order taking, optimize operations, and enhance service across its restaurants.\n[757s->779s]For AI-powered cybersecurity, leading companies like Checkpoint, CloudStrike, and Palo Alto Networks are using NVIDIA's AI security and software stack to build, optimize, and secure agentic workflows, with CloudStrike realizing 2x faster detection triage with 50% less compute cost.\n[780s->790s]Moving to networking, sequential growth in networking resumed in Q1, with revenue up 64% quarter-over-quarter to $5 billion.\n[790s->798s]Our customers continue to leverage our platform to efficiently scale up and scale out AI factory workloads.\n[798s->813s]We created the world's fastest switch, NVLink. For scale up, our NVLink compute fabric in its fifth generation offers 14x the bandwidth of PCIe Gen 5.\n[813s->825s]NVLink 72 carries 130 TB per second of bandwidth in a single rack, equivalent to the entirety of the world's peak internet traffic.\n[825s->833s]NVLink is a new growth vector and is off to a great start, with Q1 shipments exceeding $1 billion.\n[833s->848s]At Computex, we announced NVLink Fusion. Hyperscale customers can now build semi-custom CCUs and accelerators that connect directly to the NVIDIA platform with NVLink.\n[848s->868s]We are now enabling key partners, including ASIC providers such as MediaTek, Marvell, Alchip Technologies, and Astera Labs, as well as CPU suppliers such as Fujitsu and Qualcomm, to leverage NVLink Fusion to connect our respective ecosystems.\n[868s->878s]For scale out, our enhanced Ethernet offerings deliver the highest throughput, lowest latency networking for AI.\n[878s->886s]SpectrumX posted strong sequential and year-on-year growth and is now annualizing over $8 billion in revenue.\n[886s->896s]Adoption is widespread across major CSPs and consumer internet companies, including CoreWe, Microsoft Azure, Oracle Cloud, and XAI.\n[896s->903s]This quarter, we added Google Cloud and Meta to the growing list of SpectrumX customers.\n[903s->913s]We introduced SpectrumX and QuantumX, silicon photonics switches featuring the world's most advanced co-packaged optics.\n[913s->933s]These platforms will enable next level AI factory scaling to millions of GPUs through the increasingly power efficiency by 3.5x and network resiliency by 10x while accelerating customer time to market by 1.3x.\n[933s->938s]Transitioning to a quick summary of our revenue by geography.\n[938s->949s]China, as a percentage of our data center revenue, was slightly below our expectations and down sequentially due to H20 export licensing controls.\n[949s->955s]For Q2, we expect a meaningful decrease in China data center revenue.\n[955s->971s]As a reminder, while Singapore represented nearly 20% of our Q1 billed revenue, as many of our large customers use Singapore for centralized invoicing, our products are almost always sold elsewhere.\n[971s->984s]Note that over 99% of H100, H200, and Blackwell data center compute revenue billed to Singapore was for orders from US based customers.\n[984s->996s]Moving to gaming and AI PCs. Gaming revenue was a record 3.8 billion, increasing 48% sequentially and 42% year on year.\n[996s->1012s]Strong adoption by gamers, creatives, and AI enthusiasts have made Blackwell our fastest ramp ever. Against a backdrop of robust demand, we greatly improved our supply and availability in Q1 and expect to continue these efforts in Q2.\n[1012s->1017s]AI is transforming PC and creator and gamers.\n[1017s->1035s]With a 100 million user installed base, GeForce represents the largest footprint for PC developers. This quarter, we added to our AI PC laptop offerings, including models capable of running Microsoft's Copilot Plus.\n[1035s->1049s]This past quarter, we brought Blackwell architecture to mainstream gaming with its launch of GeForce RTX 5060 and 5060 Ti, starting at just $299.\n[1049s->1061s]The RTX 5060 also debuted in laptops starting at $1,099. These systems that double the frame rate and slash latency.\n[1061s->1070s]These GeForce RTX 5060 and 5060 Ti desktop GPUs and laptops are now available.\n[1070s->1090s]In console gaming, the recently unveiled Nintendo Switch 2 leverages Nvidia's neural rendering and AI technologies, including next generation custom RTX GPUs with DLSS technology, to deliver a giant leap in gaming performance to millions of players worldwide.\n[1090s->1100s]Nintendo has shipped over 150 million Switch consoles to date, making it one of the most successful gaming systems in history.\n[1100s->1108s]Moving to pro visualization. Revenue of $509 million was flat sequentially and up 19% year on year.\n[1108s->1120s]Tariff related uncertainty temporarily impacted Q1 systems and demand for AI workstations is strong and we expect sequential revenue growth to resume in Q2.\n[1120s->1130s]Nvidia DGX Spark and Station revolutionize personal computing by putting the power of an AI supercomputer in a desktop form factor.\n[1130s->1143s]DGX Spark delivers up to one petaflop of AI compute while DGX Station offers an incredible 20 petaflops and is powered by the GB300 Superchip.\n[1143s->1150s]DGX Spark will be available in Calendar Q3 and DGX Station later this year.\n[1150s->1163s]We have deepened Omniverse's integration and adoption into some of the world's leading software platforms, including Databricks, SAP, and Schneider Electric.\n[1163s->1178s]New Omniverse blueprints such as Mega for at scale robotic fleet management are being leveraged in Keyon Group, Pegatron, Accenture, and other leading companies to enhance industrial operations.\n[1178s->1189s]At Confitex, we showcased Omniverse's great traction with technology manufacturing leaders, including TSMC, Quanta, Foxconn, Pegatron.\n[1189s->1194s]Using Omniverse, TSMC saves months in work by designing fabs virtually.\n[1194s->1205s]Foxconn accelerates thermal simulations by 150x and Pegatron reduced assembly line defects rates by 67%.\n[1205s->1214s]Lastly, with our automotive group, revenue was $567 million, down 1% sequentially but up 72% year on year.\n[1214s->1222s]Year on year growth was driven by the ramp of self-driving across a number of customers and robust end demand for NEVs.\n[1222s->1232s]We are partnering with GM to build the next-gen vehicles, factories, and robots using NVIDIA AI, simulation, and accelerated computing.\n[1232s->1241s]And we are now in production with our full-stack solution for Mercedes-Benz, starting with the new CLA, hitting roads in the next few months.\n[1241s->1254s]We announced Isaac, Groot, and won the world's first open fully customizable foundation model for humanoid robots, enabling generalized reasoning and skill development.\n[1254s->1260s]We also launched new open NVIDIA Cosmo World Foundation models.\n[1260s->1268s]Leading companies include 1X, Agility Robots, Robotics, Figure AI, Uber, and Wabi.\n[1268s->1284s]We've begun integrating Cosmos into their operations for synthetic data generation, while Agility Robotics, Boston Dynamics, and XPen Robotics are harnessing Isaac simulation to advance their humanoid efforts.\n[1284s->1300s]GE Healthcare is using the new NVIDIA Isaac platform for healthcare simulation, built on NVIDIA Omniverse and using NVIDIA Cosmos, the platform speeds development of robotic imaging and surgery systems.\n[1300s->1314s]The era of robotics is here. Billions of robots, hundreds of millions of autonomous vehicles, and hundreds of thousands of robotic factories and warehouses will be developed.\n[1314s->1335s]All right, moving to the rest of the P&L. Gap gross margins and non-gap gross margins were 60.5% and 61%, respectively. Excluding the $4.5 billion charge, Q1 non-gap gross margins would have been 71.3%, slightly above our outlook at the beginning of the quarter.\n[1335s->1346s]Sequentially, gap operating expenses were up 7% and non-gap operating expenses were up 6%, reflecting higher compensation and employee growth.\n[1346s->1355s]Our investments include expanding our infrastructure capabilities and AI solutions, and we plan to grow these investments throughout the fiscal year.\n[1355s->1370s]In Q1, we returned a record $14.3 billion to shareholders in the form of share repurchases and cash dividends. Our capital return program continues to be a key element of our capital allocation strategy.\n[1370s->1384s]Let me turn to the outlook for the second quarter. Total revenue is expected to be $45 billion, plus or minus 2%. We expect modest sequential growth across all of our platforms.\n[1384s->1403s]In data center, we anticipate the continued ramp of Blackwell to be partially offset by a decline in China revenue. Note, our outlook reflects a loss in H20 revenue of approximately $8 billion for the second quarter.\n[1403s->1420s]Gap and non-gap gross margins are expected to be 71.8% and 72% respectively, plus or minus 50 basis points. We expect better Blackwell profitability to drive modest sequential improvement in gross margins.\n[1420s->1427s]We are continuing to work towards achieving gross margins in the mid-70s range late this year.\n[1427s->1444s]Gap and non-gap operating expenses are expected to be approximately $5.7 billion and $4 billion respectively, and we continue to expect full-year fiscal year 26 operating expense growth to be in the mid-30% range.\n[1444s->1457s]Gap and non-gap other income and expenses are expected to be an income of approximately $450 million, excluding gays and losses from non-marketable and publicly held equity securities.\n[1457s->1468s]Gap and non-gap tax rates are expected to be 16.5%, plus or minus 1%, excluding any discrete items.\n[1468s->1480s]Further financial details are included in the CFO commentary and other information available on our IR website, including a new financially information AI agent.\n[1480s->1488s]Let me highlight upcoming events for the financial community. We will be at the B of A Global Technology Conference in San Francisco on June 4.\n[1488s->1500s]The Rosenblatt Virtual AI Summit and NASDAQ Investor Conference in London on June 10, and GTC Paris at Viva Tech on June 11 in Paris.\n[1500s->1511s]We look forward to seeing you at these events. Our earnings call to discuss the results of our second quarter of fiscal 2026 is scheduled for August 27.\n[1511s->1517s]Well now, let me turn it over to Jensen to make some remarks.\n[1517s->1526s]Thanks, Collette. We've had a busy and productive year. Let me share my perspective on some topics we're frequently asked.\n[1526s->1536s]On export control, China is one of the world's largest AI markets and a springboard to global success.\n[1536s->1543s]With half of the world's AI researchers based there, the platform that wins China is positioned to lead globally.\n[1543s->1551s]Today, however, the $50 billion China market is effectively closed to U.S. industry.\n[1551s->1560s]The H20 export ban ended our Hopper data center business in China. We cannot reduce Hopper further to comply.\n[1561s->1568s]As a result, we are taking a multi-billion dollar write-off on inventory that cannot be sold or repurposed.\n[1568s->1575s]We are exploring limited ways to compete, but Hopper is no longer an option.\n[1575s->1584s]China's AI moves on with or without U.S. chips. It has to compute to train and deploy advanced models.\n[1585s->1590s]The question is not whether China will have AI. It already does.\n[1590s->1596s]The question is whether one of the world's largest AI markets will run on American platforms.\n[1596s->1604s]Shielding Chinese chipmakers from U.S. competition only strengthens them abroad and weakens America's position.\n[1604s->1609s]Export restrictions have spurred China's innovation in scale.\n[1609s->1616s]The AI race is not just about chips. It's about which stack the world runs on.\n[1616s->1624s]As that stack grows to include 6G and quantum, U.S. global infrastructure leadership is at stake.\n[1624s->1630s]The U.S. has based its policy on the assumption that China cannot make AI chips.\n[1630s->1635s]That assumption was always questionable, and now it's clearly wrong.\n[1635s->1639s]China has enormous manufacturing capability.\n[1639s->1645s]In the end, the platform that wins the AI developers wins AI.\n[1645s->1653s]Export controls should strengthen U.S. platforms, not drive half of the world's AI talent to rivals.\n[1654s->1657s]On DeepSeek.\n[1657s->1665s]DeepSeek and QN from China are among the best open source AI models.\n[1665s->1671s]Released freely, they've gained traction across the U.S., Europe, and beyond.\n[1671s->1680s]DeepSeek R1, like ChatGPT, introduced reasoning AI that produces better answers the longer it thinks.\n[1680s->1690s]Reasoning AI enables step-by-step problem solving, planning, and tool use, turning models into intelligent agents.\n[1690s->1705s]Reasoning is compute intensive, requires hundreds to thousands of times more tokens per task than previous one-shot inference.\n[1705s->1711s]Reasoning models are driving a step function surge in inference demand.\n[1711s->1721s]AI scaling laws remain firmly intact, not only for training, but now inference, too, requires massive scale compute.\n[1721s->1726s]DeepSeek also underscores the strategic value of open source AI.\n[1726s->1734s]When popular models are trained and optimized on U.S. platforms, it drives usage, feedback, and continuous improvement.\n[1734s->1737s]Reinforcing American leadership across the stack.\n[1737s->1743s]U.S. platforms must remain the preferred platform for open source AI.\n[1743s->1749s]That means supporting collaboration with top developers globally, including in China.\n[1749s->1757s]America wins when models like DeepSeek and QN runs best on American infrastructure.\n[1757s->1760s]Regarding onshore manufacturing.\n[1760s->1770s]President Trump has outlined a bold vision to reshore advanced manufacturing, create jobs, and strengthen national security.\n[1770s->1774s]Future plants will be highly computerized in robotics.\n[1774s->1776s]We share this vision.\n[1776s->1783s]TSMC is building six fabs and two advanced packaging plants in Arizona to make chips for Nvidia.\n[1783s->1789s]Process qualification is underway, with volume production expected by year end.\n[1790s->1798s]Spill and Amcor are also investing in Arizona, constructing packaging, assembly, and test facilities.\n[1798s->1806s]In Houston, we're partnering with Foxconn to construct a million-square-foot factory to build AI supercomputers.\n[1806s->1811s]Wistron is building a similar plant in Fort Worth, Texas.\n[1811s->1817s]To encourage and support these investments, we've made substantial long-term purchase commitments.\n[1817s->1821s]A deep investment in America's AI manufacturing future.\n[1823s->1827s]Our goal, from chip to supercomputer.\n[1827s->1830s]Built in America within a year.\n[1830s->1840s]Each GB200 and Beelink 72 racks contains 1.2 million components and weighs nearly two tons.\n[1840s->1844s]No one has produced supercomputers on this scale.\n[1844s->1847s]Our partners are doing an extraordinary job.\n[1849s->1857s]On AI diffusion rule, President Trump rescinded the AI diffusion rule, calling it counterproductive,\n[1857s->1864s]and proposed a new policy to promote US AI tech with trusted partners.\n[1864s->1868s]On his Middle East tour, he announced historic investments.\n[1868s->1875s]I was honored to join him in announcing a 500-megawatt AI infrastructure project in Saudi Arabia\n[1875s->1880s]and a 5-gigawatt AI campus in the UAE.\n[1880s->1883s]President Trump wants US tech to lead.\n[1883s->1887s]The deals he announced are wins for America.\n[1887s->1895s]Creating jobs, advancing infrastructure, generating tax revenue, and reducing the US trade deficit.\n[1895s->1902s]The US will always be Nvidia's largest market and home to the largest install base of our infrastructure.\n[1902s->1909s]Every nation now sees AI as core to the next industrial revolution.\n[1909s->1915s]A new industry that produces intelligence and essential infrastructure for every economy.\n[1915s->1922s]Countries are racing to build national AI platforms to elevate their digital capabilities.\n[1922s->1930s]At Computex, we announced Taiwan's first AI factory in partnership with Foxconn and the Taiwan government.\n[1930s->1936s]Last week, I was in Sweden to launch its first national AI infrastructure.\n[1936s->1947s]Japan, Korea, India, Canada, France, the UK, Germany, Italy, Spain, and more are now building national AI factories\n[1947s->1951s]to empower startups, industries, and societies.\n[1951s->1956s]Sovereign AI is a new growth engine for Nvidia.\n[1958s->1960s]Toshia, back to you. Thank you.\n[1960s->1966s]Operator, we will now open the call for questions. Would you please pull for questions?\n[1966s->1974s]Thank you. At this time, I would like to remind everyone in order to ask a question, press star, then the number one on your telephone keypad.\n[1974s->1978s]We'll pause for just a moment to compile the Q&A roster.\n[1982s->1988s]Your first question comes from the line of Joe Moore with Morgan Stanley. Your line is open.\n[1990s->1998s]Great. Thank you. You guys have talked about this scaling up of inference around reasoning models for at least a year now,\n[1998s->2002s]and we've really seen that come to fruition as you talked about. We've heard it from your customers.\n[2002s->2007s]Can you give us a sense for how much of that demand you're able to serve?\n[2007s->2010s]And give us a sense for maybe how big the inference business is for you guys.\n[2010s->2016s]And do we need full-on NVL-72 rack-scale solutions for reasoning inference going forward?\n[2019s->2026s]Well, we would like to serve all of it. And I think we're on track to serve most of it.\n[2026s->2038s]Grace Blackwell NVLink-72 is the ideal engine today, the ideal computer thinking machine, if you will, for reasoning AI.\n[2038s->2046s]There's a couple of reasons for that. The first reason is that the token generation amount,\n[2046s->2059s]the number of tokens reasoning goes through is a hundred, a thousand times more than a one-shot chatbot.\n[2059s->2069s]You know, it's essentially thinking to itself, breaking down a problem step by step. It might be planning multiple paths to an answer.\n[2069s->2083s]It could be using tools, reading PDFs, reading web pages, watching videos, and then producing a result, an answer.\n[2083s->2089s]The longer it thinks, the better the answer, the smarter the answer is.\n[2090s->2101s]And so what we would like to do and the reason why Grace Blackwell was designed to give such a giant step up in inference performance\n[2101s->2106s]is so that you could do all this and still get a response as quickly as possible.\n[2106s->2117s]Compared to Hopper, Grace Blackwell is some 40 times higher speed and throughput compared.\n[2117s->2132s]And so this is going to be a huge, huge benefit and driving down the cost while improving the quality of response with excellent quality of service at the same time.\n[2132s->2139s]So that's the fundamental reason. That was the core driving reason for Grace Blackwell NVLink-72.\n[2139s->2151s]Of course, in order to do that, we had to reinvent, literally redesign the entire way that these supercomputers are built.\n[2151s->2161s]But now we're in full production. It's going to be exciting. It's going to be incredibly exciting.\n[2162s->2171s]The next question comes from Vivek Arya with Bank of America Securities. Your line is open.\n[2171s->2176s]Thanks for the question. Just a clarification for Colette first.\n[2176s->2184s]So on the China impact, I think previously it was mentioned that at about $15 billion, so you had the $8 billion in Q2.\n[2184s->2190s]So is there still some left as a headwind for the remaining quarters, just Colette, how to model that?\n[2190s->2201s]And then question, Jensen, for you. Back at GTC, you had outlined a path towards almost a trillion dollars of AI spending over the next few years.\n[2201s->2212s]Where are we in that build out? And do you think it's going to be uniform that you will see every spender, whether it's ESP, sovereigns, enterprises or build out?\n[2212s->2215s]Should we expect some periods of digestion in between?\n[2215s->2221s]What are your customer discussions telling you about how to model growth for next year?\n[2223s->2232s]Yes, Vivek. Thanks so much for the question regarding H20. Yes, we recognized 4.6 H20 in Q1.\n[2232s->2241s]We were unable to ship $2.5 billion. So the total for Q1 should have been $7 billion.\n[2241s->2248s]When we look at our Q2, our Q2 is going to be meaningfully down in terms of China data center revenue.\n[2248s->2258s]And we had highlighted in terms of the amount of orders that we had planned for H20 and Q2, and that was $8 billion.\n[2258s->2266s]Now, going forward, we did have other orders going forward that we will not be able to fulfill.\n[2266s->2275s]That is what was incorporated, therefore, in the amount that we wrote down of the $4.5 billion.\n[2275s->2280s]That write down was about inventory and purchase commitments.\n[2280s->2287s]And our purchase commitments were about what we expected regarding the orders that we had received.\n[2287s->2295s]Going forward, though, it's a bigger issue regarding the amount of the market that we will not be able to serve.\n[2295s->2306s]We assess that TAM to be close to about $50 billion in the future as we don't have a product to enable for the China.\n[2310s->2318s]Vivek, probably the best way to think through it is that AI is several things.\n[2318s->2326s]Of course, we know that AI is this incredible technology that's going to transform every industry,\n[2326s->2344s]from, of course, the way we do software to health care and financial services to retail to, I guess, every industry, transportation, manufacturing.\n[2344s->2347s]And we're at the beginning of that.\n[2347s->2352s]But maybe another way to think about that is where do we need intelligence?\n[2352s->2354s]Where do we need digital intelligence?\n[2354s->2358s]And it's in every country, it's in every industry.\n[2358s->2365s]And we know because of that, we recognize that AI is also an infrastructure.\n[2365s->2371s]It's a way of delivering a technology that requires factories.\n[2371s->2374s]And these factories produce tokens.\n[2374s->2380s]And they, as I mentioned, are important to every single industry in every single country.\n[2380s->2389s]And so on that basis, we're really at the very beginning of it because the adoption of this technology is really kind of in its early stages.\n[2389s->2399s]Now, we've reached an extraordinary milestone with AIs that are reasoning, are thinking, what people call inference time scaling.\n[2399s->2413s]And of course, it created a whole new, we've entered an era where inference is going to be a significant part of the compute workload.\n[2413s->2417s]But anyhow, it's going to be a new infrastructure.\n[2417s->2421s]And we're building it out in the clouds.\n[2421s->2429s]The United States is really the early starter and available in US clouds.\n[2429s->2432s]And this is our largest market, our largest installed base.\n[2432s->2435s]And we can continue to see that happening.\n[2435s->2442s]But beyond that, we're going to have to, we're going to see AI go into enterprise, which is on prem.\n[2442s->2447s]Because so much of the data is still on prem, access control is really important.\n[2447s->2452s]It's really hard to move all of every company's data into the cloud.\n[2452s->2455s]And so we're going to move AI into the enterprise.\n[2455s->2460s]And you saw that we announced a couple of really exciting new products.\n[2460s->2468s]Our RTX Pro Enterprise AI server that runs everything enterprise and AI.\n[2468s->2476s]Our DGX Spark and DGX Station, which is designed for developers who want to work on prem.\n[2476s->2481s]And so enterprise AI is just taking off.\n[2481s->2490s]Telcos, today, a lot of the telco infrastructure will be in the future software defined and built on AI.\n[2490s->2492s]And so 6G is going to be built on AI.\n[2492s->2495s]And that infrastructure needs to be built out.\n[2495s->2498s]And they said it's very, very early stages.\n[2498s->2505s]And then, of course, every factory today that makes things will have an AI factory that sits with it.\n[2505s->2513s]And the AI factory is going to be drive creating AI and operating AI for the factory itself.\n[2513s->2518s]But also to power the products and the things that are made by the factory.\n[2518s->2523s]So it's very clear that every car company will have AI factories.\n[2523s->2527s]And very soon there'll be robotics companies, robot companies.\n[2527s->2534s]And those companies will be also building AIs to drive the robots.\n[2534s->2542s]And so we're at the beginning of all of this build out.\n[2542s->2547s]The next question comes from CJ News with Cantor Fitzgerald.\n[2547s->2549s]Your line is open.\n[2549s->2552s]Yeah, good afternoon. Thank you for taking the question.\n[2552s->2556s]There have been many large GPU cluster investment announcements in the last month.\n[2556s->2560s]And you alluded to a few of them with Saudi Arabia, the UAE.\n[2560s->2563s]And then also, you know, we heard from Oracle and XAI, just to name a few.\n[2563s->2570s]So my question, are there other that have yet to be announced of the same kind of scale and magnitude?\n[2570s->2583s]And perhaps more importantly, how are these orders impacting your lead times for Blackwell and your current visibility sitting here today, you know, almost halfway through 2025?\n[2583s->2593s]Well, we have more orders today than we did at the last time I spoke about orders at GTC.\n[2593s->2599s]However, we're also increasing our supply chain and building out our supply chain.\n[2599s->2602s]They're doing a fantastic job.\n[2602s->2613s]We're building it here on shore in the United States, but we're going to keep our supply chain quite busy for several many more years coming.\n[2613s->2625s]And with respect to further announcements, I'm going to be on the road next week through Europe.\n[2625s->2640s]And it's just about every country needs to build out AI infrastructure and their umpteen AI factories being planned.\n[2640s->2650s]I think in the remarks, Colette mentioned there's some 100 AI factories being built.\n[2650s->2653s]There's a whole bunch that haven't been announced.\n[2653s->2673s]And I think the important concept here, which makes it easier to understand, is that like other technologies that impact literally every single industry, of course, electricity was one, and it became infrastructure.\n[2673s->2683s]Of course, the information infrastructure, which we now know as the Internet, affects every single industry, every country, every society.\n[2683s->2686s]Intelligence is surely one of those things.\n[2686s->2693s]I don't know any company, industry, country who thinks that intelligence is optional.\n[2693s->2694s]It's essential infrastructure.\n[2694s->2701s]And so we've now digitalized intelligence.\n[2701s->2708s]And so I think we're clearly in the beginning of the build out of this infrastructure.\n[2708s->2712s]And every country will have it.\n[2712s->2713s]I'm certain of that.\n[2713s->2714s]Every industry will use it.\n[2714s->2716s]That I'm certain of.\n[2716s->2722s]And what's unique about this infrastructure is that it needs factories.\n[2722s->2728s]You know, it's a little bit like the energy infrastructure, electricity.\n[2728s->2730s]It needs factories.\n[2730s->2733s]We need factories to produce this intelligence.\n[2733s->2736s]And the intelligence is getting more sophisticated.\n[2736s->2743s]We were talking about earlier that we had a huge breakthrough in the last couple of years with reasoning AI.\n[2743s->2748s]And now there are agents that reason and there are super agents that use a whole bunch of tools.\n[2748s->2754s]And then there's clusters of super agents where agents are working with agents, solving problems.\n[2754s->2769s]And so you could just imagine compared to one shot chat bots and the agents that are now using AI built on these large language models, how much more compute intensive they really need to be and are.\n[2769s->2774s]And so I think we're in the beginning of the build out.\n[2774s->2782s]And there should be many, many more announcements in the future.\n[2782s->2783s]Your next question.\n[2783s->2784s]Excuse me.\n[2784s->2788s]Your next question comes from Ben Reitz's with Melius.\n[2788s->2790s]Your line is open.\n[2790s->2791s]Yeah, hi.\n[2791s->2793s]Thanks for the question.\n[2793s->2801s]I wanted to ask first to Collette just a little clarification around the guidance and maybe putting it in a different way.\n[2801s->2813s]The 8 billion for H20 just seems like it's roughly 3 billion more than most people thought with regard to what you'd be foregoing in the second quarter.\n[2813s->2823s]So that would mean that with regard to your guidance, the rest of the business in order to hit 45 is doing 2 to 3 billion or so better.\n[2824s->2828s]So I was wondering if that math made sense to you.\n[2828s->2837s]And then in terms of the guidance, that would imply the non-China business is doing a bit better than the street expected.\n[2837s->2843s]So wondering what the primary driver was there in your view.\n[2843s->2851s]And then this second part of my question, Jensen, I know you guide one quarter at a time.\n[2851s->2871s]But with regard to the AI diffusion rule being lifted and this momentum was sovereign, there's been times in your history where you guys have said on calls like this where you have more conviction and sequential growth throughout the year, et cetera.\n[2871s->2886s]And given the unleashing of demand with AI diffusion being revoked and the supply chain increasing, does the environment give you more conviction and sequential growth as we go throughout the year?\n[2886s->2891s]So first one for Collette and then next one for Jensen. Thanks so much.\n[2891s->2908s]Thanks, Ben, for the question. When we look at our Q2 guidance and our commentary that we provided that had the export controls not occurred, we would have had orders of about 8 billion for H20.\n[2908s->2918s]That's correct. That was a possibility for what we would have had in our outlook for this quarter in Q2.\n[2918s->2932s]So what we also have talked about here is the growth that we've seen in Blackwell, Blackwell across many of our customers, as well as the growth that we continue to have in terms of supply that we need for our customers.\n[2932s->2938s]So putting those together, that's where we came through with the guidance that we provided.\n[2938s->2942s]I'm going to turn the rest over to Jensen to see how he wants to.\n[2942s->2945s]Yeah, thanks. Thanks, Ben.\n[2945s->2957s]I would say compared to the beginning of the year, compared to GTC timeframe, there are four positive surprises.\n[2957s->2967s]The first positive surprise is the step function demand increase of reasoning AI.\n[2967s->2977s]I think it is fairly clear now that AI is going through an exponential growth, and reasoning AI really busts it through.\n[2977s->2986s]Concerns about hallucination or its ability to really solve problems.\n[2986s->2999s]I think a lot of people are crossing that barrier and realizing how incredibly effective agentic AI is and reasoning AI is.\n[2999s->3007s]So number one is inference reasoning and the exponential growth there, demand growth.\n[3007s->3012s]The second one, you mentioned AI diffusion.\n[3012s->3019s]It's really terrific to see that the AI diffusion rule was rescinded.\n[3019s->3032s]President Trump wants America to win, and he also realizes that we're not the only country in the race.\n[3032s->3042s]He wants the United States to win and recognizes that we have to get the American stack out to the world\n[3042s->3047s]and have the world build on top of American stacks instead of alternatives.\n[3047s->3052s]And so AI diffusion happened.\n[3052s->3062s]The rescinding of it happened at almost precisely the time that the countries around the world are awakening the importance of AI as an infrastructure,\n[3062s->3072s]not just as a technology of great curiosity and great importance, but infrastructure for their industries and startups and society.\n[3072s->3078s]Just as they had to build out infrastructure for electricity and Internet, you got to build out an infrastructure for AI.\n[3078s->3083s]I think that that's an awakening and that creates a lot of opportunity.\n[3083s->3085s]The third is enterprise AI.\n[3085s->3093s]Agents work and agents are doing these agents are really quite successful.\n[3093s->3098s]Much more than generative AI, agentic AI is game changing.\n[3098s->3113s]Agents can understand ambiguous and rather implicit instructions and able to problem solve and use tools and have memory and so on.\n[3113s->3118s]And so I think enterprise AI is ready to take off.\n[3118s->3133s]And it's taken us a few years to build a computing system that is able to integrate, run enterprise AI stacks, run enterprise IT stacks, but add AI to it.\n[3133s->3142s]And this is the RTX Pro Enterprise server that we announced at Computex just last week.\n[3142s->3149s]And just about every major IT company has joined us super excited about that.\n[3149s->3152s]And so computing is one stat, one part of it.\n[3152s->3157s]But remember, enterprise IT is really three pillars.\n[3157s->3161s]It's compute, storage and networking.\n[3161s->3164s]And we've now put all three of them together for finally.\n[3164s->3167s]And we're going to market with that.\n[3167s->3170s]And then lastly, industrial AI.\n[3170s->3186s]Remember, one of the implications of the world reordering, if you will, is regions onshore manufacturing and building plants everywhere.\n[3186s->3195s]In addition to AI factories, of course, there are new electronics manufacturing chip manufacturing being built around the world.\n[3195s->3210s]And all of these new plants and these new factories are creating exactly the right time when Omniverse and AI and all the work that we're doing with robotics is emerging.\n[3210s->3215s]And so this fourth pillar is quite important.\n[3215s->3218s]Every factory will have an AI factory associated with it.\n[3218s->3226s]And in order to create these physical AI systems, you really have to train a vast amount of data.\n[3226s->3232s]So back to more data, more training, more AIs to be created, more computers.\n[3232s->3243s]And so these four drivers are really kicking into turbocharge.\n[3243s->3248s]Your next question comes from Timothy R. Curie with UBS.\n[3248s->3250s]Your line is open.\n[3250s->3251s]Thanks a lot.\n[3251s->3253s]Jensen, I wanted to ask about China.\n[3253s->3257s]It sounds like the July guidance assumes there's no SKU replacement for the H-20.\n[3257s->3262s]But if the president wants the U.S. to win, it seems like you're going to have to be allowed to ship something into China.\n[3262s->3265s]So I guess I had two points on that.\n[3265s->3269s]First of all, have you been approved to ship a new modified version into China?\n[3269s->3272s]And you're currently building it, but you just can't ship it in fiscal Q2.\n[3272s->3277s]And then you were sort of run rating $7 to $8 billion a quarter into China.\n[3277s->3283s]Can we get back to those sorts of quarterly run rates once you get something that you're allowed to ship back into China?\n[3283s->3287s]I think we're all trying to figure out how much to add back to our models and when.\n[3287s->3289s]So whatever you can say there would be great.\n[3289s->3291s]Thanks.\n[3291s->3293s]The president has a plan.\n[3293s->3296s]He has a vision and I trust him.\n[3296s->3305s]With respect to our export controls, it's a set of limits.\n[3305s->3322s]And the new set of limits pretty much make it impossible for us to reduce Hopper any further for any productive use.\n[3322s->3330s]And so the new limits, it's kind of the end of the road for Hopper.\n[3330s->3333s]We have limited options.\n[3333s->3337s]And so the key is to understand the limits.\n[3337s->3346s]The key is to understand the limits and see if we can come up with interesting products that could continue to serve the Chinese market.\n[3346s->3350s]We don't have anything at the moment.\n[3350s->3352s]But we're considering it.\n[3352s->3354s]We're thinking about it.\n[3354s->3358s]Obviously the limits are quite stringent at the moment.\n[3358s->3363s]And we have nothing to announce today.\n[3363s->3373s]And when the time comes, you know, we'll engage the administration and discuss that.\n[3373s->3378s]Your final question comes from the line of Aaron Rakers with Wells Fargo.\n[3378s->3381s]Your line is open.\n[3381s->3382s]Hi.\n[3382s->3383s]This is Jake on for Aaron.\n[3383s->3386s]Thanks for taking the question and congrats on the great quarter.\n[3386s->3399s]I was wondering if you could give some additional color around the strength you saw within the networking business, particularly around the adoption of your Ethernet solutions at CSPs as well as your\n[3400s->3404s]any change you're seeing in network attach rates.\n[3404s->3407s]Yeah, thank you for that.\n[3407s->3415s]We now have three networking platforms, maybe four.\n[3415s->3425s]The first one is the scale up platform to turn a computer into a much larger computer.\n[3425s->3427s]Scaling up is incredibly hard to do.\n[3427s->3429s]Scaling out is easier to do.\n[3429s->3431s]But scaling up is hard to do.\n[3431s->3433s]And that platform is called NVLink.\n[3433s->3446s]And NVLink comes with it, chips and switches and, you know, NVLink spines.\n[3446s->3448s]It's really complicated.\n[3448s->3452s]But anyways, that's our new platform, scale up platform.\n[3452s->3455s]In addition to InfiniBand, we also have SpectrumX.\n[3455s->3466s]We've been fairly, fairly consistent that Ethernet was designed for a lot of traffic that are independent.\n[3466s->3471s]But in the case of AI, you have a lot of computers working together.\n[3471s->3477s]And the traffic of AI is insanely bursty.\n[3477s->3479s]Latency matters a lot.\n[3479s->3484s]Because the AI is thinking and it wants to get work done as quickly as possible.\n[3484s->3487s]And you got a whole bunch of nodes working together.\n[3487s->3499s]And so we enhanced Ethernet, added capabilities like extremely low latency, congestion control, adaptive routing,\n[3499s->3506s]the type of technologies that were available only in InfiniBand to Ethernet.\n[3506s->3511s]And as a result, we improved the utilization of Ethernet in these clusters.\n[3511s->3519s]These clusters are gigantic from as low as 50% to as high as 85%, 90%.\n[3519s->3531s]And so the difference is if you had a cluster that's $10 billion and you improved its effectiveness by 40%, that's worth $4 billion.\n[3532s->3534s]It's incredible.\n[3534s->3538s]And so SpectrumX has been really, quite frankly, a home run.\n[3538s->3553s]And this last quarter, as we said in the prepared remarks, we added two very significant CSPs to the SpectrumX adoption.\n[3553s->3558s]And then the last one, the last one is Bluefield, which is our control plane.\n[3558s->3566s]And so in those four, the control plane and network, which is used for storage, is used for security,\n[3566s->3576s]and for many of these clusters that want to achieve isolation among its users, multi-tenant clusters,\n[3576s->3583s]and still be able to use and have extremely high performance, bare metal performance,\n[3584s->3591s]Bluefield is ideal for that and is used in a lot of these cases.\n[3591s->3599s]And so we have these four networking platforms that are all growing and we're doing really well.\n[3599s->3601s]I'm very proud of the team.\n[3603s->3607s]That is all the time we have for questions.\n[3607s->3610s]Denton, I will turn the call back to you.\n[3610s->3611s]Thank you.\n[3611s->3612s]Thank you.\n[3612s->3615s]This is the start of a powerful new wave of growth.\n[3615s->3617s]Grace Blackwell is in full production.\n[3617s->3619s]We're off to the races.\n[3619s->3621s]We now have multiple significant growth engines.\n[3621s->3627s]Inference, one's delighted workload is surging with revenue generating AI services.\n[3627s->3635s]AI is growing faster and will be larger than any platform shifts before, including the internet, mobile, and cloud.\n[3635s->3645s]Blackwell is built to power the full AI lifecycle from training frontier models to running complex inference and reasoning agents at scale.\n[3645s->3655s]Training demands continues to rise with breakthroughs in post-training and like reinforcement learning and synthetic data generation.\n[3655s->3657s]But inference is exploding.\n[3657s->3661s]Reasoning AI agents require orders of magnitude more compute.\n[3661s->3667s]The foundations of our next growth platforms are in place and ready to scale.\n[3667s->3674s]Sovereign AI nations are investing in AI infrastructure like they once did for electricity and internet.\n[3674s->3681s]Enterprise AI, AI must be deployable on prem and integrated with existing IT.\n[3681s->3692s]Our RTX Pro, DGX Spark, and DGX Station Enterprise AI systems are ready to modernize the $500 billion IT infrastructure on prem or in the cloud.\n[3692s->3696s]Every major IT provider is partnering with us.\n[3696s->3710s]Industrial AI from training to digital twin simulation to deployment, NVIDIA Omniverse and Isaac Groot are powering next generation factories and humanoid robotic systems worldwide.\n[3710s->3723s]The age of AI is here from AI infrastructures, inference at scale, sovereign AI, enterprise AI, and industrial AI.\n[3723s->3725s]NVIDIA is ready.\n[3725s->3740s]Join us at GTC Paris, our keynote at VivaTech on June 11, talking about quantum GPU computing, robotic factories and robots, and celebrate our partnerships building AI factories across the region.\n[3740s->3745s]The NVIDIA Band will tour France, the UK, Germany, and Belgium.\n[3745s->3753s]Thank you for joining us at the earnings call today. See you in Paris.\n[3753s->3758s]This concludes today's conference call. You may now disconnect.\n",
            "Q2 FY26 Outlook\n\nRevenue\n\n$45.0 billion, plus or minus 2%\nExpect modest sequential growth across all our platforms\nContinued ramp of Blackwell to be partially offset by a decline in China revenue\nOutlook reflects a loss in H20 revenue of ~$8.0 billion\n\nGross Margins\n\n71.8% GAAP and 72.0% non-GAAP, plus or minus 50 basis points\nExpect better Blackwell profitability to drive a modest sequential improvement\nWorking toward achieving gross margins in the mid-70% range late this year\n\nOperating Expense\nApproximately $5.7 billion GAAP and $4.0 billion non-GAAP\nExpect full year FY26 operating expense growth to be in the mid-30% range\n\nOther Income & Expense\nIncome of approximately $450 million for GAAP and non-GAAP\nExcluding gains and losses from non-marketable and publicly-held equity securities\n\nTax Rate\n16.5% GAAP and non-GAAP, plus or minus 1%, excluding discrete items\n\nRefer to Appendix for reconciliation of Non-GAAP measures.",
            "Item 2. Management's Discussion and Analysis of Financial Condition and Results of Operations\n\nForward-Looking Statements \nThis Quarterly Report on Form 10-Q contains forward-looking statements within the meaning of Section 27A of the Securities Act of 1933, as amended, and\nSection 21E of the Securities Exchange Act of 1934, as amended, which are subject to the “safe harbor” created by those sections based on management’s\nbeliefs and assumptions and on information currently available to management. In some cases, you can identify forward-looking statements by terms such as\n“may,” “will,” “should,” “could,” “goal,” “would,” “expect,” “plan,” “anticipate,” “believe,” “estimate,” “project,” “predict,” “potential” and similar expressions intended\nto identify forward-looking statements. These statements involve known and unknown risks, uncertainties and other factors, which may cause our actual results,\nperformance, time frames or achievements to be materially different from any future results, performance, time frames or achievements expressed or implied by\nthe forward-looking statements. We discuss many of these risks, uncertainties and other factors in this Quarterly Report on Form 10-Q and our Annual Report on\nForm 10-K for the fiscal year ended January 26, 2025 in greater detail under the heading “Risk Factors” of such reports. Given these risks, uncertainties, and\nother factors, you should not place undue reliance on these forward-looking statements. Also, these forward-looking statements represent our estimates and\nassumptions only as of the date of this filing. You should read this Quarterly Report on Form 10-Q completely and understand that our actual future results may\nbe materially different from what we expect. We hereby qualify our forward-looking statements by these cautionary statements. Except as required by law, we\nassume no obligation to update these forward-looking statements publicly, or to update the reasons actual results could differ materially from those anticipated in\nthese forward-looking statements, even if new information becomes available in the future.\n\nAll references to “NVIDIA,” “we,” “us,” “our” or the “Company” mean NVIDIA Corporation and its subsidiaries.\n\n© 2025 NVIDIA Corporation. All rights reserved.\n\nThe following discussion and analysis of our financial condition and results of operations should be read in conjunction with the risk factors set forth in Item 1A.\n“Risk Factors” of our Annual Report on Form 10-K for the fiscal year ended January 26, 2025 and Part II, Item 1A. “Risk Factors” of this Quarterly Report on\nForm 10-Q and our Condensed Consolidated Financial Statements and related Notes thereto, as well as other cautionary statements and risks described\nelsewhere in this Quarterly Report on Form 10-Q and our other filings with the SEC, before deciding to purchase, hold, or sell shares of our common stock.\n\nOverview\n\nOur Company and Our Businesses\n\nNVIDIA pioneered accelerated computing to help solve the most challenging computational problems. Since our original focus on PC graphics, we have\nexpanded to several other large and important computationally intensive fields. Fueled by the sustained demand for exceptional 3D graphics and the scale of the\ngaming market, NVIDIA has leveraged its GPU architecture to create platforms for scientific computing, AI, data science, AV, robotics, and digital twin\napplications.\n\nOur two operating segments are \"Compute & Networking\" and \"Graphics,\" as described in Note 13 of the Notes to Condensed Consolidated Financial\nStatements in Part I, Item 1 of this Quarterly Report on Form 10-Q.\n\nHeadquartered in Santa Clara, California, NVIDIA was incorporated in California in April 1993 and reincorporated in Delaware in April 1998.\n\nRecent Developments, Future Objectives and Challenges\n\nRevenue growth in the first quarter of fiscal year 2026 was driven by data center compute and networking platforms for accelerated computing and AI solutions.\nOur Blackwell GPU revenue ramp continued during the quarter, extending beyond large cloud service providers to include consumer internet companies and\nregional cloud service providers.\n\nOn April 9, 2025, the U.S. government, or USG, informed us that it requires a license for export to China (including Hong Kong and Macau) and D:5 countries, or\nto companies headquartered or with an ultimate parent therein, of our H20 integrated circuits and any other circuits achieving the H20’s memory bandwidth,\ninterconnect bandwidth, or combination thereof. As a result of these new requirements, we incurred a $4.5 billion charge in the first quarter of fiscal year 2026\nassociated with H20 excess inventory and purchase obligations as the demand for H20 products diminished. The $4.5 billion charge was less than what we\ninitially anticipated as we were able to re-use certain materials. Sales of our H20 products were $4.6 billion for the first quarter of fiscal year 2026 prior to the\nnew export licensing requirements. The H20 export licensing requirements have impacted our current revenue and will also negatively affect our future revenue.\nWe are still evaluating our limited options to supply Data Center compute products compliant with the USG’s export control rules. The export controls applicable\nto China are complex and address a variety of parameters, including the total processing performance of a chip, the “performance density” of a chip, the\ninterconnect bandwidth of a chip, and the memory bandwidth of a chip. We may be unable to create a competitive product for China’s data center market that\nreceives approval from the USG. In that event, we would effectively be foreclosed from competing in China's data center computing/compute market, with a\nmaterial and adverse impact on our business, operating results, and financial condition.\n\n22"
        ]
    },
    {
        "question": "根据 10-Q 报告，公司面临的一个主要风险是什么？",
        "ground_truth": "10-Q 报告中的一个关键风险是，公司受到复杂的法律法规和政治行动的影响，特别是针对其产品的出口限制。美国政府已实施并可能进一步收紧出口管制，这已损害并将继续损害公司的竞争地位和未来业绩。",
        "answer": "根据10-Q报告，英伟达面临的一个主要风险是长制造周期和不确定的供应和产能可用性，这可能导致供需不匹配。报告指出，英伟达在制造产品时需要很长时间，并提前在预期需求之前生产并保持库存。在半导体行业短缺或供应链中供应或产能有限的情况下，某些供应的交货期可能会延长。此外，如果无法准确估计需求，或者客户更改订单，可能会导致供需不匹配，从而严重影响财务结果，并且这种情况可能会再次发生 [来源: 10-Q Report, 第 30 页]。",
        "contexts": [
            "could impact our competitive position, and may negatively impact our business operations and ability to manufacture and ship our products. There can be no\nassurance that our employees, contractors, suppliers, customers or agents will not violate applicable laws or the policies, controls, and procedures that we have\ndesigned to help ensure compliance with such laws, and violations could result in fines, criminal sanctions against us, our officers, or our employees, prohibitions\non the conduct of our business, and damage to our reputation. Changes to the laws, rules and regulations to which we are subject, or changes to their\ninterpretation and enforcement, could lead to materially greater compliance and other costs, and/or further restrictions on our ability to manufacture and supply\nour products and operate our business. For example, we may face increased compliance costs as a result of changes or increases in antitrust legislation,\nregulation, administrative rule making, increased focus from regulators on cybersecurity vulnerabilities and risks. Our position in markets relating to AI has led to\nincreased interest in our business from regulators worldwide, including the European Union, the United States, the United Kingdom, South Korea, Japan, and\nChina. For example, the French Competition Authority collected information from us regarding our business and competition in the graphics card and CSP\nmarket as part of an ongoing inquiry into competition in those markets. We have also received, and continue to receive, broad requests for information from\ncompetition regulators in the European Union, the United States, the United Kingdom, China, and South Korea regarding our sales of GPUs and other NVIDIA\nproducts, our efforts to allocate supply, foundation models and our investments, partnerships and other agreements with companies developing foundation\nmodels, the markets in which we compete and our competition, our strategies, roadmaps, and efforts to develop, market, and sell hardware, software, and\nsystem solutions, and our agreements with customers, suppliers, and partners. We expect to receive additional requests for information in the future. Such\nrequests have been and are likely to be expensive and burdensome and could negatively impact our business and our relationships with customers, suppliers,\nand partners.\n\nGovernments and regulators are also considering, and in certain cases, have imposed restrictions on the hardware, software, and systems used to develop\nfrontier foundation models and generative AI. For example, the EU AI Act became effective on August 1, 2024 and will be fully applicable after a two-year\ntransitional period. The EU AI Act may impact our ability to train, deploy, or release AI models in the EU. Several states are considering enacting or have already\nenacted regulations concerning AI technologies, which may impact our ability to train, deploy, or release AI models, and increase our compliance costs.\nRestrictions under these and any other regulations, if implemented, could increase the costs and burdens to us and our customers, delay or halt deployment of\nnew systems using our products, and reduce the number of new entrants and customers, negatively impacting our business and financial results. Revisions to\nlaws or regulations or their interpretation and enforcement could also result in increased taxation, trade sanctions, the imposition of or increase to import duties\nor tariffs, restrictions and controls on imports or exports, or other retaliatory actions, which could have an adverse effect on our business plans or impact the\ntiming of our shipments. Additionally, changes in the public perception of governments in the regions where we operate or plan to operate could negatively\nimpact our business and results of operations.\n\nGovernment actions, including trade protection and national and economic security policies of U.S. and foreign government bodies, such as tariffs, import or\nexport regulations, including deemed export restrictions and restrictions on the activities of U.S. persons, trade and economic sanctions, decrees, quotas or\nother trade barriers and restrictions could affect our ability to ship products, provide services to our customers and employees, do business without an export\nlicense with entities on the U.S. Department of Commerce’s U.S. Entity List or other USG restricted parties lists (which is expected to change from time to time),\nand generally fulfill our contractual obligations and have a material adverse effect on our business. If we were ever found to have violated export control laws or\nsanctions of the U.S. or similar applicable non-U.S. laws, even if the violation occurred without our knowledge, we may be subject to various penalties available\nunder the laws, any of which could have a material and adverse impact on our business, operating results and financial condition.\n\nFor example, in response to the war in Ukraine, the United States and other jurisdictions imposed economic sanctions and export control measures which\nblocked the passage of our products, services and support into Russia, Belarus, and certain regions of Ukraine. In fiscal year 2023, we stopped direct sales to\nRussia and closed business operations in Russia. Concurrently, the war in Ukraine has impacted sales in EMEA and may continue to do so in the future.\n\nThe increasing focus on the risks and strategic importance of AI technologies has resulted in regulatory restrictions that target products and services capable of\nenabling or facilitating AI and may in the future result in additional restrictions impacting some or all of our product and service offerings.\n\nConcerns regarding third-party use of AI for purposes contrary to local governmental interests, including concerns relating to the misuse of AI applications,\nmodels, and solutions, has resulted in and could in the future result in unilateral or multilateral restrictions on products that can be used for training, modifying,\ntuning, and deploying LLMs and other AI applications. Such restrictions have limited and could in the future limit the ability of downstream customers and users\nworldwide to acquire, deploy and use systems that include our products, software, and services, and negatively impact our business and financial results.\n\nSuch restrictions could include additional unilateral or multilateral export controls on certain products or technology, including but not limited to AI technologies.\nAs geopolitical tensions have increased, semiconductors associated with AI, including GPUs and related products, are increasingly the focus of export control\nrestrictions proposed by stakeholders in the U.S. and its allies. The United States has imposed unilateral worldwide controls restricting GPUs and associated\n\n33",
            "Item 4. Controls and Procedures\n\nControls and Procedures\n\nDisclosure Controls and Procedures\n\nBased on their evaluation as of April  27, 2025, our management, including our Chief Executive Officer and Chief Financial Officer, has concluded that our\ndisclosure controls and procedures (as defined in Rule 13a-15(e) under the Exchange Act) were effective to provide reasonable assurance that the information\nwe are required to disclose in reports that we file or submit under the Exchange Act is recorded, processed, summarized and reported within the time periods\nspecified in the SEC rules and forms, and that such information is accumulated and communicated to our management, including our Chief Executive Officer\nand our Chief Financial Officer, as appropriate, to allow timely decisions regarding required disclosures.\n\nChanges in Internal Control Over Financial Reporting\n\nThere have been no changes in our internal control over financial reporting during the first quarter of fiscal year 2026 that have materially affected, or are\nreasonably likely to materially affect, our internal control over financial reporting. We are continuing a phased upgrade of our enterprise resource planning, or\nERP, system to update our existing core financial systems. The ERP system is designed to accurately maintain our financial records used to report operating\nresults. We will continue to evaluate each quarter whether there are changes that materially affect our internal control over financial reporting.\n\nInherent Limitations on Effectiveness of Controls\n\nOur management, including our Chief Executive Officer and Chief Financial Officer, does not expect that our disclosure controls and procedures or our internal\ncontrols, will prevent all error and all fraud. A control system, no matter how well conceived and operated, can provide only reasonable, not absolute, assurance\nthat the objectives of the control system are met. Further, the design of a control system must reflect the fact that there are resource constraints, and the benefits\nof controls must be considered relative to their costs. Because of the inherent limitations in all control systems, no evaluation of controls can provide absolute\nassurance that all control issues and instances of fraud, if any, within NVIDIA have been detected.\n\nPart II. Other Information\n\nItem 1. Legal Proceedings\n\nRefer to Part I, Item 1, Note 11  of the Notes to Condensed Consolidated Financial Statements for a discussion of significant developments in our legal\nproceedings since January 26, 2025. Also refer to Item 3, “Legal Proceedings” in our Annual Report on Form 10-K for the fiscal year ended January 26, 2025 for\na prior discussion of our legal proceedings.\n\nItem 1A. Risk Factors\n\nOther than the risk factors listed below, there have been no material changes from the risk factors previously described under Item 1A of our Annual Report on\nForm 10-K for the fiscal year ended January 26, 2025.\n\nPurchasing or owning NVIDIA common stock involves investment risks including, but not limited to, the risks described in Item 1A of our Annual Report on Form\n10-K for the fiscal year ended January 26, 2025, and below. Any one of those risks could harm our business, financial condition and results of operations or\nreputation, which could cause our stock price to decline. Additional risks, trends and uncertainties not presently known to us or that we currently believe are\nimmaterial may also harm our business, financial condition, results of operations or reputation.\n\nLong manufacturing lead times and uncertain supply and capacity availability, combined with a failure to estimate customer demand accurately, has\nled and could lead to mismatches between supply and demand.\n\nWe have long manufacturing lead times and build finished products and maintain inventory in advance of anticipated demand. In periods of shortages impacting\nthe semiconductor industry and/or limited supply or capacity in our supply chain, the lead times for certain supply may be extended. We have previously\nexperienced and may continue to experience extended lead times of more than 12 months. To secure future supply and capacity, we have paid premiums,\nprovided deposits, and entered into long-term supply agreements and capacity commitments, which have increased our product costs and this may continue.\nWe may still be unable to secure sufficient commitments for capacity to address our business needs.\n\nIf we inaccurately estimate demand, or our customers change orders, as we have experienced in the past, we may not be able to reduce our supply\ncommitments in time, at the same rate, or at all. Significant mismatches between supply and demand have varied across our market platforms, resulted in both\nproduct shortages and excess inventory, significantly harmed our financial results and could reoccur. If we underestimate demand, and our foundry partners and\ncontract manufacturers are unable to increase production or provide sufficient supply, we may not be able to meet increased\n\n30",
            "[TABLE]\n| Research and development expenses | $ | 3,989 |  | $ | 2,720 |  | $ | 1,269 |  | 47 | % |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| Sales, general and administrative expenses | 1,041 |  |  | 777 |  |  | 264 |  |  | 34 | % |\n| Total operating expenses | $ | 5,030 |  | $ | 3,497 |  | $ | 1,533 |  | 44 | % |\n[/TABLE]\n\n该表格展示了公司研发和运营费用随时间的变化。研发费用逐年增长，从2019年的1,269万美元增至2021年的3,989万美元，增长率为47%。同期，销售、一般和行政费用也从777万美元增至1,041万美元，增长率为34%。总体运营费用从2019年的1,533万美元增至2021年的5,030万美元，增长率为44%。\n\n[TABLE]\n| Interest income | $ | 515 |  | $ | 359 |  | $ | 156 |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| Interest expense | (63) |  |  | (64) |  |  | 1 |  |\n| Other income (expense), net | (180) |  |  | 75 |  |  | (255) |  |\n| Total other income (expense), net | $ | 272 |  | $ | 370 |  | $ | (98) |\n[/TABLE]\n\n该表格展示了不同时间段的利息收入、利息支出、其他收入（支出）净额以及总收入（支出）净额。主要趋势是利息收入逐年增加，而利息支出和总支出净额则有所波动，其中2019年总支出净额为负值，表明支出大于收入。\n\nOperating Expenses\n\nThree Months Ended\n\nApr 27, 2025\nApr 28, 2024\n\n$\nChange\n\n%\nChange\n\nThe increase in research and development expenses for the first quarter of fiscal year 2026 was driven by a 34% increase in compensation and benefits,\nincluding stock-based compensation, reflecting employee growth and compensation increases, a 70% increase in compute and infrastructure, and a 182%\nincrease in engineering development costs for new product introductions.\n\nThe increase in sales, general and administrative expenses for the first quarter of fiscal year 2026 were primarily driven by compensation and benefits, including\nstock-based compensation, reflecting employee growth and compensation increases.\n\nTotal Other Income (Expense), Net\n\nThree Months Ended\n\nApr 27, 2025\nApr 28, 2024\n\n$\nChange\n\nThe increase in interest income for the first quarter of fiscal year 2026 was primarily due to growth in cash, cash equivalents, and debt securities.\n\nInterest expense is comprised of coupon interest and debt discount amortization related to our notes.\n\nOther income (expense), net consists of realized or unrealized gains and losses from investments in non-marketable equity securities, publicly-held equity\nsecurities, and the impact of changes in foreign currency rates. The change in Other income (expense), net, compared to the first quarter of fiscal year 2025,\nwas primarily driven by unrealized losses in our publicly-held equity securities due to fair value volatility in our investments, particularly CoreWeave and Arm, Inc.\nRefer to Notes 5 and 6 of the Notes to Condensed Consolidated Financial Statements in Part I, Item 1 of this Quarterly Report on Form 10-Q for additional\ninformation regarding our investments in non-marketable equity securities and publicly-held equity securities.\n\nIncome Taxes\n\nIncome tax expense was $3.1 billion and $2.4 billion for the first quarter of fiscal years 2026 and 2025, respectively. Income tax as a percentage of income\nbefore income tax was an expense of 14.3% and 13.9% for the first quarter of fiscal years 2026 and 2025, respectively.\n\nThe effective tax rate increased primarily due to a lower tax benefit from stock-based compensation, partially offset by an increase in tax benefit from the foreign-\nderived intangible income deduction.\n\nGiven our current and possible future earnings, we believe that we may release the valuation allowance associated with certain state deferred tax assets in the\nnear term, which would decrease our income tax expense for the period the release is recorded. The timing and amount of the valuation allowance release could\nvary based on our assessment of all available information.\n\nRefer to Note 4 of the Notes to Condensed Consolidated Financial Statements in Part I, Item 1 of this Quarterly Report on Form 10-Q for additional information.\n\n27"
        ]
    },
    {
        "question": "NVIDIA NVLink Fusion 是什么？",
        "ground_truth": "NVIDIA NVLink Fusion 是一种新的 NVLink 芯片（chiplet silicon），它使行业能够构建半定制的 AI 基础设施，并通过 NVLink 连接到 NVIDIA 庞大的 AI 基础设施生态系统。",
        "answer": "NVIDIA NVLink Fusion 是一种新的 NVLink 芯片级硅技术，它允许行业构建半定制 AI 基础设施。这种技术通过 NVLink 连接到 NVIDIA 的庞大 AI 基础设施生态系统，NVLink 是世界上最先进和最广泛采用的计算架构。NVLink Fusion 为云服务提供商提供了一种简单的方法来扩展混合架构 AI 工厂，支持数百万个 NVIDIA GPU 和定制 ASIC，并与 NVIDIA 的 NVLink 机架级系统以及 NVIDIA 的网络平台（包括 ConnectX-8 SuperNICs、Spectrum-X 以太网和 Quantum-X800 InfiniBand 交换机）统一。NVLink Fusion 还支持与 Fujitsu 和 Qualcomm Technologies 的 CPU 集成 NVIDIA GPU，以构建高性能的 NVIDIA AI 工厂 [来源: Q1_2026_presentation, 第 22 页]。",
        "contexts": [
            "[IMAGE: Q1_2026_presentation_page22_img2.png]\n\nNVLink Fusion for Industry to Build \nSemi-Custom AI Infrastructure \nwith NVIDIA Partner Ecosystem\n\n• We unveiled NVIDIA NVLink Fusion – new NVLink chiplet silicon that\n\nlets industries build semi-custom AI infrastructure connected to \nNVIDIA’s vast AI infrastructure ecosystem through NVLink, the \nworld’s most advanced and widely adopted computing fabric\n\n• NVLink Fusion equips cloud providers with an easy path to scale out\n\nhybrid architecture AI factories to millions of NVIDIA GPUs and \ncustom ASICs, unified with NVIDIA’s NVLink rack-scale systems and \nNVIDIA networking platform featuring ConnectX-8 SuperNICs, \nSpectrum-X Ethernet and Quantum-X800 InfiniBand switches, with \nco-packaged optics available soon\n\n• MediaTek, Marvell, Alchip Technologies, Astera Labs, Synopsys and\n\nCadence are among the first to adopt NVLink Fusion, enabling \ncustom silicon scale-up to meet the requirements of demanding AI \nworkloads\n\n• Using NVLink Fusion, Fujitsu and Qualcomm Technologies CPUs can\n\nbe integrated with NVIDIA GPUs to build high-performance NVIDIA AI \nfactories",
            "[IMAGE: Q1_2026_presentation_page23_img1.png]\n\n[IMAGE: Q1_2026_presentation_page23_img2.png]\n\n[IMAGE: Q1_2026_presentation_page23_img3.png]\n\nNVIDIA, Foxconn, and Taiwan \nPartner to Build Taiwan AI Cloud \nService to Advance AI for Region’s \nScience and Industry Technology\n\n• NVIDIA and Foxconn Hon Hai Technology Group announced they are\n\ndeepening their partnership and working with the Taiwan government \nto build an AI factory supercomputer that will deliver state-of-the-art \nNVIDIA infrastructure\n\n• TSMC will be the leading customer. The Taiwan government will\n\nprovide computing credits to spur AI in education, research, startups, \nand industry. This will strengthen Taiwan’s position as a leader in \nscience and technology and fuel innovation across Foxconn’s three \ncore pillars - smart cities, electric vehicles (EVs) and manufacturing\n\n• The Foxconn AI factory will feature NVIDIA Blackwell Ultra systems,\n\nincluding the NVIDIA GB300 NVL72 rack-scale solution with NVIDIA \nNVLink, Quantum InfiniBand and Spectrum-X Ethernet networking\n\n• Taiwan National Science and Technology Council to be among the first\n\nto harness the Foxconn AI supercomputer. The system will provide AI \ncloud computing resources to the Taiwan technology ecosystem, \naccelerating AI development and adoption",
            "[IMAGE: Q1_2026_presentation_page17_img1.png]\n\n[IMAGE: Q1_2026_presentation_page17_img4.png]\n\nNVIDIA Turbocharges Agentic AI \nDevelopment with Open Reasoning AI \nModels, Platforms, and Partnerships\n\n• New open Llama Nemotron reasoning models accelerate agentic AI\n\ndevelopment for complex tasks. Post training by NVIDIA boosts \nmodel accuracy by up to 20% compared with the base model and \noptimizes inference speed by 5x compared with other leading open \nreasoning models\n\n•\nAccenture, Amdocs, Atlassian, Box, Cadence, CrowdStrike, Deloitte, IQVIA, \nMicrosoft, SAP, and ServiceNow are pioneering AI agents with NVIDIA\n\n• New NVIDIA AI Data Platform powered by NVIDIA Blackwell GPUs,\n\nNVIDIA BlueField DPUs, NVIDIA Spectrum-X networking, and NVIDIA \nDynamo enables enterprise agentic workflows. AI Data Platform \nstorage infrastructure with NVIDIA AI-Q Blueprint and NeMo \naccelerate data extraction and retrieval by up to 15x\n\n•\nDell, HPE, IBM, NetApp, Nutanix, Pure Storage, and others will deliver \nthese customized AI data platforms with NVIDIA\n\n• Oracle and NVIDIA announce a landmark integration of Oracle Cloud\n\nInfrastructure and NVIDIA AI Enterprise, making 160+ AI tools and \n100+ NIM microservices natively available on OCI"
        ]
    },
    {
        "question": "Q1 FY26 的 GAAP 净收入（Net Income）是多少？",
        "ground_truth": "2026 财年第一季度的 GAAP 净收入（Net Income）为 187.75 亿美元。",
        "answer": "根据提供的财务数据表格，英伟达在2026财年第一季度的GAAP净收入为$18,775百万 [来源: Q1_2026_presentation, 第 2 页]。",
        "contexts": [
            "[TABLE]\n|  | Non-GAAP | Acquisition-Related and Other Costs (A) | Stock-Based Compensation (B) | Other (C) | Tax Impact of Adjustments | GAAP |\n| --- | --- | --- | --- | --- | --- | --- |\n|  |  |  |  |  |  |  |\n|  | $26,858 | (123) | (64) | (3) | — |  |\n|  | 61.0% | (0.3) | (0.2) | — | — |  |\n|  | $23,275 | (160) | (1,474) | (3) | — |  |\n|  | $19,894 | (160) | (1,474) | (179) | 694 |  |\n|  | 24,611 | — | — | — | — |  |\n|  | $0.81 | (0.01) | (0.06) | (0.01) | 0.03 |  |\n[/TABLE]\n\n该表格展示了公司非GAAP和GAAP利润的对比，主要趋势包括：非GAAP利润为$26,858，主要受收购相关成本和股票补偿影响；GAAP利润为$19,894，调整税后为$24,611，显示公司盈利能力受特定成本调整影响。\n\nReconciliation of Non-GAAP to GAAP Financial Measures\n\n*Q1 FY26 included a $4.5 billion charge associated with H20 excess inventory and purchase obligations\nA. Consists of amortization of intangible assets, transaction costs, and certain compensation charges.\nB. Stock-based compensation charge was allocated to cost of goods sold, research and development expense, and sales, general and administrative expense. \nC. Other consists of legal settlement cost, losses from non-marketable equity securities and publicly-held equity securities, net, and interest expense related to amortization of debt discount.",
            "[IMAGE: Q1_2026_presentation_page6_img1.png]\n\n[TABLE]\n|  | GAAP |  |  | Non-GAAP |  |  |\n| --- | --- | --- | --- | --- | --- | --- |\n|  | Q1 FY26* | Y/Y | Q/Q | Q1 FY26* | Y/Y | Q/Q |\n| Revenue | $44,062 | +69% | +12% | $44,062 | +69% | +12% |\n| Gross Margin* | 60.5% | -17.9 pts | -12.5 pts | 61.0% | -17.9 pts | -12.5 pts |\n| Operating Income | $21,638 | +28% | -10% | $23,275 | +29% | -9% |\n| Net Income | $18,775 | +26% | -15% | $19,894 | +31% | -10% |\n| Diluted EPS | $0.76 | +27% | -15% | $0.81 | +33% | -9% |\n| Cash Flow from Ops | $27,414 | +79% | +65% | $27,414 | +79% | +65% |\n[/TABLE]\n\n该表格展示了公司Q1 FY26的财务数据，GAAP和Non-GAAP收入均增长69%，但GAAP毛利率下降17.9个百分点，Non-GAAP毛利率下降17.9个百分点。GAAP和Non-GAAP运营收入和净利润均增长，但GAAP净利润下降15%，Non-GAAP净利润下降10%。GAAP和Non-GAAP每股收益均增长，但GAAP每股收益下降15%，Non-GAAP每股收益下降9%。经营活动产生的现金流大幅增长79%。\n\nQ1 FY26 Financial Summary\n\nAll dollar figures are in millions other than EPS. Refer to Appendix for reconciliation of Non-GAAP measures.\n\n*Q1 FY26 included a $4.5 billion charge associated with H20 excess inventory and purchase obligations",
            "Q1 FY26\nEarnings Summary"
        ]
    },
    {
        "question": "公司是否计划在美国本土制造 AI 超级计算机？",
        "ground_truth": "是的，NVIDIA 正在与其制造合作伙伴合作，设计和建造工厂，这将是首次在美国本土完整生产 NVIDIA AI 超级计算机。",
        "answer": "根据提供的资料，英伟达计划在美国本土制造 AI 超级计算机。NVIDIA 正在与制造合作伙伴合作，设计和建造工厂，这些工厂将首次完全在美国生产 NVIDIA AI 超级计算机。英伟达已与其合作伙伴委托了超过一百万平方英尺的制造空间，在亚利桑那州制造和测试 NVIDIA Blackwell 芯片，在德克萨斯州制造 AI 超级计算机。这些信息来源于 Q1_2026_presentation 的第 25 页 [来源: Q1_2026_presentation, 第 25 页]。",
        "contexts": [
            "[IMAGE: Q1_2026_presentation_page25_img1.png]\n\n[IMAGE: Q1_2026_presentation_page25_img2.png]\n\n[IMAGE: Q1_2026_presentation_page25_img3.png]\n\n[IMAGE: Q1_2026_presentation_page25_img4.png]\n\nNVIDIA to Manufacture American-\nMade AI Supercomputers in US for \nFirst Time\n\n• NVIDIA is working with its manufacturing partners to design and\n\nbuild factories that, for the first time, will produce NVIDIA AI \nsupercomputers entirely in the US\n\n• With partners, the company has commissioned more than a million\n\nsquare feet of manufacturing space to build and test NVIDIA \nBlackwell chips in Arizona and AI supercomputers in Texas\n\n• Within the next four years, NVIDIA plans to produce up to half a\n\ntrillion dollars of AI infrastructure in the United States through \npartnerships with TSMC, Foxconn, Wistron, Amkor and SPIL\n\n• Manufacturing NVIDIA AI chips and supercomputers for American AI\n\nfactories is expected to create hundreds of thousands of jobs and \ndrive trillions of dollars in economic security over the coming decades",
            "[IMAGE: Q1_2026_presentation_page23_img1.png]\n\n[IMAGE: Q1_2026_presentation_page23_img2.png]\n\n[IMAGE: Q1_2026_presentation_page23_img3.png]\n\nNVIDIA, Foxconn, and Taiwan \nPartner to Build Taiwan AI Cloud \nService to Advance AI for Region’s \nScience and Industry Technology\n\n• NVIDIA and Foxconn Hon Hai Technology Group announced they are\n\ndeepening their partnership and working with the Taiwan government \nto build an AI factory supercomputer that will deliver state-of-the-art \nNVIDIA infrastructure\n\n• TSMC will be the leading customer. The Taiwan government will\n\nprovide computing credits to spur AI in education, research, startups, \nand industry. This will strengthen Taiwan’s position as a leader in \nscience and technology and fuel innovation across Foxconn’s three \ncore pillars - smart cities, electric vehicles (EVs) and manufacturing\n\n• The Foxconn AI factory will feature NVIDIA Blackwell Ultra systems,\n\nincluding the NVIDIA GB300 NVL72 rack-scale solution with NVIDIA \nNVLink, Quantum InfiniBand and Spectrum-X Ethernet networking\n\n• Taiwan National Science and Technology Council to be among the first\n\nto harness the Foxconn AI supercomputer. The system will provide AI \ncloud computing resources to the Taiwan technology ecosystem, \naccelerating AI development and adoption",
            "In addition to controls targeting D:4 and D:5 countries, the USG has also imposed worldwide export controls impacting our products, and may impose additional\ncontrols in the future. On January 15, 2025, the USG published the “AI Diffusion” IFR in the Federal Register. After a 120-day delayed compliance period, the\nIFR would have imposed a worldwide licensing requirement on all products classified under Export Control Classification Numbers, or ECCNs, 3A090.a,\n4A090.a, or corresponding .z ECCNs, including all related software and technology. The licensing requirement would have applied to our most popular data\ncenter products, such as our H200 and GB200. The AI Diffusion IFR would have divided the world into three tiers, relegating most countries to “Tier 2” status,\nand would have created a complex and burdensome scheme for licensing approvals.\n\nIn May 2025, the USG announced that it would rescind the AI Diffusion IFR and implement a replacement rule. The scope, timing, and requirements of the\nforthcoming rule remain uncertain. The replacement rule may impose new restrictions on our products or operations and/or add license requirements that could\nhave a material impact on our business, operating results, and financial condition.\n\nOur competitive position has been harmed by export controls, and our competitive position and future results may be further harmed, over the long term, if there\nare further changes in the USG’s export controls, including further expansion of the geographic, customer, or product scope of the controls, if customers\npurchase product from competitors, if customers develop their own internal solution, if we are unable to provide contractual warranty or other extended service\nobligations, if the USG does not grant licenses in a timely manner or denies licenses to significant customers or if we incur significant transition costs. Even if the\nUSG grants any requested licenses, the licenses may be temporary or impose burdensome conditions that we or our customers or end users cannot or choose\nnot to fulfill. The licensing requirements may benefit certain of our competitors, as the licensing process will make our pre-sale and post-sale technical support\nefforts more cumbersome and less certain and encourage customers in China to pursue alternatives to our products, including semiconductor suppliers based in\nChina, Europe, and Israel.\n\nGiven the increasing strategic importance of AI and rising geopolitical tensions, the USG has changed and may again change the export control rules at any time\nand further subject a wider range of our products to export restrictions and licensing requirements, negatively impacting our business and financial results. In the\nevent of such change, we may be unable to sell our inventory of such products and may be unable to develop replacement products not subject to the licensing\nrequirements.\n\nFor example, the USG has already imposed conditions to limit the ability of foreign firms to create and offer as a service large-scale GPU clusters, for example\nby imposing license conditions on the use of products to be exported to certain countries, and may impose additional conditions such as requiring chip tracking\nand throttling mechanisms that could disable or impair GPUs if certain events, including unauthorized system configuration, use, or location, are detected. Such\ngovernment mandates in chip designs could introduce system vulnerabilities and expose us to significant risk and potential liability, negatively impact demand for\nour products, and could have a material impact on our business, operating results, and financial condition.\n\nOpen-source foundation models are rapidly growing in popularity with developers worldwide. Any regulatory control or other restriction that limits our ability to\nprovide products and services that support third-party applications and models, including applications built on foundation models originating in China such as\nDeepSeek or Qwen, could have a material impact on our business, operating results, and financial condition.\n\nThe USG has already imposed export controls restricting certain gaming GPUs, and if the USG expands such controls to restrict additional gaming products, it\nmay disrupt a significant portion of our supply and distribution chain and negatively impact sales of such products to markets outside China, including the U.S.\nand Europe. In addition, as the performance of the gaming GPUs increases over time, export controls may have a greater impact on our ability to compete in\nmarkets subject to those controls. Export controls may disrupt our supply and distribution chain for a substantial portion of our products, which are warehoused\nin and distributed from Hong Kong.\n\nExport controls restricting our ability to sell data center GPUs may also negatively impact demand for our networking products used in servers containing our\nGPUs. The USG may also impose export controls on our networking products, such as high-speed network interconnects, to limit the ability of downstream\nparties to create large clusters for frontier model training.\n\nAny new control that impacts a wider range of our products would likely have a disproportionate impact on NVIDIA and may disadvantage us against certain of\nour competitors that sell chips that are outside the scope of such control. Export controls have already and may in the future encourage customers outside China\nand other impacted regions to “design-out” certain U.S. semiconductors from their products to reduce the compliance burden and risk, and to ensure that they\nare able to serve markets worldwide. Export controls have already encouraged and may in the future encourage overseas governments to request that our\ncustomers purchase from our competitors rather than NVIDIA or other U.S. firms, harming our business, market position, and financial results.\n\nAs a result, export controls may negatively impact demand for our products and services not only in China, but also in other markets, such as Europe, Latin\nAmerica, and Southeast Asia. Export controls increase the risk of investing in U.S.\n\n35"
        ]
    },
    {
        "question": "汽车（Automotive）业务 Q1 FY26 的收入是多少？",
        "ground_truth": "2026 财年第一季度，汽车（Automotive）业务收入为 5.67 亿美元，同比增长 72%。",
        "answer": "根据提供的财务报告，英伟达在2026财年第一季度的汽车业务收入为5.67亿美元。这一数据在报告的第1214页有所提及 [来源: Q1_2026_presentation, 第 1214 页]。",
        "contexts": [
            "Q1 FY26\nEarnings Summary",
            "[0s->5s]Good afternoon. My name is Sarah and I will be your conference operator today.\n[5s->13s]At this time, I would like to welcome everyone to NVIDIA's first quarter fiscal 2026 financial results conference call.\n[13s->17s]All lines have been placed on mute to prevent any background noise.\n[17s->21s]After the speaker's remarks, there will be a question and answer session.\n[21s->27s]If you would like to ask a question during this time, simply press star one on your telephone keypad.\n[27s->32s]If you would like to withdraw your question, please press star one again. Thank you.\n[32s->36s]Toshia Hari, you may begin your conference.\n[36s->44s]Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the first quarter of fiscal 2026.\n[44s->54s]With me today from NVIDIA are Jensen Wong, President and Chief Executive Officer, and Collette Press, Executive Vice President and Chief Financial Officer.\n[54s->60s]I'd like to remind you that our call is being webcast live on NVIDIA's investor relations website.\n[60s->68s]The webcast will be available for replay until the conference call to discuss our financial results for the second quarter of fiscal 2026.\n[68s->76s]The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent.\n[76s->81s]During this call, we may make forward looking statements based on current expectations.\n[81s->88s]These are subject to a number of significant risks and uncertainties, and our actual results may differ materially.\n[88s->96s]For a discussion of factors that could affect our future financial results in business, please refer to the disclosure in today's earnings release,\n[96s->104s]our most recent forms 10-K and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission.\n[105s->112s]All our statements are made as of today, May 28, 2025, based on information currently available to us.\n[112s->118s]Except as required by law, we assume no obligation to update any such statements.\n[118s->122s]During this call, we will discuss non-GAAP financial measures.\n[122s->130s]You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website.\n[130s->133s]With that, let me turn the call over to Collette.\n[133s->135s]Thank you, Toshio.\n[135s->148s]We delivered another strong quarter with revenue of $44 billion, up 69% year-over-year, exceeding our outlook in what proved to be a challenging operating environment.\n[148s->153s]Data center revenue of $39 billion grew 73% year-on-year.\n[153s->161s]AI workloads have transitioned strongly to inference, and AI factory build-outs are driving significant revenue.\n[161s->165s]Our customers' commitments are firm.\n[165s->176s]On April 9, the U.S. government issued new export controls on H20, our data center GPU designed specifically for the China market.\n[176s->181s]We sold H20 with the approval of the previous administration.\n[181s->187s]Although our H20 has been in the market for over a year and does not have a market outside of China,\n[187s->195s]the new export controls on H20 did not provide a grace period to allow us to sell through our inventory.\n[195s->204s]In Q1, we recognized $4.6 billion in H20 revenue, which occurred prior to April 9,\n[204s->216s]but also recognized a $4.5 billion charge as we wrote down inventory and purchase obligations tied to orders we had received prior to April 9.\n[216s->226s]We were unable to ship $2.5 billion in H20 revenue in the first quarter due to the new export controls.\n[226s->234s]The $4.5 billion charge was less than what we initially anticipated as we were able to reuse certain materials.\n[234s->244s]We are still evaluating our limited options to supply data center compute products compliant with the U.S. government's revised export control rules,\n[244s->252s]losing access to the China AI accelerator market, which we believe will grow to nearly $50 billion,\n[252s->262s]would have a material adverse impact on our business going forward and benefit our foreign competitors in China and worldwide.\n[262s->271s]Our Blackwell ramp, the fastest in our company's history, drove a 73% year-on-year increase in data center revenue.\n[271s->280s]Blackwell contributed nearly 70% of data center compute revenue in the quarter, with a transition from Hopper nearly complete.\n[280s->293s]The introduction of GB200 NVL was a fundamental architectural change to enable data center scale workloads and to achieve the lowest cost per inference token.\n[293s->305s]While these systems are complex to build, we have seen a significant improvement in manufacturing yields and rack shipments are moving to strong rates to end customers.\n[305s->315s]GB200 NVL racks are now generally available for modern builders, enterprises, and sovereign customers to develop and deploy AI.\n[315s->332s]On average, major hyperscalers are each deploying nearly 1,000 NVL 72 racks or 72,000 Blackwell GPUs per week and are on track to further ramp output this quarter.\n[332s->346s]Microsoft, for example, has already deployed tens of thousands of Blackwell GPUs and is expected to ramp to hundreds of thousands of GB200s with OpenAI as one of its key customers.\n[346s->356s]Key learnings from the GB200 ramp will allow for a smooth transition to the next phase of our product roadmap, Blackwell Ultra.\n[357s->370s]Sampling of GB300 systems began earlier this month at the major CSPs, and we expect production shipments to commerce later this quarter.\n[370s->381s]GB300 will leverage the same architecture, same physical footprint, and the same electrical and mechanical specifications as GB200.\n[381s->394s]The GB300 drop-in design will allow CSPs to seamlessly transition their systems and manufacturing used for GB200 while maintaining high yields.\n[394s->407s]B300 GPUs with 50% more HBM will deliver another 50% increase in dense FP4 inference compute performance compared to the B200.\n[407s->421s]We remain committed to our annual product cadence, with our roadmap extending through 2028 tightly aligned with the multiple-year planning cycles of our customers.\n[421s->432s]We are witnessing a sharp jump in inference demand. OpenAI, Microsoft, and Google are seeing a step-function leap in token generation.\n[432s->441s]Microsoft processed over 100 trillion tokens in Q1, a five-fold increase on a year-over-year basis.\n[441s->453s]This exponential growth in Azure OpenAI is representative of strong demand for Azure AI Foundry as well as other AI services across Microsoft's platform.\n[453s->471s]Inference-serving startups are now serving models using B200, tripling their token generation rate and corresponding revenues for high-value reasoning models such as DeepSeq R1, as reported by Artificial Analysis.\n[471s->485s]NVIDIA Dynamo on Blackwell NVL72 turbocharges AI inference throughput by 30x for the new reasoning models sweeping the industry.\n[485s->497s]Developer engagements increased with adoption ranging from LLM providers such as Perplexity to financial services institutions such as Capital One,\n[497s->503s]who reduced agentic chatbox latency by 5x with Dynamo.\n[503s->511s]In the latest MLPerf inference results, we submitted our first results using GB200 NVL72,\n[511s->523s]delivering up to 30x higher inference throughput compared to our 8 GPU H200 submission on the challenging Llama 3.1 benchmark.\n[523s->535s]This feat was achieved through a combination of tripling the performance per GPU as well as 9x more GPUs, all connected on a single NVLink domain.\n[535s->547s]And while Blackwell is still early in its lifecycle, software optimizations have already improved its performance by 1.5x in the last month alone.\n[547s->555s]We expect to continue improving the performance of Blackwell through its operational life as we have done with Hopper and Amper.\n[555s->560s]For example, we increased the inference performance of Hopper by 4x over two years.\n[560s->568s]This is the benefit of NVIDIA's programmable CUDA architecture and rich ecosystem.\n[568s->578s]The pace and scale of AI factory deployments are accelerating with nearly 100 NVIDIA-powered AI factories in flight this quarter,\n[578s->588s]a two-fold increase year over year, with the average number of GPUs powering each factory also doubling in the same period.\n[588s->593s]And more AI factory projects are starting across industries and geographies.\n[593s->605s]NVIDIA's full-stack architecture is underpinning AI factory deployments as industry leaders like AT&T, BYD, Capital One, Foxconn, MediaTek, and Telenor\n[605s->614s]are strategically vital sovereign clouds like those recently announced in Saudi Arabia, Taiwan, and the UAE.\n[614s->623s]We have a line of sight to projects requiring tens of gigawatts of NVIDIA AI infrastructure in the not-too-distant future.\n[623s->639s]The transition from generative to agentic AI, AI capable of perceiving, reasoning, planning, and acting, will transform every industry, every company, and country.\n[639s->650s]We envision AI agents as a new digital workforce capable of handling tasks ranging from customer service to complex decision-making processes.\n[650s->660s]We introduced the LAMA Nemotron family of open reasoning models designed to supercharge identical AI platforms for enterprises.\n[660s->672s]Built on the LAMA architecture, these models are available as NIMS or NVIDIA inference microservices with multiple sizes to meet diverse deployment needs.\n[672s->681s]Our post-training enhancements have yielded a 20% accuracy boost and a 5x increase in inference speed.\n[681s->690s]Leading platform companies including Accenture, Cadence, Deloitte, and Microsoft are transforming work with our reasoning models.\n[690s->702s]NVIDIA NEMO microservices are generally available across industries that are being leveraged by leading enterprises to build, optimize, and scale AI applications.\n[702s->711s]With NEMO, Cisco increased model accuracy by 40% and improved response time by 10x in its code assistant.\n[711s->719s]NASDAQ realized a 30% improvement in accuracy and response time in its AI platform's search capabilities.\n[719s->727s]And Shell's Custom LLM achieved a 30% increase in accuracy when trained with NVIDIA NEMO.\n[727s->735s]NEMO's parallelism techniques accelerated model training time by 20% when compared to other frameworks.\n[735s->757s]We also announced a partnership with Yum Brands, the world's largest restaurant company, to bring NVIDIA AI to 500 of its restaurants this year and expanding to 61,000 restaurants over time to streamline order taking, optimize operations, and enhance service across its restaurants.\n[757s->779s]For AI-powered cybersecurity, leading companies like Checkpoint, CloudStrike, and Palo Alto Networks are using NVIDIA's AI security and software stack to build, optimize, and secure agentic workflows, with CloudStrike realizing 2x faster detection triage with 50% less compute cost.\n[780s->790s]Moving to networking, sequential growth in networking resumed in Q1, with revenue up 64% quarter-over-quarter to $5 billion.\n[790s->798s]Our customers continue to leverage our platform to efficiently scale up and scale out AI factory workloads.\n[798s->813s]We created the world's fastest switch, NVLink. For scale up, our NVLink compute fabric in its fifth generation offers 14x the bandwidth of PCIe Gen 5.\n[813s->825s]NVLink 72 carries 130 TB per second of bandwidth in a single rack, equivalent to the entirety of the world's peak internet traffic.\n[825s->833s]NVLink is a new growth vector and is off to a great start, with Q1 shipments exceeding $1 billion.\n[833s->848s]At Computex, we announced NVLink Fusion. Hyperscale customers can now build semi-custom CCUs and accelerators that connect directly to the NVIDIA platform with NVLink.\n[848s->868s]We are now enabling key partners, including ASIC providers such as MediaTek, Marvell, Alchip Technologies, and Astera Labs, as well as CPU suppliers such as Fujitsu and Qualcomm, to leverage NVLink Fusion to connect our respective ecosystems.\n[868s->878s]For scale out, our enhanced Ethernet offerings deliver the highest throughput, lowest latency networking for AI.\n[878s->886s]SpectrumX posted strong sequential and year-on-year growth and is now annualizing over $8 billion in revenue.\n[886s->896s]Adoption is widespread across major CSPs and consumer internet companies, including CoreWe, Microsoft Azure, Oracle Cloud, and XAI.\n[896s->903s]This quarter, we added Google Cloud and Meta to the growing list of SpectrumX customers.\n[903s->913s]We introduced SpectrumX and QuantumX, silicon photonics switches featuring the world's most advanced co-packaged optics.\n[913s->933s]These platforms will enable next level AI factory scaling to millions of GPUs through the increasingly power efficiency by 3.5x and network resiliency by 10x while accelerating customer time to market by 1.3x.\n[933s->938s]Transitioning to a quick summary of our revenue by geography.\n[938s->949s]China, as a percentage of our data center revenue, was slightly below our expectations and down sequentially due to H20 export licensing controls.\n[949s->955s]For Q2, we expect a meaningful decrease in China data center revenue.\n[955s->971s]As a reminder, while Singapore represented nearly 20% of our Q1 billed revenue, as many of our large customers use Singapore for centralized invoicing, our products are almost always sold elsewhere.\n[971s->984s]Note that over 99% of H100, H200, and Blackwell data center compute revenue billed to Singapore was for orders from US based customers.\n[984s->996s]Moving to gaming and AI PCs. Gaming revenue was a record 3.8 billion, increasing 48% sequentially and 42% year on year.\n[996s->1012s]Strong adoption by gamers, creatives, and AI enthusiasts have made Blackwell our fastest ramp ever. Against a backdrop of robust demand, we greatly improved our supply and availability in Q1 and expect to continue these efforts in Q2.\n[1012s->1017s]AI is transforming PC and creator and gamers.\n[1017s->1035s]With a 100 million user installed base, GeForce represents the largest footprint for PC developers. This quarter, we added to our AI PC laptop offerings, including models capable of running Microsoft's Copilot Plus.\n[1035s->1049s]This past quarter, we brought Blackwell architecture to mainstream gaming with its launch of GeForce RTX 5060 and 5060 Ti, starting at just $299.\n[1049s->1061s]The RTX 5060 also debuted in laptops starting at $1,099. These systems that double the frame rate and slash latency.\n[1061s->1070s]These GeForce RTX 5060 and 5060 Ti desktop GPUs and laptops are now available.\n[1070s->1090s]In console gaming, the recently unveiled Nintendo Switch 2 leverages Nvidia's neural rendering and AI technologies, including next generation custom RTX GPUs with DLSS technology, to deliver a giant leap in gaming performance to millions of players worldwide.\n[1090s->1100s]Nintendo has shipped over 150 million Switch consoles to date, making it one of the most successful gaming systems in history.\n[1100s->1108s]Moving to pro visualization. Revenue of $509 million was flat sequentially and up 19% year on year.\n[1108s->1120s]Tariff related uncertainty temporarily impacted Q1 systems and demand for AI workstations is strong and we expect sequential revenue growth to resume in Q2.\n[1120s->1130s]Nvidia DGX Spark and Station revolutionize personal computing by putting the power of an AI supercomputer in a desktop form factor.\n[1130s->1143s]DGX Spark delivers up to one petaflop of AI compute while DGX Station offers an incredible 20 petaflops and is powered by the GB300 Superchip.\n[1143s->1150s]DGX Spark will be available in Calendar Q3 and DGX Station later this year.\n[1150s->1163s]We have deepened Omniverse's integration and adoption into some of the world's leading software platforms, including Databricks, SAP, and Schneider Electric.\n[1163s->1178s]New Omniverse blueprints such as Mega for at scale robotic fleet management are being leveraged in Keyon Group, Pegatron, Accenture, and other leading companies to enhance industrial operations.\n[1178s->1189s]At Confitex, we showcased Omniverse's great traction with technology manufacturing leaders, including TSMC, Quanta, Foxconn, Pegatron.\n[1189s->1194s]Using Omniverse, TSMC saves months in work by designing fabs virtually.\n[1194s->1205s]Foxconn accelerates thermal simulations by 150x and Pegatron reduced assembly line defects rates by 67%.\n[1205s->1214s]Lastly, with our automotive group, revenue was $567 million, down 1% sequentially but up 72% year on year.\n[1214s->1222s]Year on year growth was driven by the ramp of self-driving across a number of customers and robust end demand for NEVs.\n[1222s->1232s]We are partnering with GM to build the next-gen vehicles, factories, and robots using NVIDIA AI, simulation, and accelerated computing.\n[1232s->1241s]And we are now in production with our full-stack solution for Mercedes-Benz, starting with the new CLA, hitting roads in the next few months.\n[1241s->1254s]We announced Isaac, Groot, and won the world's first open fully customizable foundation model for humanoid robots, enabling generalized reasoning and skill development.\n[1254s->1260s]We also launched new open NVIDIA Cosmo World Foundation models.\n[1260s->1268s]Leading companies include 1X, Agility Robots, Robotics, Figure AI, Uber, and Wabi.\n[1268s->1284s]We've begun integrating Cosmos into their operations for synthetic data generation, while Agility Robotics, Boston Dynamics, and XPen Robotics are harnessing Isaac simulation to advance their humanoid efforts.\n[1284s->1300s]GE Healthcare is using the new NVIDIA Isaac platform for healthcare simulation, built on NVIDIA Omniverse and using NVIDIA Cosmos, the platform speeds development of robotic imaging and surgery systems.\n[1300s->1314s]The era of robotics is here. Billions of robots, hundreds of millions of autonomous vehicles, and hundreds of thousands of robotic factories and warehouses will be developed.\n[1314s->1335s]All right, moving to the rest of the P&L. Gap gross margins and non-gap gross margins were 60.5% and 61%, respectively. Excluding the $4.5 billion charge, Q1 non-gap gross margins would have been 71.3%, slightly above our outlook at the beginning of the quarter.\n[1335s->1346s]Sequentially, gap operating expenses were up 7% and non-gap operating expenses were up 6%, reflecting higher compensation and employee growth.\n[1346s->1355s]Our investments include expanding our infrastructure capabilities and AI solutions, and we plan to grow these investments throughout the fiscal year.\n[1355s->1370s]In Q1, we returned a record $14.3 billion to shareholders in the form of share repurchases and cash dividends. Our capital return program continues to be a key element of our capital allocation strategy.\n[1370s->1384s]Let me turn to the outlook for the second quarter. Total revenue is expected to be $45 billion, plus or minus 2%. We expect modest sequential growth across all of our platforms.\n[1384s->1403s]In data center, we anticipate the continued ramp of Blackwell to be partially offset by a decline in China revenue. Note, our outlook reflects a loss in H20 revenue of approximately $8 billion for the second quarter.\n[1403s->1420s]Gap and non-gap gross margins are expected to be 71.8% and 72% respectively, plus or minus 50 basis points. We expect better Blackwell profitability to drive modest sequential improvement in gross margins.\n[1420s->1427s]We are continuing to work towards achieving gross margins in the mid-70s range late this year.\n[1427s->1444s]Gap and non-gap operating expenses are expected to be approximately $5.7 billion and $4 billion respectively, and we continue to expect full-year fiscal year 26 operating expense growth to be in the mid-30% range.\n[1444s->1457s]Gap and non-gap other income and expenses are expected to be an income of approximately $450 million, excluding gays and losses from non-marketable and publicly held equity securities.\n[1457s->1468s]Gap and non-gap tax rates are expected to be 16.5%, plus or minus 1%, excluding any discrete items.\n[1468s->1480s]Further financial details are included in the CFO commentary and other information available on our IR website, including a new financially information AI agent.\n[1480s->1488s]Let me highlight upcoming events for the financial community. We will be at the B of A Global Technology Conference in San Francisco on June 4.\n[1488s->1500s]The Rosenblatt Virtual AI Summit and NASDAQ Investor Conference in London on June 10, and GTC Paris at Viva Tech on June 11 in Paris.\n[1500s->1511s]We look forward to seeing you at these events. Our earnings call to discuss the results of our second quarter of fiscal 2026 is scheduled for August 27.\n[1511s->1517s]Well now, let me turn it over to Jensen to make some remarks.\n[1517s->1526s]Thanks, Collette. We've had a busy and productive year. Let me share my perspective on some topics we're frequently asked.\n[1526s->1536s]On export control, China is one of the world's largest AI markets and a springboard to global success.\n[1536s->1543s]With half of the world's AI researchers based there, the platform that wins China is positioned to lead globally.\n[1543s->1551s]Today, however, the $50 billion China market is effectively closed to U.S. industry.\n[1551s->1560s]The H20 export ban ended our Hopper data center business in China. We cannot reduce Hopper further to comply.\n[1561s->1568s]As a result, we are taking a multi-billion dollar write-off on inventory that cannot be sold or repurposed.\n[1568s->1575s]We are exploring limited ways to compete, but Hopper is no longer an option.\n[1575s->1584s]China's AI moves on with or without U.S. chips. It has to compute to train and deploy advanced models.\n[1585s->1590s]The question is not whether China will have AI. It already does.\n[1590s->1596s]The question is whether one of the world's largest AI markets will run on American platforms.\n[1596s->1604s]Shielding Chinese chipmakers from U.S. competition only strengthens them abroad and weakens America's position.\n[1604s->1609s]Export restrictions have spurred China's innovation in scale.\n[1609s->1616s]The AI race is not just about chips. It's about which stack the world runs on.\n[1616s->1624s]As that stack grows to include 6G and quantum, U.S. global infrastructure leadership is at stake.\n[1624s->1630s]The U.S. has based its policy on the assumption that China cannot make AI chips.\n[1630s->1635s]That assumption was always questionable, and now it's clearly wrong.\n[1635s->1639s]China has enormous manufacturing capability.\n[1639s->1645s]In the end, the platform that wins the AI developers wins AI.\n[1645s->1653s]Export controls should strengthen U.S. platforms, not drive half of the world's AI talent to rivals.\n[1654s->1657s]On DeepSeek.\n[1657s->1665s]DeepSeek and QN from China are among the best open source AI models.\n[1665s->1671s]Released freely, they've gained traction across the U.S., Europe, and beyond.\n[1671s->1680s]DeepSeek R1, like ChatGPT, introduced reasoning AI that produces better answers the longer it thinks.\n[1680s->1690s]Reasoning AI enables step-by-step problem solving, planning, and tool use, turning models into intelligent agents.\n[1690s->1705s]Reasoning is compute intensive, requires hundreds to thousands of times more tokens per task than previous one-shot inference.\n[1705s->1711s]Reasoning models are driving a step function surge in inference demand.\n[1711s->1721s]AI scaling laws remain firmly intact, not only for training, but now inference, too, requires massive scale compute.\n[1721s->1726s]DeepSeek also underscores the strategic value of open source AI.\n[1726s->1734s]When popular models are trained and optimized on U.S. platforms, it drives usage, feedback, and continuous improvement.\n[1734s->1737s]Reinforcing American leadership across the stack.\n[1737s->1743s]U.S. platforms must remain the preferred platform for open source AI.\n[1743s->1749s]That means supporting collaboration with top developers globally, including in China.\n[1749s->1757s]America wins when models like DeepSeek and QN runs best on American infrastructure.\n[1757s->1760s]Regarding onshore manufacturing.\n[1760s->1770s]President Trump has outlined a bold vision to reshore advanced manufacturing, create jobs, and strengthen national security.\n[1770s->1774s]Future plants will be highly computerized in robotics.\n[1774s->1776s]We share this vision.\n[1776s->1783s]TSMC is building six fabs and two advanced packaging plants in Arizona to make chips for Nvidia.\n[1783s->1789s]Process qualification is underway, with volume production expected by year end.\n[1790s->1798s]Spill and Amcor are also investing in Arizona, constructing packaging, assembly, and test facilities.\n[1798s->1806s]In Houston, we're partnering with Foxconn to construct a million-square-foot factory to build AI supercomputers.\n[1806s->1811s]Wistron is building a similar plant in Fort Worth, Texas.\n[1811s->1817s]To encourage and support these investments, we've made substantial long-term purchase commitments.\n[1817s->1821s]A deep investment in America's AI manufacturing future.\n[1823s->1827s]Our goal, from chip to supercomputer.\n[1827s->1830s]Built in America within a year.\n[1830s->1840s]Each GB200 and Beelink 72 racks contains 1.2 million components and weighs nearly two tons.\n[1840s->1844s]No one has produced supercomputers on this scale.\n[1844s->1847s]Our partners are doing an extraordinary job.\n[1849s->1857s]On AI diffusion rule, President Trump rescinded the AI diffusion rule, calling it counterproductive,\n[1857s->1864s]and proposed a new policy to promote US AI tech with trusted partners.\n[1864s->1868s]On his Middle East tour, he announced historic investments.\n[1868s->1875s]I was honored to join him in announcing a 500-megawatt AI infrastructure project in Saudi Arabia\n[1875s->1880s]and a 5-gigawatt AI campus in the UAE.\n[1880s->1883s]President Trump wants US tech to lead.\n[1883s->1887s]The deals he announced are wins for America.\n[1887s->1895s]Creating jobs, advancing infrastructure, generating tax revenue, and reducing the US trade deficit.\n[1895s->1902s]The US will always be Nvidia's largest market and home to the largest install base of our infrastructure.\n[1902s->1909s]Every nation now sees AI as core to the next industrial revolution.\n[1909s->1915s]A new industry that produces intelligence and essential infrastructure for every economy.\n[1915s->1922s]Countries are racing to build national AI platforms to elevate their digital capabilities.\n[1922s->1930s]At Computex, we announced Taiwan's first AI factory in partnership with Foxconn and the Taiwan government.\n[1930s->1936s]Last week, I was in Sweden to launch its first national AI infrastructure.\n[1936s->1947s]Japan, Korea, India, Canada, France, the UK, Germany, Italy, Spain, and more are now building national AI factories\n[1947s->1951s]to empower startups, industries, and societies.\n[1951s->1956s]Sovereign AI is a new growth engine for Nvidia.\n[1958s->1960s]Toshia, back to you. Thank you.\n[1960s->1966s]Operator, we will now open the call for questions. Would you please pull for questions?\n[1966s->1974s]Thank you. At this time, I would like to remind everyone in order to ask a question, press star, then the number one on your telephone keypad.\n[1974s->1978s]We'll pause for just a moment to compile the Q&A roster.\n[1982s->1988s]Your first question comes from the line of Joe Moore with Morgan Stanley. Your line is open.\n[1990s->1998s]Great. Thank you. You guys have talked about this scaling up of inference around reasoning models for at least a year now,\n[1998s->2002s]and we've really seen that come to fruition as you talked about. We've heard it from your customers.\n[2002s->2007s]Can you give us a sense for how much of that demand you're able to serve?\n[2007s->2010s]And give us a sense for maybe how big the inference business is for you guys.\n[2010s->2016s]And do we need full-on NVL-72 rack-scale solutions for reasoning inference going forward?\n[2019s->2026s]Well, we would like to serve all of it. And I think we're on track to serve most of it.\n[2026s->2038s]Grace Blackwell NVLink-72 is the ideal engine today, the ideal computer thinking machine, if you will, for reasoning AI.\n[2038s->2046s]There's a couple of reasons for that. The first reason is that the token generation amount,\n[2046s->2059s]the number of tokens reasoning goes through is a hundred, a thousand times more than a one-shot chatbot.\n[2059s->2069s]You know, it's essentially thinking to itself, breaking down a problem step by step. It might be planning multiple paths to an answer.\n[2069s->2083s]It could be using tools, reading PDFs, reading web pages, watching videos, and then producing a result, an answer.\n[2083s->2089s]The longer it thinks, the better the answer, the smarter the answer is.\n[2090s->2101s]And so what we would like to do and the reason why Grace Blackwell was designed to give such a giant step up in inference performance\n[2101s->2106s]is so that you could do all this and still get a response as quickly as possible.\n[2106s->2117s]Compared to Hopper, Grace Blackwell is some 40 times higher speed and throughput compared.\n[2117s->2132s]And so this is going to be a huge, huge benefit and driving down the cost while improving the quality of response with excellent quality of service at the same time.\n[2132s->2139s]So that's the fundamental reason. That was the core driving reason for Grace Blackwell NVLink-72.\n[2139s->2151s]Of course, in order to do that, we had to reinvent, literally redesign the entire way that these supercomputers are built.\n[2151s->2161s]But now we're in full production. It's going to be exciting. It's going to be incredibly exciting.\n[2162s->2171s]The next question comes from Vivek Arya with Bank of America Securities. Your line is open.\n[2171s->2176s]Thanks for the question. Just a clarification for Colette first.\n[2176s->2184s]So on the China impact, I think previously it was mentioned that at about $15 billion, so you had the $8 billion in Q2.\n[2184s->2190s]So is there still some left as a headwind for the remaining quarters, just Colette, how to model that?\n[2190s->2201s]And then question, Jensen, for you. Back at GTC, you had outlined a path towards almost a trillion dollars of AI spending over the next few years.\n[2201s->2212s]Where are we in that build out? And do you think it's going to be uniform that you will see every spender, whether it's ESP, sovereigns, enterprises or build out?\n[2212s->2215s]Should we expect some periods of digestion in between?\n[2215s->2221s]What are your customer discussions telling you about how to model growth for next year?\n[2223s->2232s]Yes, Vivek. Thanks so much for the question regarding H20. Yes, we recognized 4.6 H20 in Q1.\n[2232s->2241s]We were unable to ship $2.5 billion. So the total for Q1 should have been $7 billion.\n[2241s->2248s]When we look at our Q2, our Q2 is going to be meaningfully down in terms of China data center revenue.\n[2248s->2258s]And we had highlighted in terms of the amount of orders that we had planned for H20 and Q2, and that was $8 billion.\n[2258s->2266s]Now, going forward, we did have other orders going forward that we will not be able to fulfill.\n[2266s->2275s]That is what was incorporated, therefore, in the amount that we wrote down of the $4.5 billion.\n[2275s->2280s]That write down was about inventory and purchase commitments.\n[2280s->2287s]And our purchase commitments were about what we expected regarding the orders that we had received.\n[2287s->2295s]Going forward, though, it's a bigger issue regarding the amount of the market that we will not be able to serve.\n[2295s->2306s]We assess that TAM to be close to about $50 billion in the future as we don't have a product to enable for the China.\n[2310s->2318s]Vivek, probably the best way to think through it is that AI is several things.\n[2318s->2326s]Of course, we know that AI is this incredible technology that's going to transform every industry,\n[2326s->2344s]from, of course, the way we do software to health care and financial services to retail to, I guess, every industry, transportation, manufacturing.\n[2344s->2347s]And we're at the beginning of that.\n[2347s->2352s]But maybe another way to think about that is where do we need intelligence?\n[2352s->2354s]Where do we need digital intelligence?\n[2354s->2358s]And it's in every country, it's in every industry.\n[2358s->2365s]And we know because of that, we recognize that AI is also an infrastructure.\n[2365s->2371s]It's a way of delivering a technology that requires factories.\n[2371s->2374s]And these factories produce tokens.\n[2374s->2380s]And they, as I mentioned, are important to every single industry in every single country.\n[2380s->2389s]And so on that basis, we're really at the very beginning of it because the adoption of this technology is really kind of in its early stages.\n[2389s->2399s]Now, we've reached an extraordinary milestone with AIs that are reasoning, are thinking, what people call inference time scaling.\n[2399s->2413s]And of course, it created a whole new, we've entered an era where inference is going to be a significant part of the compute workload.\n[2413s->2417s]But anyhow, it's going to be a new infrastructure.\n[2417s->2421s]And we're building it out in the clouds.\n[2421s->2429s]The United States is really the early starter and available in US clouds.\n[2429s->2432s]And this is our largest market, our largest installed base.\n[2432s->2435s]And we can continue to see that happening.\n[2435s->2442s]But beyond that, we're going to have to, we're going to see AI go into enterprise, which is on prem.\n[2442s->2447s]Because so much of the data is still on prem, access control is really important.\n[2447s->2452s]It's really hard to move all of every company's data into the cloud.\n[2452s->2455s]And so we're going to move AI into the enterprise.\n[2455s->2460s]And you saw that we announced a couple of really exciting new products.\n[2460s->2468s]Our RTX Pro Enterprise AI server that runs everything enterprise and AI.\n[2468s->2476s]Our DGX Spark and DGX Station, which is designed for developers who want to work on prem.\n[2476s->2481s]And so enterprise AI is just taking off.\n[2481s->2490s]Telcos, today, a lot of the telco infrastructure will be in the future software defined and built on AI.\n[2490s->2492s]And so 6G is going to be built on AI.\n[2492s->2495s]And that infrastructure needs to be built out.\n[2495s->2498s]And they said it's very, very early stages.\n[2498s->2505s]And then, of course, every factory today that makes things will have an AI factory that sits with it.\n[2505s->2513s]And the AI factory is going to be drive creating AI and operating AI for the factory itself.\n[2513s->2518s]But also to power the products and the things that are made by the factory.\n[2518s->2523s]So it's very clear that every car company will have AI factories.\n[2523s->2527s]And very soon there'll be robotics companies, robot companies.\n[2527s->2534s]And those companies will be also building AIs to drive the robots.\n[2534s->2542s]And so we're at the beginning of all of this build out.\n[2542s->2547s]The next question comes from CJ News with Cantor Fitzgerald.\n[2547s->2549s]Your line is open.\n[2549s->2552s]Yeah, good afternoon. Thank you for taking the question.\n[2552s->2556s]There have been many large GPU cluster investment announcements in the last month.\n[2556s->2560s]And you alluded to a few of them with Saudi Arabia, the UAE.\n[2560s->2563s]And then also, you know, we heard from Oracle and XAI, just to name a few.\n[2563s->2570s]So my question, are there other that have yet to be announced of the same kind of scale and magnitude?\n[2570s->2583s]And perhaps more importantly, how are these orders impacting your lead times for Blackwell and your current visibility sitting here today, you know, almost halfway through 2025?\n[2583s->2593s]Well, we have more orders today than we did at the last time I spoke about orders at GTC.\n[2593s->2599s]However, we're also increasing our supply chain and building out our supply chain.\n[2599s->2602s]They're doing a fantastic job.\n[2602s->2613s]We're building it here on shore in the United States, but we're going to keep our supply chain quite busy for several many more years coming.\n[2613s->2625s]And with respect to further announcements, I'm going to be on the road next week through Europe.\n[2625s->2640s]And it's just about every country needs to build out AI infrastructure and their umpteen AI factories being planned.\n[2640s->2650s]I think in the remarks, Colette mentioned there's some 100 AI factories being built.\n[2650s->2653s]There's a whole bunch that haven't been announced.\n[2653s->2673s]And I think the important concept here, which makes it easier to understand, is that like other technologies that impact literally every single industry, of course, electricity was one, and it became infrastructure.\n[2673s->2683s]Of course, the information infrastructure, which we now know as the Internet, affects every single industry, every country, every society.\n[2683s->2686s]Intelligence is surely one of those things.\n[2686s->2693s]I don't know any company, industry, country who thinks that intelligence is optional.\n[2693s->2694s]It's essential infrastructure.\n[2694s->2701s]And so we've now digitalized intelligence.\n[2701s->2708s]And so I think we're clearly in the beginning of the build out of this infrastructure.\n[2708s->2712s]And every country will have it.\n[2712s->2713s]I'm certain of that.\n[2713s->2714s]Every industry will use it.\n[2714s->2716s]That I'm certain of.\n[2716s->2722s]And what's unique about this infrastructure is that it needs factories.\n[2722s->2728s]You know, it's a little bit like the energy infrastructure, electricity.\n[2728s->2730s]It needs factories.\n[2730s->2733s]We need factories to produce this intelligence.\n[2733s->2736s]And the intelligence is getting more sophisticated.\n[2736s->2743s]We were talking about earlier that we had a huge breakthrough in the last couple of years with reasoning AI.\n[2743s->2748s]And now there are agents that reason and there are super agents that use a whole bunch of tools.\n[2748s->2754s]And then there's clusters of super agents where agents are working with agents, solving problems.\n[2754s->2769s]And so you could just imagine compared to one shot chat bots and the agents that are now using AI built on these large language models, how much more compute intensive they really need to be and are.\n[2769s->2774s]And so I think we're in the beginning of the build out.\n[2774s->2782s]And there should be many, many more announcements in the future.\n[2782s->2783s]Your next question.\n[2783s->2784s]Excuse me.\n[2784s->2788s]Your next question comes from Ben Reitz's with Melius.\n[2788s->2790s]Your line is open.\n[2790s->2791s]Yeah, hi.\n[2791s->2793s]Thanks for the question.\n[2793s->2801s]I wanted to ask first to Collette just a little clarification around the guidance and maybe putting it in a different way.\n[2801s->2813s]The 8 billion for H20 just seems like it's roughly 3 billion more than most people thought with regard to what you'd be foregoing in the second quarter.\n[2813s->2823s]So that would mean that with regard to your guidance, the rest of the business in order to hit 45 is doing 2 to 3 billion or so better.\n[2824s->2828s]So I was wondering if that math made sense to you.\n[2828s->2837s]And then in terms of the guidance, that would imply the non-China business is doing a bit better than the street expected.\n[2837s->2843s]So wondering what the primary driver was there in your view.\n[2843s->2851s]And then this second part of my question, Jensen, I know you guide one quarter at a time.\n[2851s->2871s]But with regard to the AI diffusion rule being lifted and this momentum was sovereign, there's been times in your history where you guys have said on calls like this where you have more conviction and sequential growth throughout the year, et cetera.\n[2871s->2886s]And given the unleashing of demand with AI diffusion being revoked and the supply chain increasing, does the environment give you more conviction and sequential growth as we go throughout the year?\n[2886s->2891s]So first one for Collette and then next one for Jensen. Thanks so much.\n[2891s->2908s]Thanks, Ben, for the question. When we look at our Q2 guidance and our commentary that we provided that had the export controls not occurred, we would have had orders of about 8 billion for H20.\n[2908s->2918s]That's correct. That was a possibility for what we would have had in our outlook for this quarter in Q2.\n[2918s->2932s]So what we also have talked about here is the growth that we've seen in Blackwell, Blackwell across many of our customers, as well as the growth that we continue to have in terms of supply that we need for our customers.\n[2932s->2938s]So putting those together, that's where we came through with the guidance that we provided.\n[2938s->2942s]I'm going to turn the rest over to Jensen to see how he wants to.\n[2942s->2945s]Yeah, thanks. Thanks, Ben.\n[2945s->2957s]I would say compared to the beginning of the year, compared to GTC timeframe, there are four positive surprises.\n[2957s->2967s]The first positive surprise is the step function demand increase of reasoning AI.\n[2967s->2977s]I think it is fairly clear now that AI is going through an exponential growth, and reasoning AI really busts it through.\n[2977s->2986s]Concerns about hallucination or its ability to really solve problems.\n[2986s->2999s]I think a lot of people are crossing that barrier and realizing how incredibly effective agentic AI is and reasoning AI is.\n[2999s->3007s]So number one is inference reasoning and the exponential growth there, demand growth.\n[3007s->3012s]The second one, you mentioned AI diffusion.\n[3012s->3019s]It's really terrific to see that the AI diffusion rule was rescinded.\n[3019s->3032s]President Trump wants America to win, and he also realizes that we're not the only country in the race.\n[3032s->3042s]He wants the United States to win and recognizes that we have to get the American stack out to the world\n[3042s->3047s]and have the world build on top of American stacks instead of alternatives.\n[3047s->3052s]And so AI diffusion happened.\n[3052s->3062s]The rescinding of it happened at almost precisely the time that the countries around the world are awakening the importance of AI as an infrastructure,\n[3062s->3072s]not just as a technology of great curiosity and great importance, but infrastructure for their industries and startups and society.\n[3072s->3078s]Just as they had to build out infrastructure for electricity and Internet, you got to build out an infrastructure for AI.\n[3078s->3083s]I think that that's an awakening and that creates a lot of opportunity.\n[3083s->3085s]The third is enterprise AI.\n[3085s->3093s]Agents work and agents are doing these agents are really quite successful.\n[3093s->3098s]Much more than generative AI, agentic AI is game changing.\n[3098s->3113s]Agents can understand ambiguous and rather implicit instructions and able to problem solve and use tools and have memory and so on.\n[3113s->3118s]And so I think enterprise AI is ready to take off.\n[3118s->3133s]And it's taken us a few years to build a computing system that is able to integrate, run enterprise AI stacks, run enterprise IT stacks, but add AI to it.\n[3133s->3142s]And this is the RTX Pro Enterprise server that we announced at Computex just last week.\n[3142s->3149s]And just about every major IT company has joined us super excited about that.\n[3149s->3152s]And so computing is one stat, one part of it.\n[3152s->3157s]But remember, enterprise IT is really three pillars.\n[3157s->3161s]It's compute, storage and networking.\n[3161s->3164s]And we've now put all three of them together for finally.\n[3164s->3167s]And we're going to market with that.\n[3167s->3170s]And then lastly, industrial AI.\n[3170s->3186s]Remember, one of the implications of the world reordering, if you will, is regions onshore manufacturing and building plants everywhere.\n[3186s->3195s]In addition to AI factories, of course, there are new electronics manufacturing chip manufacturing being built around the world.\n[3195s->3210s]And all of these new plants and these new factories are creating exactly the right time when Omniverse and AI and all the work that we're doing with robotics is emerging.\n[3210s->3215s]And so this fourth pillar is quite important.\n[3215s->3218s]Every factory will have an AI factory associated with it.\n[3218s->3226s]And in order to create these physical AI systems, you really have to train a vast amount of data.\n[3226s->3232s]So back to more data, more training, more AIs to be created, more computers.\n[3232s->3243s]And so these four drivers are really kicking into turbocharge.\n[3243s->3248s]Your next question comes from Timothy R. Curie with UBS.\n[3248s->3250s]Your line is open.\n[3250s->3251s]Thanks a lot.\n[3251s->3253s]Jensen, I wanted to ask about China.\n[3253s->3257s]It sounds like the July guidance assumes there's no SKU replacement for the H-20.\n[3257s->3262s]But if the president wants the U.S. to win, it seems like you're going to have to be allowed to ship something into China.\n[3262s->3265s]So I guess I had two points on that.\n[3265s->3269s]First of all, have you been approved to ship a new modified version into China?\n[3269s->3272s]And you're currently building it, but you just can't ship it in fiscal Q2.\n[3272s->3277s]And then you were sort of run rating $7 to $8 billion a quarter into China.\n[3277s->3283s]Can we get back to those sorts of quarterly run rates once you get something that you're allowed to ship back into China?\n[3283s->3287s]I think we're all trying to figure out how much to add back to our models and when.\n[3287s->3289s]So whatever you can say there would be great.\n[3289s->3291s]Thanks.\n[3291s->3293s]The president has a plan.\n[3293s->3296s]He has a vision and I trust him.\n[3296s->3305s]With respect to our export controls, it's a set of limits.\n[3305s->3322s]And the new set of limits pretty much make it impossible for us to reduce Hopper any further for any productive use.\n[3322s->3330s]And so the new limits, it's kind of the end of the road for Hopper.\n[3330s->3333s]We have limited options.\n[3333s->3337s]And so the key is to understand the limits.\n[3337s->3346s]The key is to understand the limits and see if we can come up with interesting products that could continue to serve the Chinese market.\n[3346s->3350s]We don't have anything at the moment.\n[3350s->3352s]But we're considering it.\n[3352s->3354s]We're thinking about it.\n[3354s->3358s]Obviously the limits are quite stringent at the moment.\n[3358s->3363s]And we have nothing to announce today.\n[3363s->3373s]And when the time comes, you know, we'll engage the administration and discuss that.\n[3373s->3378s]Your final question comes from the line of Aaron Rakers with Wells Fargo.\n[3378s->3381s]Your line is open.\n[3381s->3382s]Hi.\n[3382s->3383s]This is Jake on for Aaron.\n[3383s->3386s]Thanks for taking the question and congrats on the great quarter.\n[3386s->3399s]I was wondering if you could give some additional color around the strength you saw within the networking business, particularly around the adoption of your Ethernet solutions at CSPs as well as your\n[3400s->3404s]any change you're seeing in network attach rates.\n[3404s->3407s]Yeah, thank you for that.\n[3407s->3415s]We now have three networking platforms, maybe four.\n[3415s->3425s]The first one is the scale up platform to turn a computer into a much larger computer.\n[3425s->3427s]Scaling up is incredibly hard to do.\n[3427s->3429s]Scaling out is easier to do.\n[3429s->3431s]But scaling up is hard to do.\n[3431s->3433s]And that platform is called NVLink.\n[3433s->3446s]And NVLink comes with it, chips and switches and, you know, NVLink spines.\n[3446s->3448s]It's really complicated.\n[3448s->3452s]But anyways, that's our new platform, scale up platform.\n[3452s->3455s]In addition to InfiniBand, we also have SpectrumX.\n[3455s->3466s]We've been fairly, fairly consistent that Ethernet was designed for a lot of traffic that are independent.\n[3466s->3471s]But in the case of AI, you have a lot of computers working together.\n[3471s->3477s]And the traffic of AI is insanely bursty.\n[3477s->3479s]Latency matters a lot.\n[3479s->3484s]Because the AI is thinking and it wants to get work done as quickly as possible.\n[3484s->3487s]And you got a whole bunch of nodes working together.\n[3487s->3499s]And so we enhanced Ethernet, added capabilities like extremely low latency, congestion control, adaptive routing,\n[3499s->3506s]the type of technologies that were available only in InfiniBand to Ethernet.\n[3506s->3511s]And as a result, we improved the utilization of Ethernet in these clusters.\n[3511s->3519s]These clusters are gigantic from as low as 50% to as high as 85%, 90%.\n[3519s->3531s]And so the difference is if you had a cluster that's $10 billion and you improved its effectiveness by 40%, that's worth $4 billion.\n[3532s->3534s]It's incredible.\n[3534s->3538s]And so SpectrumX has been really, quite frankly, a home run.\n[3538s->3553s]And this last quarter, as we said in the prepared remarks, we added two very significant CSPs to the SpectrumX adoption.\n[3553s->3558s]And then the last one, the last one is Bluefield, which is our control plane.\n[3558s->3566s]And so in those four, the control plane and network, which is used for storage, is used for security,\n[3566s->3576s]and for many of these clusters that want to achieve isolation among its users, multi-tenant clusters,\n[3576s->3583s]and still be able to use and have extremely high performance, bare metal performance,\n[3584s->3591s]Bluefield is ideal for that and is used in a lot of these cases.\n[3591s->3599s]And so we have these four networking platforms that are all growing and we're doing really well.\n[3599s->3601s]I'm very proud of the team.\n[3603s->3607s]That is all the time we have for questions.\n[3607s->3610s]Denton, I will turn the call back to you.\n[3610s->3611s]Thank you.\n[3611s->3612s]Thank you.\n[3612s->3615s]This is the start of a powerful new wave of growth.\n[3615s->3617s]Grace Blackwell is in full production.\n[3617s->3619s]We're off to the races.\n[3619s->3621s]We now have multiple significant growth engines.\n[3621s->3627s]Inference, one's delighted workload is surging with revenue generating AI services.\n[3627s->3635s]AI is growing faster and will be larger than any platform shifts before, including the internet, mobile, and cloud.\n[3635s->3645s]Blackwell is built to power the full AI lifecycle from training frontier models to running complex inference and reasoning agents at scale.\n[3645s->3655s]Training demands continues to rise with breakthroughs in post-training and like reinforcement learning and synthetic data generation.\n[3655s->3657s]But inference is exploding.\n[3657s->3661s]Reasoning AI agents require orders of magnitude more compute.\n[3661s->3667s]The foundations of our next growth platforms are in place and ready to scale.\n[3667s->3674s]Sovereign AI nations are investing in AI infrastructure like they once did for electricity and internet.\n[3674s->3681s]Enterprise AI, AI must be deployable on prem and integrated with existing IT.\n[3681s->3692s]Our RTX Pro, DGX Spark, and DGX Station Enterprise AI systems are ready to modernize the $500 billion IT infrastructure on prem or in the cloud.\n[3692s->3696s]Every major IT provider is partnering with us.\n[3696s->3710s]Industrial AI from training to digital twin simulation to deployment, NVIDIA Omniverse and Isaac Groot are powering next generation factories and humanoid robotic systems worldwide.\n[3710s->3723s]The age of AI is here from AI infrastructures, inference at scale, sovereign AI, enterprise AI, and industrial AI.\n[3723s->3725s]NVIDIA is ready.\n[3725s->3740s]Join us at GTC Paris, our keynote at VivaTech on June 11, talking about quantum GPU computing, robotic factories and robots, and celebrate our partnerships building AI factories across the region.\n[3740s->3745s]The NVIDIA Band will tour France, the UK, Germany, and Belgium.\n[3745s->3753s]Thank you for joining us at the earnings call today. See you in Paris.\n[3753s->3758s]This concludes today's conference call. You may now disconnect.\n",
            "Reportable segment operating income – The year over year increase in Compute & Networking segment operating income in the first quarter of fiscal year 2026\nwas driven by growth in revenue, partially offset by a $4.5 billion charge associated with H20 excess inventory and purchase obligations. The year over year\nincrease in Graphics segment operating income in the first quarter of fiscal year 2026 was driven by growth in revenue.\n\nAll Other operating loss – The year over year increase in the first quarter of fiscal year 2026 was due to an increase in stock-based compensation expense\nreflecting employee growth.\n\nConcentration of Revenue\n\nWe refer to customers who purchase products directly from NVIDIA as direct customers, such as add-in board manufacturers, distributors, ODMs, OEMs, and\nsystem integrators. We have certain customers that may purchase products directly from NVIDIA and may use either internal resources or third-party system\nintegrators to complete their build. We also have indirect customers, who purchase products through our direct customers; indirect customers include CSPs,\nconsumer internet companies, enterprises, and public sector entities.\n\nDirect Customers – Sales to one direct customer, Customer A, represented 16% of total revenue and sales to a second direct customer, Customer B,\nrepresented 14% of total revenue for the first quarter of fiscal year 2026, both of which were attributable to the Compute & Networking segment. Sales to two\ndirect customers represented 11% and 13% of total revenue for the first quarter of fiscal year 2025, both of which were attributable to the Compute & Networking\nsegment.\n\nIndirect Customers – Indirect customer revenue is an estimation based upon multiple factors including customer purchase order information, product\nspecifications, internal sales data, and other sources. Actual indirect customer revenue may differ from our estimates. For the first quarter of fiscal year 2026, two\nindirect customers which primarily purchase our products through system integrators and distributors, including through Direct Customers A and B, are estimated\nto represent 10% or more of total revenue, attributable to the Compute & Networking segment.\n\nWe estimate that in the first quarter of fiscal year 2026, an AI research and deployment company contributed to a meaningful amount of our revenue, through\none of the above indirect customers and through other indirect customers that provide cloud services.\n\nWe have experienced periods where we receive a significant amount of our revenue from a limited number of customers, and this trend may continue.\n\nRevenue by geographic region is designated based on the billing location even if the revenue may be attributable to indirect customers in a different location.\nRevenue from sales to customers outside of the United States accounted for 53% and 48% of total revenue for the first quarter of fiscal years 2026 and 2025,\nrespectively.\n\nGross Profit and Gross Margin\n\nGross profit consists of total net revenue less cost of revenue. Cost of revenue consists primarily of the cost of semiconductors, including wafer fabrication,\nassembly, testing and packaging, board and device costs, manufacturing support costs, including labor and overhead associated with such purchases, final test\nyield fallout, inventory and warranty provisions, memory and component costs, tariffs, and shipping costs. Cost of revenue also includes acquisition-related\nintangible amortization expense, costs for license and development and service arrangements, IP-related costs, and stock-based compensation related to\npersonnel associated with manufacturing operations.\n\nGross margins decreased to 60.5% for the first quarter of fiscal year 2026 compared to 78.4% for the first quarter of fiscal year 2025, primarily due to a $4.5\nbillion charge associated with H20 excess inventory and purchase obligations and the initial ramp of more sophisticated systems within Data Center.\n\nProvisions for inventory and excess inventory purchase obligations totaled $5.3 billion, including $4.5 billion associated with H20 excess inventory and purchase\nobligations, and $393 million for the first quarter of fiscal years 2026 and 2025, respectively. Sales of previously reserved inventory and settlements of excess\ninventory purchase obligations resulted in a provision release of $436 million and $114 million for the first quarter of fiscal years 2026 and 2025, respectively.\nThe net effect of provisions for inventory and excess inventory purchase obligations on our gross margin was an unfavorable impact of 11.0% and 1.1% in the\nfirst quarter of fiscal years 2026 and 2025, respectively.\n\n26"
        ]
    },
    {
        "question": "Q1 FY26 的运营现金流（Cash Flow from Operations）是多少？",
        "ground_truth": "2026 财年第一季度，运营活动产生的净现金（Net cash provided by operating activities）为 274.14 亿美元。",
        "answer": "NVIDIA在2026财年第一季度的运营现金流为27,414亿美元 [来源: Q1_2026_presentation, 第 7 页]。",
        "contexts": [
            "Q1 FY26\nEarnings Summary",
            "[TABLE]\n| Cash flows from operating activities: |  |  |  |  |  |\n| --- | --- | --- | --- | --- | --- |\n| Net income | $ | 18,775 |  | $ | 14,881 |\n| Adjustments to reconcile net income to net cash provided by operating activities: |  |  |  |  |  |\n| Stock-based compensation expense | 1,474 |  |  | 1,011 |  |\n| Depreciation and amortization | 611 |  |  | 410 |  |\n| (Gains) losses on non-marketable equity securities and publicly-held equity securities, net | 175 |  |  | (69) |  |\n| Deferred income taxes | (2,177) |  |  | (1,577) |  |\n| Other | (98) |  |  | (145) |  |\n| Changes in operating assets and liabilities, net of acquisitions: |  |  |  |  |  |\n| Accounts receivable | 933 |  |  | (2,366) |  |\n| Inventories | (1,258) |  |  | (577) |  |\n| Prepaid expenses and other assets | 560 |  |  | (726) |  |\n| Accounts payable | 941 |  |  | (22) |  |\n| Accrued and other current liabilities | 7,128 |  |  | 4,202 |  |\n| Other long-term liabilities | 350 |  |  | 323 |  |\n| Net cash provided by operating activities | 27,414 |  |  | 15,345 |  |\n| Cash flows from investing activities: |  |  |  |  |  |\n| Proceeds from maturities of marketable securities | 3,122 |  |  | 4,004 |  |\n| Proceeds from sales of marketable securities | 467 |  |  | 149 |  |\n| Proceeds from sales of non-marketable equity securities | — |  |  | 55 |  |\n| Purchases of marketable securities | (6,546) |  |  | (9,303) |  |\n| Purchases related to property and equipment and intangible assets | (1,227) |  |  | (369) |  |\n| Purchases of non-marketable equity securities | (649) |  |  | (190) |  |\n| Acquisitions, net of cash acquired | (383) |  |  | (39) |  |\n| Net cash used in investing activities | (5,216) |  |  | (5,693) |  |\n| Cash flows from financing activities: |  |  |  |  |  |\n| Proceeds related to employee stock plans | 370 |  |  | 285 |  |\n| Payments related to repurchases of common stock | (14,095) |  |  | (7,740) |  |\n| Payments related to employee stock plan taxes | (1,532) |  |  | (1,752) |  |\n| Dividends paid | (244) |  |  | (98) |  |\n| Principal payments on property and equipment and intangible assets | (52) |  |  | (40) |  |\n| Net cash used in financing activities | (15,553) |  |  | (9,345) |  |\n| Change in cash and cash equivalents | 6,645 |  |  | 307 |  |\n| Cash and cash equivalents at beginning of period | 8,589 |  |  | 7,280 |  |\n| Cash and cash equivalents at end of period | $ | 15,234 |  | $ | 7,587 |\n[/TABLE]\n\n该表格显示公司经营活动产生的现金流量从15,345美元增长至27,414美元，主要得益于收入增加和资产调整。投资活动现金流出减少，而融资活动现金流出增加，导致期末现金及现金等价物从7,587美元增至15,234美元。\n\nNVIDIA Corporation and Subsidiaries\nCondensed Consolidated Statements of Cash Flows\n\n(In millions)\n(Unaudited)\n\nThree Months Ended\n\nSee accompanying Notes to Condensed Consolidated Financial Statements.\n\n7",
            "[0s->5s]Good afternoon. My name is Sarah and I will be your conference operator today.\n[5s->13s]At this time, I would like to welcome everyone to NVIDIA's first quarter fiscal 2026 financial results conference call.\n[13s->17s]All lines have been placed on mute to prevent any background noise.\n[17s->21s]After the speaker's remarks, there will be a question and answer session.\n[21s->27s]If you would like to ask a question during this time, simply press star one on your telephone keypad.\n[27s->32s]If you would like to withdraw your question, please press star one again. Thank you.\n[32s->36s]Toshia Hari, you may begin your conference.\n[36s->44s]Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the first quarter of fiscal 2026.\n[44s->54s]With me today from NVIDIA are Jensen Wong, President and Chief Executive Officer, and Collette Press, Executive Vice President and Chief Financial Officer.\n[54s->60s]I'd like to remind you that our call is being webcast live on NVIDIA's investor relations website.\n[60s->68s]The webcast will be available for replay until the conference call to discuss our financial results for the second quarter of fiscal 2026.\n[68s->76s]The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent.\n[76s->81s]During this call, we may make forward looking statements based on current expectations.\n[81s->88s]These are subject to a number of significant risks and uncertainties, and our actual results may differ materially.\n[88s->96s]For a discussion of factors that could affect our future financial results in business, please refer to the disclosure in today's earnings release,\n[96s->104s]our most recent forms 10-K and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission.\n[105s->112s]All our statements are made as of today, May 28, 2025, based on information currently available to us.\n[112s->118s]Except as required by law, we assume no obligation to update any such statements.\n[118s->122s]During this call, we will discuss non-GAAP financial measures.\n[122s->130s]You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website.\n[130s->133s]With that, let me turn the call over to Collette.\n[133s->135s]Thank you, Toshio.\n[135s->148s]We delivered another strong quarter with revenue of $44 billion, up 69% year-over-year, exceeding our outlook in what proved to be a challenging operating environment.\n[148s->153s]Data center revenue of $39 billion grew 73% year-on-year.\n[153s->161s]AI workloads have transitioned strongly to inference, and AI factory build-outs are driving significant revenue.\n[161s->165s]Our customers' commitments are firm.\n[165s->176s]On April 9, the U.S. government issued new export controls on H20, our data center GPU designed specifically for the China market.\n[176s->181s]We sold H20 with the approval of the previous administration.\n[181s->187s]Although our H20 has been in the market for over a year and does not have a market outside of China,\n[187s->195s]the new export controls on H20 did not provide a grace period to allow us to sell through our inventory.\n[195s->204s]In Q1, we recognized $4.6 billion in H20 revenue, which occurred prior to April 9,\n[204s->216s]but also recognized a $4.5 billion charge as we wrote down inventory and purchase obligations tied to orders we had received prior to April 9.\n[216s->226s]We were unable to ship $2.5 billion in H20 revenue in the first quarter due to the new export controls.\n[226s->234s]The $4.5 billion charge was less than what we initially anticipated as we were able to reuse certain materials.\n[234s->244s]We are still evaluating our limited options to supply data center compute products compliant with the U.S. government's revised export control rules,\n[244s->252s]losing access to the China AI accelerator market, which we believe will grow to nearly $50 billion,\n[252s->262s]would have a material adverse impact on our business going forward and benefit our foreign competitors in China and worldwide.\n[262s->271s]Our Blackwell ramp, the fastest in our company's history, drove a 73% year-on-year increase in data center revenue.\n[271s->280s]Blackwell contributed nearly 70% of data center compute revenue in the quarter, with a transition from Hopper nearly complete.\n[280s->293s]The introduction of GB200 NVL was a fundamental architectural change to enable data center scale workloads and to achieve the lowest cost per inference token.\n[293s->305s]While these systems are complex to build, we have seen a significant improvement in manufacturing yields and rack shipments are moving to strong rates to end customers.\n[305s->315s]GB200 NVL racks are now generally available for modern builders, enterprises, and sovereign customers to develop and deploy AI.\n[315s->332s]On average, major hyperscalers are each deploying nearly 1,000 NVL 72 racks or 72,000 Blackwell GPUs per week and are on track to further ramp output this quarter.\n[332s->346s]Microsoft, for example, has already deployed tens of thousands of Blackwell GPUs and is expected to ramp to hundreds of thousands of GB200s with OpenAI as one of its key customers.\n[346s->356s]Key learnings from the GB200 ramp will allow for a smooth transition to the next phase of our product roadmap, Blackwell Ultra.\n[357s->370s]Sampling of GB300 systems began earlier this month at the major CSPs, and we expect production shipments to commerce later this quarter.\n[370s->381s]GB300 will leverage the same architecture, same physical footprint, and the same electrical and mechanical specifications as GB200.\n[381s->394s]The GB300 drop-in design will allow CSPs to seamlessly transition their systems and manufacturing used for GB200 while maintaining high yields.\n[394s->407s]B300 GPUs with 50% more HBM will deliver another 50% increase in dense FP4 inference compute performance compared to the B200.\n[407s->421s]We remain committed to our annual product cadence, with our roadmap extending through 2028 tightly aligned with the multiple-year planning cycles of our customers.\n[421s->432s]We are witnessing a sharp jump in inference demand. OpenAI, Microsoft, and Google are seeing a step-function leap in token generation.\n[432s->441s]Microsoft processed over 100 trillion tokens in Q1, a five-fold increase on a year-over-year basis.\n[441s->453s]This exponential growth in Azure OpenAI is representative of strong demand for Azure AI Foundry as well as other AI services across Microsoft's platform.\n[453s->471s]Inference-serving startups are now serving models using B200, tripling their token generation rate and corresponding revenues for high-value reasoning models such as DeepSeq R1, as reported by Artificial Analysis.\n[471s->485s]NVIDIA Dynamo on Blackwell NVL72 turbocharges AI inference throughput by 30x for the new reasoning models sweeping the industry.\n[485s->497s]Developer engagements increased with adoption ranging from LLM providers such as Perplexity to financial services institutions such as Capital One,\n[497s->503s]who reduced agentic chatbox latency by 5x with Dynamo.\n[503s->511s]In the latest MLPerf inference results, we submitted our first results using GB200 NVL72,\n[511s->523s]delivering up to 30x higher inference throughput compared to our 8 GPU H200 submission on the challenging Llama 3.1 benchmark.\n[523s->535s]This feat was achieved through a combination of tripling the performance per GPU as well as 9x more GPUs, all connected on a single NVLink domain.\n[535s->547s]And while Blackwell is still early in its lifecycle, software optimizations have already improved its performance by 1.5x in the last month alone.\n[547s->555s]We expect to continue improving the performance of Blackwell through its operational life as we have done with Hopper and Amper.\n[555s->560s]For example, we increased the inference performance of Hopper by 4x over two years.\n[560s->568s]This is the benefit of NVIDIA's programmable CUDA architecture and rich ecosystem.\n[568s->578s]The pace and scale of AI factory deployments are accelerating with nearly 100 NVIDIA-powered AI factories in flight this quarter,\n[578s->588s]a two-fold increase year over year, with the average number of GPUs powering each factory also doubling in the same period.\n[588s->593s]And more AI factory projects are starting across industries and geographies.\n[593s->605s]NVIDIA's full-stack architecture is underpinning AI factory deployments as industry leaders like AT&T, BYD, Capital One, Foxconn, MediaTek, and Telenor\n[605s->614s]are strategically vital sovereign clouds like those recently announced in Saudi Arabia, Taiwan, and the UAE.\n[614s->623s]We have a line of sight to projects requiring tens of gigawatts of NVIDIA AI infrastructure in the not-too-distant future.\n[623s->639s]The transition from generative to agentic AI, AI capable of perceiving, reasoning, planning, and acting, will transform every industry, every company, and country.\n[639s->650s]We envision AI agents as a new digital workforce capable of handling tasks ranging from customer service to complex decision-making processes.\n[650s->660s]We introduced the LAMA Nemotron family of open reasoning models designed to supercharge identical AI platforms for enterprises.\n[660s->672s]Built on the LAMA architecture, these models are available as NIMS or NVIDIA inference microservices with multiple sizes to meet diverse deployment needs.\n[672s->681s]Our post-training enhancements have yielded a 20% accuracy boost and a 5x increase in inference speed.\n[681s->690s]Leading platform companies including Accenture, Cadence, Deloitte, and Microsoft are transforming work with our reasoning models.\n[690s->702s]NVIDIA NEMO microservices are generally available across industries that are being leveraged by leading enterprises to build, optimize, and scale AI applications.\n[702s->711s]With NEMO, Cisco increased model accuracy by 40% and improved response time by 10x in its code assistant.\n[711s->719s]NASDAQ realized a 30% improvement in accuracy and response time in its AI platform's search capabilities.\n[719s->727s]And Shell's Custom LLM achieved a 30% increase in accuracy when trained with NVIDIA NEMO.\n[727s->735s]NEMO's parallelism techniques accelerated model training time by 20% when compared to other frameworks.\n[735s->757s]We also announced a partnership with Yum Brands, the world's largest restaurant company, to bring NVIDIA AI to 500 of its restaurants this year and expanding to 61,000 restaurants over time to streamline order taking, optimize operations, and enhance service across its restaurants.\n[757s->779s]For AI-powered cybersecurity, leading companies like Checkpoint, CloudStrike, and Palo Alto Networks are using NVIDIA's AI security and software stack to build, optimize, and secure agentic workflows, with CloudStrike realizing 2x faster detection triage with 50% less compute cost.\n[780s->790s]Moving to networking, sequential growth in networking resumed in Q1, with revenue up 64% quarter-over-quarter to $5 billion.\n[790s->798s]Our customers continue to leverage our platform to efficiently scale up and scale out AI factory workloads.\n[798s->813s]We created the world's fastest switch, NVLink. For scale up, our NVLink compute fabric in its fifth generation offers 14x the bandwidth of PCIe Gen 5.\n[813s->825s]NVLink 72 carries 130 TB per second of bandwidth in a single rack, equivalent to the entirety of the world's peak internet traffic.\n[825s->833s]NVLink is a new growth vector and is off to a great start, with Q1 shipments exceeding $1 billion.\n[833s->848s]At Computex, we announced NVLink Fusion. Hyperscale customers can now build semi-custom CCUs and accelerators that connect directly to the NVIDIA platform with NVLink.\n[848s->868s]We are now enabling key partners, including ASIC providers such as MediaTek, Marvell, Alchip Technologies, and Astera Labs, as well as CPU suppliers such as Fujitsu and Qualcomm, to leverage NVLink Fusion to connect our respective ecosystems.\n[868s->878s]For scale out, our enhanced Ethernet offerings deliver the highest throughput, lowest latency networking for AI.\n[878s->886s]SpectrumX posted strong sequential and year-on-year growth and is now annualizing over $8 billion in revenue.\n[886s->896s]Adoption is widespread across major CSPs and consumer internet companies, including CoreWe, Microsoft Azure, Oracle Cloud, and XAI.\n[896s->903s]This quarter, we added Google Cloud and Meta to the growing list of SpectrumX customers.\n[903s->913s]We introduced SpectrumX and QuantumX, silicon photonics switches featuring the world's most advanced co-packaged optics.\n[913s->933s]These platforms will enable next level AI factory scaling to millions of GPUs through the increasingly power efficiency by 3.5x and network resiliency by 10x while accelerating customer time to market by 1.3x.\n[933s->938s]Transitioning to a quick summary of our revenue by geography.\n[938s->949s]China, as a percentage of our data center revenue, was slightly below our expectations and down sequentially due to H20 export licensing controls.\n[949s->955s]For Q2, we expect a meaningful decrease in China data center revenue.\n[955s->971s]As a reminder, while Singapore represented nearly 20% of our Q1 billed revenue, as many of our large customers use Singapore for centralized invoicing, our products are almost always sold elsewhere.\n[971s->984s]Note that over 99% of H100, H200, and Blackwell data center compute revenue billed to Singapore was for orders from US based customers.\n[984s->996s]Moving to gaming and AI PCs. Gaming revenue was a record 3.8 billion, increasing 48% sequentially and 42% year on year.\n[996s->1012s]Strong adoption by gamers, creatives, and AI enthusiasts have made Blackwell our fastest ramp ever. Against a backdrop of robust demand, we greatly improved our supply and availability in Q1 and expect to continue these efforts in Q2.\n[1012s->1017s]AI is transforming PC and creator and gamers.\n[1017s->1035s]With a 100 million user installed base, GeForce represents the largest footprint for PC developers. This quarter, we added to our AI PC laptop offerings, including models capable of running Microsoft's Copilot Plus.\n[1035s->1049s]This past quarter, we brought Blackwell architecture to mainstream gaming with its launch of GeForce RTX 5060 and 5060 Ti, starting at just $299.\n[1049s->1061s]The RTX 5060 also debuted in laptops starting at $1,099. These systems that double the frame rate and slash latency.\n[1061s->1070s]These GeForce RTX 5060 and 5060 Ti desktop GPUs and laptops are now available.\n[1070s->1090s]In console gaming, the recently unveiled Nintendo Switch 2 leverages Nvidia's neural rendering and AI technologies, including next generation custom RTX GPUs with DLSS technology, to deliver a giant leap in gaming performance to millions of players worldwide.\n[1090s->1100s]Nintendo has shipped over 150 million Switch consoles to date, making it one of the most successful gaming systems in history.\n[1100s->1108s]Moving to pro visualization. Revenue of $509 million was flat sequentially and up 19% year on year.\n[1108s->1120s]Tariff related uncertainty temporarily impacted Q1 systems and demand for AI workstations is strong and we expect sequential revenue growth to resume in Q2.\n[1120s->1130s]Nvidia DGX Spark and Station revolutionize personal computing by putting the power of an AI supercomputer in a desktop form factor.\n[1130s->1143s]DGX Spark delivers up to one petaflop of AI compute while DGX Station offers an incredible 20 petaflops and is powered by the GB300 Superchip.\n[1143s->1150s]DGX Spark will be available in Calendar Q3 and DGX Station later this year.\n[1150s->1163s]We have deepened Omniverse's integration and adoption into some of the world's leading software platforms, including Databricks, SAP, and Schneider Electric.\n[1163s->1178s]New Omniverse blueprints such as Mega for at scale robotic fleet management are being leveraged in Keyon Group, Pegatron, Accenture, and other leading companies to enhance industrial operations.\n[1178s->1189s]At Confitex, we showcased Omniverse's great traction with technology manufacturing leaders, including TSMC, Quanta, Foxconn, Pegatron.\n[1189s->1194s]Using Omniverse, TSMC saves months in work by designing fabs virtually.\n[1194s->1205s]Foxconn accelerates thermal simulations by 150x and Pegatron reduced assembly line defects rates by 67%.\n[1205s->1214s]Lastly, with our automotive group, revenue was $567 million, down 1% sequentially but up 72% year on year.\n[1214s->1222s]Year on year growth was driven by the ramp of self-driving across a number of customers and robust end demand for NEVs.\n[1222s->1232s]We are partnering with GM to build the next-gen vehicles, factories, and robots using NVIDIA AI, simulation, and accelerated computing.\n[1232s->1241s]And we are now in production with our full-stack solution for Mercedes-Benz, starting with the new CLA, hitting roads in the next few months.\n[1241s->1254s]We announced Isaac, Groot, and won the world's first open fully customizable foundation model for humanoid robots, enabling generalized reasoning and skill development.\n[1254s->1260s]We also launched new open NVIDIA Cosmo World Foundation models.\n[1260s->1268s]Leading companies include 1X, Agility Robots, Robotics, Figure AI, Uber, and Wabi.\n[1268s->1284s]We've begun integrating Cosmos into their operations for synthetic data generation, while Agility Robotics, Boston Dynamics, and XPen Robotics are harnessing Isaac simulation to advance their humanoid efforts.\n[1284s->1300s]GE Healthcare is using the new NVIDIA Isaac platform for healthcare simulation, built on NVIDIA Omniverse and using NVIDIA Cosmos, the platform speeds development of robotic imaging and surgery systems.\n[1300s->1314s]The era of robotics is here. Billions of robots, hundreds of millions of autonomous vehicles, and hundreds of thousands of robotic factories and warehouses will be developed.\n[1314s->1335s]All right, moving to the rest of the P&L. Gap gross margins and non-gap gross margins were 60.5% and 61%, respectively. Excluding the $4.5 billion charge, Q1 non-gap gross margins would have been 71.3%, slightly above our outlook at the beginning of the quarter.\n[1335s->1346s]Sequentially, gap operating expenses were up 7% and non-gap operating expenses were up 6%, reflecting higher compensation and employee growth.\n[1346s->1355s]Our investments include expanding our infrastructure capabilities and AI solutions, and we plan to grow these investments throughout the fiscal year.\n[1355s->1370s]In Q1, we returned a record $14.3 billion to shareholders in the form of share repurchases and cash dividends. Our capital return program continues to be a key element of our capital allocation strategy.\n[1370s->1384s]Let me turn to the outlook for the second quarter. Total revenue is expected to be $45 billion, plus or minus 2%. We expect modest sequential growth across all of our platforms.\n[1384s->1403s]In data center, we anticipate the continued ramp of Blackwell to be partially offset by a decline in China revenue. Note, our outlook reflects a loss in H20 revenue of approximately $8 billion for the second quarter.\n[1403s->1420s]Gap and non-gap gross margins are expected to be 71.8% and 72% respectively, plus or minus 50 basis points. We expect better Blackwell profitability to drive modest sequential improvement in gross margins.\n[1420s->1427s]We are continuing to work towards achieving gross margins in the mid-70s range late this year.\n[1427s->1444s]Gap and non-gap operating expenses are expected to be approximately $5.7 billion and $4 billion respectively, and we continue to expect full-year fiscal year 26 operating expense growth to be in the mid-30% range.\n[1444s->1457s]Gap and non-gap other income and expenses are expected to be an income of approximately $450 million, excluding gays and losses from non-marketable and publicly held equity securities.\n[1457s->1468s]Gap and non-gap tax rates are expected to be 16.5%, plus or minus 1%, excluding any discrete items.\n[1468s->1480s]Further financial details are included in the CFO commentary and other information available on our IR website, including a new financially information AI agent.\n[1480s->1488s]Let me highlight upcoming events for the financial community. We will be at the B of A Global Technology Conference in San Francisco on June 4.\n[1488s->1500s]The Rosenblatt Virtual AI Summit and NASDAQ Investor Conference in London on June 10, and GTC Paris at Viva Tech on June 11 in Paris.\n[1500s->1511s]We look forward to seeing you at these events. Our earnings call to discuss the results of our second quarter of fiscal 2026 is scheduled for August 27.\n[1511s->1517s]Well now, let me turn it over to Jensen to make some remarks.\n[1517s->1526s]Thanks, Collette. We've had a busy and productive year. Let me share my perspective on some topics we're frequently asked.\n[1526s->1536s]On export control, China is one of the world's largest AI markets and a springboard to global success.\n[1536s->1543s]With half of the world's AI researchers based there, the platform that wins China is positioned to lead globally.\n[1543s->1551s]Today, however, the $50 billion China market is effectively closed to U.S. industry.\n[1551s->1560s]The H20 export ban ended our Hopper data center business in China. We cannot reduce Hopper further to comply.\n[1561s->1568s]As a result, we are taking a multi-billion dollar write-off on inventory that cannot be sold or repurposed.\n[1568s->1575s]We are exploring limited ways to compete, but Hopper is no longer an option.\n[1575s->1584s]China's AI moves on with or without U.S. chips. It has to compute to train and deploy advanced models.\n[1585s->1590s]The question is not whether China will have AI. It already does.\n[1590s->1596s]The question is whether one of the world's largest AI markets will run on American platforms.\n[1596s->1604s]Shielding Chinese chipmakers from U.S. competition only strengthens them abroad and weakens America's position.\n[1604s->1609s]Export restrictions have spurred China's innovation in scale.\n[1609s->1616s]The AI race is not just about chips. It's about which stack the world runs on.\n[1616s->1624s]As that stack grows to include 6G and quantum, U.S. global infrastructure leadership is at stake.\n[1624s->1630s]The U.S. has based its policy on the assumption that China cannot make AI chips.\n[1630s->1635s]That assumption was always questionable, and now it's clearly wrong.\n[1635s->1639s]China has enormous manufacturing capability.\n[1639s->1645s]In the end, the platform that wins the AI developers wins AI.\n[1645s->1653s]Export controls should strengthen U.S. platforms, not drive half of the world's AI talent to rivals.\n[1654s->1657s]On DeepSeek.\n[1657s->1665s]DeepSeek and QN from China are among the best open source AI models.\n[1665s->1671s]Released freely, they've gained traction across the U.S., Europe, and beyond.\n[1671s->1680s]DeepSeek R1, like ChatGPT, introduced reasoning AI that produces better answers the longer it thinks.\n[1680s->1690s]Reasoning AI enables step-by-step problem solving, planning, and tool use, turning models into intelligent agents.\n[1690s->1705s]Reasoning is compute intensive, requires hundreds to thousands of times more tokens per task than previous one-shot inference.\n[1705s->1711s]Reasoning models are driving a step function surge in inference demand.\n[1711s->1721s]AI scaling laws remain firmly intact, not only for training, but now inference, too, requires massive scale compute.\n[1721s->1726s]DeepSeek also underscores the strategic value of open source AI.\n[1726s->1734s]When popular models are trained and optimized on U.S. platforms, it drives usage, feedback, and continuous improvement.\n[1734s->1737s]Reinforcing American leadership across the stack.\n[1737s->1743s]U.S. platforms must remain the preferred platform for open source AI.\n[1743s->1749s]That means supporting collaboration with top developers globally, including in China.\n[1749s->1757s]America wins when models like DeepSeek and QN runs best on American infrastructure.\n[1757s->1760s]Regarding onshore manufacturing.\n[1760s->1770s]President Trump has outlined a bold vision to reshore advanced manufacturing, create jobs, and strengthen national security.\n[1770s->1774s]Future plants will be highly computerized in robotics.\n[1774s->1776s]We share this vision.\n[1776s->1783s]TSMC is building six fabs and two advanced packaging plants in Arizona to make chips for Nvidia.\n[1783s->1789s]Process qualification is underway, with volume production expected by year end.\n[1790s->1798s]Spill and Amcor are also investing in Arizona, constructing packaging, assembly, and test facilities.\n[1798s->1806s]In Houston, we're partnering with Foxconn to construct a million-square-foot factory to build AI supercomputers.\n[1806s->1811s]Wistron is building a similar plant in Fort Worth, Texas.\n[1811s->1817s]To encourage and support these investments, we've made substantial long-term purchase commitments.\n[1817s->1821s]A deep investment in America's AI manufacturing future.\n[1823s->1827s]Our goal, from chip to supercomputer.\n[1827s->1830s]Built in America within a year.\n[1830s->1840s]Each GB200 and Beelink 72 racks contains 1.2 million components and weighs nearly two tons.\n[1840s->1844s]No one has produced supercomputers on this scale.\n[1844s->1847s]Our partners are doing an extraordinary job.\n[1849s->1857s]On AI diffusion rule, President Trump rescinded the AI diffusion rule, calling it counterproductive,\n[1857s->1864s]and proposed a new policy to promote US AI tech with trusted partners.\n[1864s->1868s]On his Middle East tour, he announced historic investments.\n[1868s->1875s]I was honored to join him in announcing a 500-megawatt AI infrastructure project in Saudi Arabia\n[1875s->1880s]and a 5-gigawatt AI campus in the UAE.\n[1880s->1883s]President Trump wants US tech to lead.\n[1883s->1887s]The deals he announced are wins for America.\n[1887s->1895s]Creating jobs, advancing infrastructure, generating tax revenue, and reducing the US trade deficit.\n[1895s->1902s]The US will always be Nvidia's largest market and home to the largest install base of our infrastructure.\n[1902s->1909s]Every nation now sees AI as core to the next industrial revolution.\n[1909s->1915s]A new industry that produces intelligence and essential infrastructure for every economy.\n[1915s->1922s]Countries are racing to build national AI platforms to elevate their digital capabilities.\n[1922s->1930s]At Computex, we announced Taiwan's first AI factory in partnership with Foxconn and the Taiwan government.\n[1930s->1936s]Last week, I was in Sweden to launch its first national AI infrastructure.\n[1936s->1947s]Japan, Korea, India, Canada, France, the UK, Germany, Italy, Spain, and more are now building national AI factories\n[1947s->1951s]to empower startups, industries, and societies.\n[1951s->1956s]Sovereign AI is a new growth engine for Nvidia.\n[1958s->1960s]Toshia, back to you. Thank you.\n[1960s->1966s]Operator, we will now open the call for questions. Would you please pull for questions?\n[1966s->1974s]Thank you. At this time, I would like to remind everyone in order to ask a question, press star, then the number one on your telephone keypad.\n[1974s->1978s]We'll pause for just a moment to compile the Q&A roster.\n[1982s->1988s]Your first question comes from the line of Joe Moore with Morgan Stanley. Your line is open.\n[1990s->1998s]Great. Thank you. You guys have talked about this scaling up of inference around reasoning models for at least a year now,\n[1998s->2002s]and we've really seen that come to fruition as you talked about. We've heard it from your customers.\n[2002s->2007s]Can you give us a sense for how much of that demand you're able to serve?\n[2007s->2010s]And give us a sense for maybe how big the inference business is for you guys.\n[2010s->2016s]And do we need full-on NVL-72 rack-scale solutions for reasoning inference going forward?\n[2019s->2026s]Well, we would like to serve all of it. And I think we're on track to serve most of it.\n[2026s->2038s]Grace Blackwell NVLink-72 is the ideal engine today, the ideal computer thinking machine, if you will, for reasoning AI.\n[2038s->2046s]There's a couple of reasons for that. The first reason is that the token generation amount,\n[2046s->2059s]the number of tokens reasoning goes through is a hundred, a thousand times more than a one-shot chatbot.\n[2059s->2069s]You know, it's essentially thinking to itself, breaking down a problem step by step. It might be planning multiple paths to an answer.\n[2069s->2083s]It could be using tools, reading PDFs, reading web pages, watching videos, and then producing a result, an answer.\n[2083s->2089s]The longer it thinks, the better the answer, the smarter the answer is.\n[2090s->2101s]And so what we would like to do and the reason why Grace Blackwell was designed to give such a giant step up in inference performance\n[2101s->2106s]is so that you could do all this and still get a response as quickly as possible.\n[2106s->2117s]Compared to Hopper, Grace Blackwell is some 40 times higher speed and throughput compared.\n[2117s->2132s]And so this is going to be a huge, huge benefit and driving down the cost while improving the quality of response with excellent quality of service at the same time.\n[2132s->2139s]So that's the fundamental reason. That was the core driving reason for Grace Blackwell NVLink-72.\n[2139s->2151s]Of course, in order to do that, we had to reinvent, literally redesign the entire way that these supercomputers are built.\n[2151s->2161s]But now we're in full production. It's going to be exciting. It's going to be incredibly exciting.\n[2162s->2171s]The next question comes from Vivek Arya with Bank of America Securities. Your line is open.\n[2171s->2176s]Thanks for the question. Just a clarification for Colette first.\n[2176s->2184s]So on the China impact, I think previously it was mentioned that at about $15 billion, so you had the $8 billion in Q2.\n[2184s->2190s]So is there still some left as a headwind for the remaining quarters, just Colette, how to model that?\n[2190s->2201s]And then question, Jensen, for you. Back at GTC, you had outlined a path towards almost a trillion dollars of AI spending over the next few years.\n[2201s->2212s]Where are we in that build out? And do you think it's going to be uniform that you will see every spender, whether it's ESP, sovereigns, enterprises or build out?\n[2212s->2215s]Should we expect some periods of digestion in between?\n[2215s->2221s]What are your customer discussions telling you about how to model growth for next year?\n[2223s->2232s]Yes, Vivek. Thanks so much for the question regarding H20. Yes, we recognized 4.6 H20 in Q1.\n[2232s->2241s]We were unable to ship $2.5 billion. So the total for Q1 should have been $7 billion.\n[2241s->2248s]When we look at our Q2, our Q2 is going to be meaningfully down in terms of China data center revenue.\n[2248s->2258s]And we had highlighted in terms of the amount of orders that we had planned for H20 and Q2, and that was $8 billion.\n[2258s->2266s]Now, going forward, we did have other orders going forward that we will not be able to fulfill.\n[2266s->2275s]That is what was incorporated, therefore, in the amount that we wrote down of the $4.5 billion.\n[2275s->2280s]That write down was about inventory and purchase commitments.\n[2280s->2287s]And our purchase commitments were about what we expected regarding the orders that we had received.\n[2287s->2295s]Going forward, though, it's a bigger issue regarding the amount of the market that we will not be able to serve.\n[2295s->2306s]We assess that TAM to be close to about $50 billion in the future as we don't have a product to enable for the China.\n[2310s->2318s]Vivek, probably the best way to think through it is that AI is several things.\n[2318s->2326s]Of course, we know that AI is this incredible technology that's going to transform every industry,\n[2326s->2344s]from, of course, the way we do software to health care and financial services to retail to, I guess, every industry, transportation, manufacturing.\n[2344s->2347s]And we're at the beginning of that.\n[2347s->2352s]But maybe another way to think about that is where do we need intelligence?\n[2352s->2354s]Where do we need digital intelligence?\n[2354s->2358s]And it's in every country, it's in every industry.\n[2358s->2365s]And we know because of that, we recognize that AI is also an infrastructure.\n[2365s->2371s]It's a way of delivering a technology that requires factories.\n[2371s->2374s]And these factories produce tokens.\n[2374s->2380s]And they, as I mentioned, are important to every single industry in every single country.\n[2380s->2389s]And so on that basis, we're really at the very beginning of it because the adoption of this technology is really kind of in its early stages.\n[2389s->2399s]Now, we've reached an extraordinary milestone with AIs that are reasoning, are thinking, what people call inference time scaling.\n[2399s->2413s]And of course, it created a whole new, we've entered an era where inference is going to be a significant part of the compute workload.\n[2413s->2417s]But anyhow, it's going to be a new infrastructure.\n[2417s->2421s]And we're building it out in the clouds.\n[2421s->2429s]The United States is really the early starter and available in US clouds.\n[2429s->2432s]And this is our largest market, our largest installed base.\n[2432s->2435s]And we can continue to see that happening.\n[2435s->2442s]But beyond that, we're going to have to, we're going to see AI go into enterprise, which is on prem.\n[2442s->2447s]Because so much of the data is still on prem, access control is really important.\n[2447s->2452s]It's really hard to move all of every company's data into the cloud.\n[2452s->2455s]And so we're going to move AI into the enterprise.\n[2455s->2460s]And you saw that we announced a couple of really exciting new products.\n[2460s->2468s]Our RTX Pro Enterprise AI server that runs everything enterprise and AI.\n[2468s->2476s]Our DGX Spark and DGX Station, which is designed for developers who want to work on prem.\n[2476s->2481s]And so enterprise AI is just taking off.\n[2481s->2490s]Telcos, today, a lot of the telco infrastructure will be in the future software defined and built on AI.\n[2490s->2492s]And so 6G is going to be built on AI.\n[2492s->2495s]And that infrastructure needs to be built out.\n[2495s->2498s]And they said it's very, very early stages.\n[2498s->2505s]And then, of course, every factory today that makes things will have an AI factory that sits with it.\n[2505s->2513s]And the AI factory is going to be drive creating AI and operating AI for the factory itself.\n[2513s->2518s]But also to power the products and the things that are made by the factory.\n[2518s->2523s]So it's very clear that every car company will have AI factories.\n[2523s->2527s]And very soon there'll be robotics companies, robot companies.\n[2527s->2534s]And those companies will be also building AIs to drive the robots.\n[2534s->2542s]And so we're at the beginning of all of this build out.\n[2542s->2547s]The next question comes from CJ News with Cantor Fitzgerald.\n[2547s->2549s]Your line is open.\n[2549s->2552s]Yeah, good afternoon. Thank you for taking the question.\n[2552s->2556s]There have been many large GPU cluster investment announcements in the last month.\n[2556s->2560s]And you alluded to a few of them with Saudi Arabia, the UAE.\n[2560s->2563s]And then also, you know, we heard from Oracle and XAI, just to name a few.\n[2563s->2570s]So my question, are there other that have yet to be announced of the same kind of scale and magnitude?\n[2570s->2583s]And perhaps more importantly, how are these orders impacting your lead times for Blackwell and your current visibility sitting here today, you know, almost halfway through 2025?\n[2583s->2593s]Well, we have more orders today than we did at the last time I spoke about orders at GTC.\n[2593s->2599s]However, we're also increasing our supply chain and building out our supply chain.\n[2599s->2602s]They're doing a fantastic job.\n[2602s->2613s]We're building it here on shore in the United States, but we're going to keep our supply chain quite busy for several many more years coming.\n[2613s->2625s]And with respect to further announcements, I'm going to be on the road next week through Europe.\n[2625s->2640s]And it's just about every country needs to build out AI infrastructure and their umpteen AI factories being planned.\n[2640s->2650s]I think in the remarks, Colette mentioned there's some 100 AI factories being built.\n[2650s->2653s]There's a whole bunch that haven't been announced.\n[2653s->2673s]And I think the important concept here, which makes it easier to understand, is that like other technologies that impact literally every single industry, of course, electricity was one, and it became infrastructure.\n[2673s->2683s]Of course, the information infrastructure, which we now know as the Internet, affects every single industry, every country, every society.\n[2683s->2686s]Intelligence is surely one of those things.\n[2686s->2693s]I don't know any company, industry, country who thinks that intelligence is optional.\n[2693s->2694s]It's essential infrastructure.\n[2694s->2701s]And so we've now digitalized intelligence.\n[2701s->2708s]And so I think we're clearly in the beginning of the build out of this infrastructure.\n[2708s->2712s]And every country will have it.\n[2712s->2713s]I'm certain of that.\n[2713s->2714s]Every industry will use it.\n[2714s->2716s]That I'm certain of.\n[2716s->2722s]And what's unique about this infrastructure is that it needs factories.\n[2722s->2728s]You know, it's a little bit like the energy infrastructure, electricity.\n[2728s->2730s]It needs factories.\n[2730s->2733s]We need factories to produce this intelligence.\n[2733s->2736s]And the intelligence is getting more sophisticated.\n[2736s->2743s]We were talking about earlier that we had a huge breakthrough in the last couple of years with reasoning AI.\n[2743s->2748s]And now there are agents that reason and there are super agents that use a whole bunch of tools.\n[2748s->2754s]And then there's clusters of super agents where agents are working with agents, solving problems.\n[2754s->2769s]And so you could just imagine compared to one shot chat bots and the agents that are now using AI built on these large language models, how much more compute intensive they really need to be and are.\n[2769s->2774s]And so I think we're in the beginning of the build out.\n[2774s->2782s]And there should be many, many more announcements in the future.\n[2782s->2783s]Your next question.\n[2783s->2784s]Excuse me.\n[2784s->2788s]Your next question comes from Ben Reitz's with Melius.\n[2788s->2790s]Your line is open.\n[2790s->2791s]Yeah, hi.\n[2791s->2793s]Thanks for the question.\n[2793s->2801s]I wanted to ask first to Collette just a little clarification around the guidance and maybe putting it in a different way.\n[2801s->2813s]The 8 billion for H20 just seems like it's roughly 3 billion more than most people thought with regard to what you'd be foregoing in the second quarter.\n[2813s->2823s]So that would mean that with regard to your guidance, the rest of the business in order to hit 45 is doing 2 to 3 billion or so better.\n[2824s->2828s]So I was wondering if that math made sense to you.\n[2828s->2837s]And then in terms of the guidance, that would imply the non-China business is doing a bit better than the street expected.\n[2837s->2843s]So wondering what the primary driver was there in your view.\n[2843s->2851s]And then this second part of my question, Jensen, I know you guide one quarter at a time.\n[2851s->2871s]But with regard to the AI diffusion rule being lifted and this momentum was sovereign, there's been times in your history where you guys have said on calls like this where you have more conviction and sequential growth throughout the year, et cetera.\n[2871s->2886s]And given the unleashing of demand with AI diffusion being revoked and the supply chain increasing, does the environment give you more conviction and sequential growth as we go throughout the year?\n[2886s->2891s]So first one for Collette and then next one for Jensen. Thanks so much.\n[2891s->2908s]Thanks, Ben, for the question. When we look at our Q2 guidance and our commentary that we provided that had the export controls not occurred, we would have had orders of about 8 billion for H20.\n[2908s->2918s]That's correct. That was a possibility for what we would have had in our outlook for this quarter in Q2.\n[2918s->2932s]So what we also have talked about here is the growth that we've seen in Blackwell, Blackwell across many of our customers, as well as the growth that we continue to have in terms of supply that we need for our customers.\n[2932s->2938s]So putting those together, that's where we came through with the guidance that we provided.\n[2938s->2942s]I'm going to turn the rest over to Jensen to see how he wants to.\n[2942s->2945s]Yeah, thanks. Thanks, Ben.\n[2945s->2957s]I would say compared to the beginning of the year, compared to GTC timeframe, there are four positive surprises.\n[2957s->2967s]The first positive surprise is the step function demand increase of reasoning AI.\n[2967s->2977s]I think it is fairly clear now that AI is going through an exponential growth, and reasoning AI really busts it through.\n[2977s->2986s]Concerns about hallucination or its ability to really solve problems.\n[2986s->2999s]I think a lot of people are crossing that barrier and realizing how incredibly effective agentic AI is and reasoning AI is.\n[2999s->3007s]So number one is inference reasoning and the exponential growth there, demand growth.\n[3007s->3012s]The second one, you mentioned AI diffusion.\n[3012s->3019s]It's really terrific to see that the AI diffusion rule was rescinded.\n[3019s->3032s]President Trump wants America to win, and he also realizes that we're not the only country in the race.\n[3032s->3042s]He wants the United States to win and recognizes that we have to get the American stack out to the world\n[3042s->3047s]and have the world build on top of American stacks instead of alternatives.\n[3047s->3052s]And so AI diffusion happened.\n[3052s->3062s]The rescinding of it happened at almost precisely the time that the countries around the world are awakening the importance of AI as an infrastructure,\n[3062s->3072s]not just as a technology of great curiosity and great importance, but infrastructure for their industries and startups and society.\n[3072s->3078s]Just as they had to build out infrastructure for electricity and Internet, you got to build out an infrastructure for AI.\n[3078s->3083s]I think that that's an awakening and that creates a lot of opportunity.\n[3083s->3085s]The third is enterprise AI.\n[3085s->3093s]Agents work and agents are doing these agents are really quite successful.\n[3093s->3098s]Much more than generative AI, agentic AI is game changing.\n[3098s->3113s]Agents can understand ambiguous and rather implicit instructions and able to problem solve and use tools and have memory and so on.\n[3113s->3118s]And so I think enterprise AI is ready to take off.\n[3118s->3133s]And it's taken us a few years to build a computing system that is able to integrate, run enterprise AI stacks, run enterprise IT stacks, but add AI to it.\n[3133s->3142s]And this is the RTX Pro Enterprise server that we announced at Computex just last week.\n[3142s->3149s]And just about every major IT company has joined us super excited about that.\n[3149s->3152s]And so computing is one stat, one part of it.\n[3152s->3157s]But remember, enterprise IT is really three pillars.\n[3157s->3161s]It's compute, storage and networking.\n[3161s->3164s]And we've now put all three of them together for finally.\n[3164s->3167s]And we're going to market with that.\n[3167s->3170s]And then lastly, industrial AI.\n[3170s->3186s]Remember, one of the implications of the world reordering, if you will, is regions onshore manufacturing and building plants everywhere.\n[3186s->3195s]In addition to AI factories, of course, there are new electronics manufacturing chip manufacturing being built around the world.\n[3195s->3210s]And all of these new plants and these new factories are creating exactly the right time when Omniverse and AI and all the work that we're doing with robotics is emerging.\n[3210s->3215s]And so this fourth pillar is quite important.\n[3215s->3218s]Every factory will have an AI factory associated with it.\n[3218s->3226s]And in order to create these physical AI systems, you really have to train a vast amount of data.\n[3226s->3232s]So back to more data, more training, more AIs to be created, more computers.\n[3232s->3243s]And so these four drivers are really kicking into turbocharge.\n[3243s->3248s]Your next question comes from Timothy R. Curie with UBS.\n[3248s->3250s]Your line is open.\n[3250s->3251s]Thanks a lot.\n[3251s->3253s]Jensen, I wanted to ask about China.\n[3253s->3257s]It sounds like the July guidance assumes there's no SKU replacement for the H-20.\n[3257s->3262s]But if the president wants the U.S. to win, it seems like you're going to have to be allowed to ship something into China.\n[3262s->3265s]So I guess I had two points on that.\n[3265s->3269s]First of all, have you been approved to ship a new modified version into China?\n[3269s->3272s]And you're currently building it, but you just can't ship it in fiscal Q2.\n[3272s->3277s]And then you were sort of run rating $7 to $8 billion a quarter into China.\n[3277s->3283s]Can we get back to those sorts of quarterly run rates once you get something that you're allowed to ship back into China?\n[3283s->3287s]I think we're all trying to figure out how much to add back to our models and when.\n[3287s->3289s]So whatever you can say there would be great.\n[3289s->3291s]Thanks.\n[3291s->3293s]The president has a plan.\n[3293s->3296s]He has a vision and I trust him.\n[3296s->3305s]With respect to our export controls, it's a set of limits.\n[3305s->3322s]And the new set of limits pretty much make it impossible for us to reduce Hopper any further for any productive use.\n[3322s->3330s]And so the new limits, it's kind of the end of the road for Hopper.\n[3330s->3333s]We have limited options.\n[3333s->3337s]And so the key is to understand the limits.\n[3337s->3346s]The key is to understand the limits and see if we can come up with interesting products that could continue to serve the Chinese market.\n[3346s->3350s]We don't have anything at the moment.\n[3350s->3352s]But we're considering it.\n[3352s->3354s]We're thinking about it.\n[3354s->3358s]Obviously the limits are quite stringent at the moment.\n[3358s->3363s]And we have nothing to announce today.\n[3363s->3373s]And when the time comes, you know, we'll engage the administration and discuss that.\n[3373s->3378s]Your final question comes from the line of Aaron Rakers with Wells Fargo.\n[3378s->3381s]Your line is open.\n[3381s->3382s]Hi.\n[3382s->3383s]This is Jake on for Aaron.\n[3383s->3386s]Thanks for taking the question and congrats on the great quarter.\n[3386s->3399s]I was wondering if you could give some additional color around the strength you saw within the networking business, particularly around the adoption of your Ethernet solutions at CSPs as well as your\n[3400s->3404s]any change you're seeing in network attach rates.\n[3404s->3407s]Yeah, thank you for that.\n[3407s->3415s]We now have three networking platforms, maybe four.\n[3415s->3425s]The first one is the scale up platform to turn a computer into a much larger computer.\n[3425s->3427s]Scaling up is incredibly hard to do.\n[3427s->3429s]Scaling out is easier to do.\n[3429s->3431s]But scaling up is hard to do.\n[3431s->3433s]And that platform is called NVLink.\n[3433s->3446s]And NVLink comes with it, chips and switches and, you know, NVLink spines.\n[3446s->3448s]It's really complicated.\n[3448s->3452s]But anyways, that's our new platform, scale up platform.\n[3452s->3455s]In addition to InfiniBand, we also have SpectrumX.\n[3455s->3466s]We've been fairly, fairly consistent that Ethernet was designed for a lot of traffic that are independent.\n[3466s->3471s]But in the case of AI, you have a lot of computers working together.\n[3471s->3477s]And the traffic of AI is insanely bursty.\n[3477s->3479s]Latency matters a lot.\n[3479s->3484s]Because the AI is thinking and it wants to get work done as quickly as possible.\n[3484s->3487s]And you got a whole bunch of nodes working together.\n[3487s->3499s]And so we enhanced Ethernet, added capabilities like extremely low latency, congestion control, adaptive routing,\n[3499s->3506s]the type of technologies that were available only in InfiniBand to Ethernet.\n[3506s->3511s]And as a result, we improved the utilization of Ethernet in these clusters.\n[3511s->3519s]These clusters are gigantic from as low as 50% to as high as 85%, 90%.\n[3519s->3531s]And so the difference is if you had a cluster that's $10 billion and you improved its effectiveness by 40%, that's worth $4 billion.\n[3532s->3534s]It's incredible.\n[3534s->3538s]And so SpectrumX has been really, quite frankly, a home run.\n[3538s->3553s]And this last quarter, as we said in the prepared remarks, we added two very significant CSPs to the SpectrumX adoption.\n[3553s->3558s]And then the last one, the last one is Bluefield, which is our control plane.\n[3558s->3566s]And so in those four, the control plane and network, which is used for storage, is used for security,\n[3566s->3576s]and for many of these clusters that want to achieve isolation among its users, multi-tenant clusters,\n[3576s->3583s]and still be able to use and have extremely high performance, bare metal performance,\n[3584s->3591s]Bluefield is ideal for that and is used in a lot of these cases.\n[3591s->3599s]And so we have these four networking platforms that are all growing and we're doing really well.\n[3599s->3601s]I'm very proud of the team.\n[3603s->3607s]That is all the time we have for questions.\n[3607s->3610s]Denton, I will turn the call back to you.\n[3610s->3611s]Thank you.\n[3611s->3612s]Thank you.\n[3612s->3615s]This is the start of a powerful new wave of growth.\n[3615s->3617s]Grace Blackwell is in full production.\n[3617s->3619s]We're off to the races.\n[3619s->3621s]We now have multiple significant growth engines.\n[3621s->3627s]Inference, one's delighted workload is surging with revenue generating AI services.\n[3627s->3635s]AI is growing faster and will be larger than any platform shifts before, including the internet, mobile, and cloud.\n[3635s->3645s]Blackwell is built to power the full AI lifecycle from training frontier models to running complex inference and reasoning agents at scale.\n[3645s->3655s]Training demands continues to rise with breakthroughs in post-training and like reinforcement learning and synthetic data generation.\n[3655s->3657s]But inference is exploding.\n[3657s->3661s]Reasoning AI agents require orders of magnitude more compute.\n[3661s->3667s]The foundations of our next growth platforms are in place and ready to scale.\n[3667s->3674s]Sovereign AI nations are investing in AI infrastructure like they once did for electricity and internet.\n[3674s->3681s]Enterprise AI, AI must be deployable on prem and integrated with existing IT.\n[3681s->3692s]Our RTX Pro, DGX Spark, and DGX Station Enterprise AI systems are ready to modernize the $500 billion IT infrastructure on prem or in the cloud.\n[3692s->3696s]Every major IT provider is partnering with us.\n[3696s->3710s]Industrial AI from training to digital twin simulation to deployment, NVIDIA Omniverse and Isaac Groot are powering next generation factories and humanoid robotic systems worldwide.\n[3710s->3723s]The age of AI is here from AI infrastructures, inference at scale, sovereign AI, enterprise AI, and industrial AI.\n[3723s->3725s]NVIDIA is ready.\n[3725s->3740s]Join us at GTC Paris, our keynote at VivaTech on June 11, talking about quantum GPU computing, robotic factories and robots, and celebrate our partnerships building AI factories across the region.\n[3740s->3745s]The NVIDIA Band will tour France, the UK, Germany, and Belgium.\n[3745s->3753s]Thank you for joining us at the earnings call today. See you in Paris.\n[3753s->3758s]This concludes today's conference call. You may now disconnect.\n"
        ]
    }
]