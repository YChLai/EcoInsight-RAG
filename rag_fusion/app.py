import streamlit as st
import os
import json
from typing import List, Tuple, Dict  # <--- BUGä¿®å¤åœ¨è¿™é‡Œï¼
from langchain_core.documents import Document
import retriever
import config


# --- ç¼“å­˜åŠ è½½å‡½æ•° ---
@st.cache_resource
def get_llm_client_cached():
    """è·å–å¹¶ç¼“å­˜æ™ºè°±AIå®¢æˆ·ç«¯ã€‚"""
    return retriever.get_llm_client()


@st.cache_resource
def get_embedding_model_cached():
    """è·å–å¹¶ç¼“å­˜åµŒå…¥æ¨¡å‹ã€‚"""
    return retriever.get_embedding_model()


@st.cache_resource
def get_db_client_cached():
    """è·å–å¹¶ç¼“å­˜å‘é‡æ•°æ®åº“å®¢æˆ·ç«¯ã€‚"""
    retriever.embedding_model = get_embedding_model_cached()
    return retriever.get_db_client()


# --- æ ¸å¿ƒé€»è¾‘å‡½æ•° ---

def rephrase_question_with_history(question: str, chat_history: List[Tuple[str, str]]) -> str:
    """æ ¹æ®å¯¹è¯å†å²é‡æ„é—®é¢˜ã€‚"""
    if not chat_history:
        return question

    formatted_history = ""
    for user_msg, bot_msg in chat_history:
        formatted_history += f"ç”¨æˆ·: {user_msg}\nåŠ©æ‰‹: {bot_msg}\n"

    client = get_llm_client_cached()
    prompt = config.HISTORY_REPHRASE_PROMPT.format(chat_history=formatted_history, question=question)

    with st.spinner("æ­£åœ¨ç†è§£ä¸Šä¸‹æ–‡..."):
        response = client.chat.completions.create(
            model=config.QUERY_GENERATION_MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
        )
        rephrased_question = response.choices[0].message.content.strip()

    st.info(f"é‡æ„åé—®é¢˜: *{rephrased_question}*")
    return rephrased_question


@st.cache_data(show_spinner=False)
def _load_all_processed_data() -> dict:
    """åŠ è½½æ‰€æœ‰å¤„ç†å¥½çš„jsonå’Œtxtæ–‡ä»¶å†…å®¹åˆ°å†…å­˜ã€‚"""
    data = {}
    folder = "D:\\LLM\\RAG\\Nvidia-Finance-Rag\\processed_data"
    for filename in os.listdir(folder):
        file_path = os.path.join(folder, filename)
        if filename.endswith(".json"):
            with open(file_path, 'r', encoding='utf-8') as f:
                key = filename.replace("_processed.json", "")
                data[key] = {'type': 'json', 'content': json.load(f)}
        elif filename.endswith("_transcribed.txt"):
            with open(file_path, 'r', encoding='utf-8') as f:
                key = filename.replace("_transcribed.txt", "")
                data[key] = {'type': 'txt', 'content': f.read()}
    return data


def _find_page_content(all_data: dict, source: str, page: int) -> str:
    """ä»åŠ è½½çš„æ•°æ®ä¸­æ‰¾åˆ°ç‰¹å®šæºæ–‡ä»¶çš„ç‰¹å®šé¡µé¢çš„å†…å®¹ã€‚"""
    clean_source = source.replace(".pdf", "").replace(".json", "")
    source_data = all_data.get(clean_source)
    if not source_data: return ""

    if source_data['type'] == 'txt':
        return source_data['content']
    elif source_data['type'] == 'json':
        json_content = source_data['content']
        page_blocks = [block['content'] for block in json_content if block['metadata'].get('page') == page]
        return "\n\n".join(page_blocks)
    return ""


def format_context_for_llm(docs: List[Document]) -> Tuple[str, List[Dict]]:
    """å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£æ ¼å¼åŒ–ä¸ºLLMä¸Šä¸‹æ–‡ï¼Œå¹¶æå–å¼•ç”¨æ¥æºã€‚"""
    context_str = ""
    sources = []
    all_processed_data = _load_all_processed_data()

    for doc in docs:
        source = doc.metadata.get('source', 'æœªçŸ¥æ¥æº')
        page = doc.metadata.get('page', 'æœªçŸ¥é¡µç ')

        page_content = _find_page_content(all_processed_data, source, page)

        if page_content:
            context_str += f"--- ç›¸å…³èµ„æ–™æ¥æº: {source}, ç¬¬ {page} é¡µ ---\n"
            context_str += page_content
            context_str += "\n\n"
            sources.append({"source": source, "page": page})

    return context_str.strip(), sources


def generate_final_answer_stream(query: str, context: str):
    """ä»¥æµå¼æ–¹å¼è°ƒç”¨LLMç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚"""
    if not context or not context.strip():
        yield "æ ¹æ®æä¾›çš„èµ„æ–™ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œå› ä¸ºæœªèƒ½åŠ è½½åˆ°ç›¸å…³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚"
        return

    client = get_llm_client_cached()
    prompt = config.FINAL_ANSWER_PROMPT.format(context=context, question=query)

    response_stream = client.chat.completions.create(
        model=config.GENERATION_MODEL_NAME,
        messages=[{"role": "user", "content": prompt}],
        stream=True
    )

    for chunk in response_stream:
        content = chunk.choices[0].delta.content or ""
        yield content


# --- Streamlit UI ---

st.set_page_config(page_title="ç¯å¢ƒæ•°æ®æ™ºèƒ½åˆ†æåŠ©æ‰‹", page_icon="ğŸŒ±")
st.title("ğŸŒ± ç¯å¢ƒæ•°æ®æ™ºèƒ½åˆ†æåŠ©æ‰‹")
st.caption("ç”± RAG-Fusion å’Œå¤šæ¨¡æ€æ•°æ®å¤„ç†é©±åŠ¨")

if "messages" not in st.session_state:
    st.session_state.messages = []
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("è¯·è¾“å…¥ä½ å…³äºç¯å¢ƒæ•°æ®çš„é—®é¢˜..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        standalone_query = rephrase_question_with_history(prompt, st.session_state.chat_history)

        with st.spinner("æ­£åœ¨æ‰§è¡ŒRAG-Fusionæ£€ç´¢..."):
            final_docs_for_context = retriever.reranker_based_fusion(
                retriever.parallel_search(
                    retriever.generate_queries(standalone_query)
                )
            )

        if not final_docs_for_context:
            st.warning("æŠ±æ­‰ï¼Œæœªèƒ½æ£€ç´¢åˆ°ä»»ä½•ç›¸å…³ä¿¡æ¯ã€‚")
            full_response = "æŠ±æ­‰ï¼Œæœªèƒ½æ£€ç´¢åˆ°ä»»ä½•ç›¸å…³ä¿¡æ¯ã€‚"
        else:
            context, sources = format_context_for_llm(final_docs_for_context[:3])

            with st.sidebar:
                st.subheader("æœ¬æ¬¡å›ç­”å¼•ç”¨çš„èµ„æ–™æ¥æº")
                st.empty()  # æ¸…ç©ºæ—§çš„æ¥æº
                for src in sources:
                    st.markdown(f"- **æ¥æº**: `{src['source']}`\n- **é¡µç **: `{src['page']}`")

            response_generator = generate_final_answer_stream(standalone_query, context)
            full_response = st.write_stream(response_generator)

    st.session_state.messages.append({"role": "assistant", "content": full_response})
    st.session_state.chat_history.append((prompt, full_response))

with st.sidebar:
    st.header("åº”ç”¨è¯´æ˜")
    st.markdown("""
    è¿™æ˜¯ä¸€ä¸ªåŸºäºå¤šæ¨¡æ€ç¯å¢ƒæ•°æ®ï¼ˆPDFså’ŒéŸ³é¢‘è½¬å½•ï¼‰æ„å»ºçš„é«˜çº§RAGé—®ç­”åº”ç”¨ã€‚

    **ä¸»è¦æŠ€æœ¯æ ˆ:**
    - **Streamlit**: ç”¨äºæ„å»ºäº¤äº’å¼Webç•Œé¢ã€‚
    - **RAG-Fusion**: é€šè¿‡ç”Ÿæˆå¤šä¸ªæŸ¥è¯¢å¹¶èåˆç»“æœï¼Œæå‡æ£€ç´¢å‡†ç¡®æ€§ã€‚
    - **çˆ¶å­å—åˆ†å—**: ä¼˜åŒ–æ£€ç´¢å•å…ƒå’Œä¸Šä¸‹æ–‡å•å…ƒã€‚
    - **å¯¹è¯è®°å¿†**: æ”¯æŒå¤šè½®å¯¹è¯ï¼Œç†è§£ä¸Šä¸‹æ–‡ã€‚

    **å¦‚ä½•ä½¿ç”¨:**
    1. åœ¨ä¸‹æ–¹çš„è¾“å…¥æ¡†ä¸­æé—®ã€‚
    2. åº”ç”¨ä¼šè‡ªåŠ¨ç†è§£ä¸Šä¸‹æ–‡å¹¶æ£€ç´¢ç›¸å…³ä¿¡æ¯ã€‚
    3. ç­”æ¡ˆä¼šå®æ—¶æ˜¾ç¤ºï¼Œå¼•ç”¨çš„æ¥æºä¼šå±•ç¤ºåœ¨è¿™é‡Œã€‚
    """)